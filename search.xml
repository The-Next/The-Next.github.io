<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[端到端文本翻译实验记录]]></title>
    <url>%2F2022%2F10%2F20%2F%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%96%87%E6%9C%AC%E7%BF%BB%E8%AF%91%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[端到端文本翻译实验记录记录实验数据 任务描述端到端图像翻译，输入一个图片（已经被文本检测分割好的框，背景噪音已经压到了最低），输出图片中文本的翻译结果。 数据集设置United Nations Parallel Corpus：一共15,595,948条数据，经过sentence piece model进行分词处理，全部的数据用于预训练标准transformer使用。一共训练六个周期到达收敛效果。 截取2,000,000条较短的数据(len &lt; 20)用作图片生成，文字大小统一25pt，文字颜色为黑色，图片长度根据文字长度决定，与真实文本检测出来的图像框相同，更符合实际情况。在训练时使用padding图片进行统一处理统一填充为[480,680]大小，背景为随机背景。生成图像如下图所示。 1234567891011121314'''图像生成代码，大小不固定，规格由文字长度决定'''def write(string, img_path): cv2img = load_back() # cv2 to PIL pilimg = Image.fromarray(cv2img) draw = ImageDraw.Draw(pilimg) x, y = 0, 0 exceed_sign = 0 org = (x, y) write_width, write_height = draw.textsize(string, font) draw.text(org, string,font=font,fill=color) img = cv2.cvtColor(np.array(pilimg),cv2.COLOR_RGB2BGR) img = img[:write_height,:write_width] cv2.imwrite(img_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 25]) sentence piece model听说过但是从未用过，SpeechT5也是用的这个分词，记录一下，后面可以使用。github地址直接编译源码，用命令行调用更方便一些。先用训练集数据训练分词模型，然后再使用训练好的分词模型进行分词即可。12345678910CODES=32000SPM_TRAIN=sentencepiece/build/src/spm_train#这个目录需要编译sentencepiece才会出现en_spm_model="/home/sxy/Projects/cp/base_data/en-zh/english.model"$SPM_TRAIN \ --input=train.zh-en.en \ --model_prefix=/home/sxy/Projects/cp/base_data/en-zh/english \ --vocab_size=$CODES \ --character_coverage=1.0 \ --model_type=bpe 分词：123456SPM_ENCODE=sentencepiece/build/src/spm_encode$SPM_ENCODE \--model $en_spm_model \--output_format=piece \--input "train.en" \--output "train.spm.en" 实测中文速度会非常慢，与bpe相同，解码后需要进行还原处理，在使用fairseq-generate解码时需要加上参数--bpe &quot;sentencepiece&quot; --sentencepiece-model &quot;/home/sxy/Projects/cp/base_data/en-zh/english.model&quot; 实验概况预训练模型使用标准transformer,使用一千五百万数据集作为语料进行训练，训练脚本如下：12345678910111213141516171819dest="/home/sxy/Projects/cp/base_data/en-zh/binary"model_path="/home/sxy/Projects/cp/checkpoints/text_model100000"fairseq-train $dest \ --save-dir $model_path \ --source-lang "zh" \ --target-lang "en" \ --batch-size 64 \ --save-interval-updates 10000 \ --arch transformer \ --task translation \ --patience 10 \ --no-epoch-checkpoints \ --optimizer adam --adam-betas '(0.9, 0.98)' \ --dropout 0.1 --lr 5e-4 --lr-scheduler inverse_sqrt \ --log-interval 10 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --update-freq 8 --skip-invalid-size-inputs-valid-test \ --encoder-embed-dim 256 --encoder-ffn-embed-dim 2048 --encoder-attention-heads 8 --decoder-attention-heads 8 --share-decoder-input-output-embed \ --dataset-impl "mmap" \ --ddp-backend "c10d" &amp; 实验结果（从MutiUN抽出1000条数据进行测试）： 测试 bleu 从MutiUN抽出1000条数据 64.14 baseline使用ITNet，架构如下图所示： 测试 bleu 从1500w数据里抽出来1000条（v1） 27.76 v1.0在resnet后添加encoder模块，并加入对比学习。架构如下图所示： 训练脚本：1234567891011121314dest=/home/sxy/Projects/cp/base_data/en-zh/contrastivemodel_path="/home/sxy/Projects/cp/checkpoints/duibi_model"fairseq-train $dest -s "zh" -t "en" \--save-dir $model_path \--save-interval-updates 10000 \--no-epoch-checkpoints \--arch contrastive \--batch-size 16 \--patience 10 \--task image_translation \--optimizer adam --adam-betas "(0.9, 0.98)" --lr 1e-4 --lr-scheduler inverse_sqrt \--criterion my_criterion --label-smoothing 0.1 --dropout 0.1 \--finetune-from-model "/home/sxy/Projects/cp/checkpoints/text_model100000/checkpoint_best.pt" \--max-tokens 1024 --update-freq "4" --skip-invalid-size-inputs-valid-test --log-interval 10 &amp; 这里的参数finetune-from-model参数是加载预训练的模型，但是这里我们的模型已经破坏了fairseq的encoder-decoder结构，预训练的encoder因该是Encoder text部分，而我们模型中真正的encoder因该是ResNet101+transformer encoder部分，所以要重写参数加载过程（在主model里）。123456789101112131415161718192021222324252627282930def load_state_dict( self, state_dict, strict=True, model_cfg=None, args=None,): """ Copies parameters and buffers from *state_dict* into this module and its descendants. Overrides the method in :class:`nn.Module`. Compared with that method this additionally "upgrades" *state_dicts* from old checkpoints. """ model_state_dict = self.state_dict() initialized_keys = [] for key in state_dict: temp_key = key.replace('encoder.','contrastive_encoder.') #更改指向，这里contrastive_encoder才是原版encoder，需要对比学习的地方 #从头开始训练的时候才用 if temp_key in model_state_dict: # 对应参数位置进行替换 model_state_dict[temp_key] = state_dict[key].to(model_state_dict[temp_key].device) initialized_keys.append(key) logger.info(f"Keys initialized with pretrained model: &#123;initialized_keys&#125;") logger.info(f"coverage percent: &#123;len(initialized_keys) / len(model_state_dict)&#125;") return super().load_state_dict(model_state_dict, strict, model_cfg, args) 这里还有另外一个问题，如果模型因非模型本身原因中断的时候，想要继续训练，那就不需要再更改加载过程，因为先前训练保存的参数和模型是一致的。 由于图片生成大小不一，所以在加载时要针对图片进行padding，需要在dataset里对图片统一补齐至横480竖680，图片本体会放在左上角。123456789'''图片补齐，还需完善，目前只能将小于这个规格的图片进行处理需要添加一个缩放功能，将图片缩至规定大小'''def padding_feat(img): s_h,s_w = 680,480 h, w, n = img.shape img_modify = cv2.copyMakeBorder(img, 0, s_h - h, 0, s_w - w, cv2.BORDER_CONSTANT, value=[0,0,0,0]) return img_modify 测试 bleu 从MutiUN抽出1000条数据（v2） 60.51 从1500w数据里抽出来1000条（v2） 35.04 问题1：淘宝随便弄了一张图测试，效果很不好，因为淘宝所用词的语料和预训练的语料完全不同，词表里基本不会涵盖，故翻译不会有效果。问题2：提取1500w数据中倒序抽取1000条数据（防止和之前抽取的数据重合）测试，发现效果会降低。统计了词表之后，发现淘宝中的词在词表中很少出现。下一步可以将分词分到字的级别进行预训练。 v2.0数据集改动：United Nations Parallel Corpus：一共15,595,948条数据，分割为训练集15784041条，验证集100000条，测试集2000条，英文部分经过sentence piece model进行分词处理，中文部分按照字进行分词。 截取2,022,000条较短的数据(len &lt; 20)用作图片生成，其中训练集2000000条，验证集20000条，测试集2000条，文字在10-40pt之间随机设置，文字颜色为黑色，图片长度根据文字长度决定，与真实文本检测出来的图像框相同，更符合实际情况。在训练时使用padding图片进行统一处理统一填充为[480,680]大小，如果某一边界超出该大小，就先缩放，再padding，背景为随机背景。 模型改动未改动 测试测试集测试测试|bleu– |:–:Transformer|49.99OurModel|42.51 真实测试测试用例： 基于字的模型结果： 基于词的模型结果： v3.0数据集改动：沿用v2.0的数据集设置，更改图像生成。使用SynthTIGER生成图片。效果如下:]]></content>
      <categories>
        <category>实验记录</category>
      </categories>
      <tags>
        <tag>实验记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2F2020%2F03%2F31%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[动态规划&emsp;&emsp;中间因为各种破事，好久没有写了，趁着这次开学上算法课，把学了好久的动态规划算法总结一下。&emsp;&emsp;说动态规划之前先说一下贪心算法。比如说给你一堆价值为1、2、3、4、5的物品若干（物品可以装一半或者某一个分数），让你取一定价值的物品，使得取得的物品数量最少，正常的思路就是先拿价值为5的，拿完了之后再拿价值为4的，依此类推，直到拿够数量为止，这样拿到的物品总价值也就最大。这种策略就是贪心。 &emsp;&emsp;现在同一个问题换一些条件，假如口袋里只能放下价值为15的东西，但是现在我们的东西的价值分别为1、5、11（一次必须装一个，不能分开），那么此时如果使用贪心算法的话，结果就是。 1 x 14 + 4 x 1 = 15 &emsp;&emsp;但是正解确是 3 x 5 = 15 &emsp;&emsp;这个问题就是经典的0-1背包问题，但是贪心算法为何不能解决这个问题？原因就是贪心算法的目光太短浅。贪心的纲领是只顾眼前的利益，如果选择11，那么剩下来的容量就变成了4，凑够这个4要花费的代价比选择5并且凑够10的代价更大。&emsp;&emsp;其实想要解决这个目光短浅的问题，有一个特别简单的方法就是暴力枚举，但是如果使用这个方法，先不说运行效率什么的，期末改卷老师或者面试官估计会锤爆你的狗头。所以这个方法自己玩玩就行了。&emsp;&emsp;分析一下刚才的问题，总容量是15，如果我们取11，空间还剩下4，如果取5，空间还剩下10，现在对这两个问题进行抽象，其实就是给定总价值，凑出填满该价值的最少的物品是多少。我们假设f(n)为凑出n所需要的最少物品数量。如果取11，那么付出的代价是f(15) = f(4) + 1 = 4 + 1 = 5。如果取5，那么付出的代价是f(15) = f(10) + 1 = 2 + 1 = 3。那么再带上取1的情况f(15) = f(14) + 1 = 4 + 1 = 5。综合来估计，肯定是付出代价最少的最好，显而易见取5，这是通过三个结果所取得的最优结果。通过观察不难得出，f(15)的最终结果只与f(4)、f(10)和f(14)相关。再进一步就是与f(n-11)、f(n-5)和f(n-1)相关。那么上面的比较就可以化为 f(n) = Max{f(n-11),f(n-5),f(n-1)}+1 &emsp;&emsp;到这里，我们想取到f(n)的话，只需要取得子问题的f(i)即可，只要规定边界，就能够得到f(n), 代码如下所示，很容易看出来，复杂度只有O(n)。1234567891011121314151617#include&lt;iostream&gt;#include&lt;stdlib.h&gt;#include&lt;limits.h&gt;using namespace std;int main()&#123; int a[100],i,n,c; cin&gt;&gt;n; a[0] = 0; for(i = 1;i &lt;= n;i++)&#123; c = INT_MAX; if(i - 1 &gt;= 0) c = min(c,a[i-1]+1); if(i - 5 &gt;= 0) c = min(c,a[i-5]+1); if(i - 11 &gt;= 0) c = min(c,a[i-11]+1); a[i] = c;//三个值中选取最小代价 cout&lt;&lt;"f["&lt;&lt;i&lt;&lt;"]="&lt;&lt;a[i]&lt;&lt;endl; &#125;&#125; &emsp;&emsp;到这里我们只是使用了两个条件。 1.f(n)只与f(n-11)、f(n-5)和f(n-1)相关2.只关心f(i)的值，但不关心i是怎么凑出来的 &emsp;&emsp;比起贪心算法来说，我们通过对三种情况的抉择，避免了因目光短浅而造成的无法得出最优解。对比暴力破解，我们并不需要去枚举得到答案是怎么凑出来的，我们要求出f(15)，只需要得到f(14)，f(10)，f(4)，就够了，而对于这三个的值，用同样的方法即可。&emsp;&emsp;这样，问题的性质就出来了：要想求出f(n)，只需要求出更小的问题f(i)即可。f(i)作为f(n)的子问题而存在。这就是动态规划。 动态规划概念&emsp;&emsp;就像上面说的，将一个问题拆解成数个子问题，分别求出几个子问题，就能够求出原问题的解。动态规划有那么几个概念。 无后效性&emsp;&emsp;只关心f(i)的值，但不关心i是怎么凑出来的，也就是要求f(15)，需要求出来f(14)，f(10)，f(4)，但是并不需要直知道f(14)，f(10)，f(4)是怎么求出来的，因为这个对f(15)没有任何影响，简而言之就是未来与过去无关，也称之为无后效性。按照书上说的就是给定某一状态，这一状态之后的过程发展不受这一状态之前的各个状态影响。 最优子结构&emsp;&emsp;根据前面对f(n)的计算，在求解f(15)的过程中，我们对f(14)，f(10)，f(4)进行了一个比较，这个过程就蕴含了最优的思想。而f(14)，f(10)，f(4)这三个值也可以由更小问题选出的最优解得出。那么，大问题的最优解可以由小问题的最优解推出，这个性质就是最优子结构。 重叠子问题&emsp;&emsp;刚才那个问题中并没有凸显这个重叠子问题的概念，但是在向下求解的时候，很可能某一个个f(i)在不同的f(n)被求解的时候都被用到，比如说f(15)需要f(14)，f(10)，f(4)，而f(12)需要f(10)，f(7)，f(1)。这就有了一个公共的子问题f(10)。这就是动态规划与分治法的最大区别，分治法的子问题与子问题之间是互不相交，而动态规划的子问题是相互重叠的。从上面的代码中也体现出来，动态规划其实就是把已经解决的问题进行了一个缓存，这样就避免了对同一个问题的重复求解，节省了时间。而且我觉得动态规划的另个好处就是我们不用去剪枝了，因为动归只保留了最优解，其他的情况都已经被排除了。&emsp;&emsp;设计一个动态规划通常需要三步，人称“dp三连”。 1.刻画最优结构特征。(我是谁？)2.递归定义最优解。(我从哪里来？)3.计算最优解。(我到哪里去？) &emsp;&emsp;只要搞明白前两者，就能设计出状态转移方程，也就是类似于前面提到的f(n) = Max{f(n-11),f(n-5),f(n-1)}+1这个方程。下面通过几个问题来对这个算法加深一下了解。 钢条切割 公司购买长钢条，将其且各位短钢条出售。切割时不考虑工本费。求一个最佳切割方案。 &emsp;&emsp;假定出售一个长度为i英寸对钢条价格为pi(i = 1，2……)，长度均为整数价格表如下。长度i|1|2|3|4|5|6|7|8|9|10– |:–:|–:|–:|–:|–:|:–:|–:|–:|–:|–:价格pi|1|5|8|9|10|17|17|20|24|30&emsp;&emsp;这个问题是这样的。给定一短长度为n英寸的钢条和上表的方案，求一个切割方案，使得销售利益最大化。对于这个问题，可以使用朴素的递归求解方法，但是由于前面说过，因为子问题会重叠，所以朴素递归会对子问题进行重复求解，下面只说一下动态规划的解法。 &emsp;&emsp;一种是带备忘录的自顶向下法。这种方法的过程与自然递归一样，但是会保存每一步的解，在子问题求解时，会先检查备忘录，如果这个解不存在，计算完之后将答案记录在备忘录中，如果这个解已经存在，就直接使用，不再去计算。与上面的0-1背包问题一样。下面是把《算法导论》上的伪代码用C++写了一遍，在原代码的基础上，添加了断点记录，用来给出切割方案。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;stdlib.h&gt;using namespace std;int *arr;int MemoizedCutRodAus(int *p,int n,int *r)&#123; int q = 0; if(r[n] &gt; 0)&#123;//如果这个值被记录的有，就直接返回 return r[n]; &#125; if(n == 0)&#123;//剩下长度为0，价值就是0 return 0; &#125;else&#123; q = -1; int a; for(int i = 1;i &lt;= n;i++)&#123;//状态转移方程就体现在这里 //开始递归，对每一种子切割方案进行评估，选出最大的 int temp = p[i] + MemoizedCutRodAus(p,n - i,r); if(q &lt; temp)&#123; q = temp; a = i; &#125; &#125; arr[n] = a;//记录一下断点 &#125; r[n] = q;//缓存结果 return q;//返回当前子问题对最优方案&#125;int MemoizedCutRod(int *p,int n)&#123; int r[n+1]; for(int i = 0;i &lt; n;i++)&#123; r[i] = -1;//表示未知，因为总收益是非负的 &#125; return MemoizedCutRodAus(p,n,r);&#125;int main()&#123; int p[11] = &#123;0,1,5,8,9,10,17,17,20,24,30&#125;; int n = 7; arr = new int[n+1]; int s = MemoizedCutRod(p,n); cout&lt;&lt;"最优收益为:"&lt;&lt;s&lt;&lt;endl; cout&lt;&lt;"切割方案为:"&lt;&lt;endl; while(n &gt; 0)&#123;//通过断点来得出切割方案 cout&lt;&lt;arr[n]&lt;&lt;"米"&lt;&lt;endl; n = n - arr[n]; &#125; return 0;&#125; &emsp;&emsp;还有一种方法是自底向上。我个人更推崇这种方法，因为我写代码的时候一般会避免递归的产生。顾名思义，这种方法的解是从最底部开始的，没有自顶向下算法中向下递的过程，直接按照子问题的自然排序求解，从规模为0的子问题开始，直到求解规模为n的子问题，并且和备忘录算法一样，记录子问题的解，防止重复计算，代码如下。123456789101112131415161718192021222324252627#include&lt;iostream&gt;using namespace std;void ExtendedBottomUpCutRod(int *p,int n,int *r,int *s)&#123; r[0] = 0;//给一个起点 for(int i = 1;i &lt;= n;i++)&#123;//从r[1]开始，对每一种子问题求解 int q = -1; for(int j = 1;j &lt;= i;j++)&#123;//根据子问题的最优解，选取当前问题最优解 if(q &lt; p[j] + r[i-j])&#123;//状态转移方程就体现在这里 q = p[j] + r[i-j]; s[i] = j;//记录断点 &#125; &#125; r[i] = q;//缓存结果 &#125;&#125;int main()&#123; int n = 4; int r[n+1],s[n+1];//此时r不用初始化，因为从底下开始的 int p[11] = &#123;0,1,5,8,9,10,17,17,20,24,30&#125;; ExtendedBottomUpCutRod(p,n,r,s); while(n &gt; 0)&#123;//根据最优解逆推子问题最优解，得到方案 cout&lt;&lt;s[n]&lt;&lt;endl; n = n - s[n]; &#125; return 0;&#125; &emsp;&emsp;这个算法和上面的运行效率差不多，但是要简单上许多。 矩阵链乘法&emsp;&emsp;给定一个n个矩阵的序列(A1,A2,…,An)，我们的目的是计算它的乘积。 A1A2A3…AN &emsp;&emsp;因为矩阵乘法是满足结合律的，所以在计算的时候无论哪种结合方式，都会得到相同的计算结果。算法导论上给出了一个叫做完全括号化矩阵的概念： 它是单一矩阵，或者是两个完全括号化的矩阵乘积链的乘积，且自己外加括号。例如，如果矩阵链为(A1,A2,A3,4),则有五种完全括号化的矩阵乘积链：(A1(A2(A34)))(A1((A2A3)4))((A1A2)(A34))((A1(A2A3))4)(((A1A2)A3)4) &emsp;&emsp;虽然加括号的方式对结果没有什么影响，但是对运算代价产生的影响是巨大的，学过线性代数的都知道，A、B两个矩阵能够相乘的条件是A的列数要等于B的行数，而且一个m乘n的矩阵与一个哪、乘q的矩阵相乘，其结果是一个m乘q的矩阵。&emsp;&emsp;假设一个矩阵链(A1,A2,A3)，规模分别是10*100、100*5、5*50。按照不同的结合方式来计算一下他们的运算量。如果结合方式是((A1A2)A3),先计算A1A2(结果规模是105)，需要10\100*5=5000次标量乘法。这个10*5的矩阵再与A3结合，需要10*5*50=2500次标量乘法，一共是7500次标量乘法。如果结合方式为(A1(A2A3)), 先计算A2A3(结果规模是100*50)，需要100*5*50=25000次标量乘法。A1再与规模是100*50的矩阵相乘，则需要10*100*50=50000次标量乘法。总共75000标量乘法。可见第二种方案的运算量是前者的十倍。&emsp;&emsp;现在我们就可以提出矩阵链乘法问题： 给定n个矩阵的链(A1,A2,…,An)，矩阵Ai的规模为pi-1 x pi(1&lt;=i&lt;=n),求完全括号化方案，使得矩阵链乘法的乘积所需要的标量乘法最少。 &emsp;&emsp;首先排除暴力破解。&emsp;&emsp; 未完持续，先考试]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-支持向量机]]></title>
    <url>%2F2019%2F11%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[支持向量机&emsp;&emsp;这里有一个非常强大的算法，它在工业和学术界应用的非常广泛，它叫做支持向量机，与逻辑回归和神经网络相比，SVM（支持向量机）可以在学习复杂的非线性方程时提供更强大和更清晰的方式。 优化目标&emsp;&emsp;为了引入支持向量机，我们还需要从逻辑回归说起。 &emsp;&emsp;左边是假设函数，右边是激活函数，按照惯例我们还将z = $\theta$Tx。先考虑一下逻辑回归到底做了什么。假设我们有一组样本，它的y等于1，如果我们想让h$\theta$(x) $\approx$ 1,从图上来看，我们得让$\theta$Tx远大于0，才能让h$\theta$(x)的值趋近于1。相应的，y = 0，如果我们想让h$\theta$(x) $\approx$ 0，那么我也也要让$\theta$Tx远小于0才能达到。&emsp;&emsp;我们来观察一下逻辑回归的代价函数。 J($\theta$)=-$\frac{1}{m}$ [$\sum_{i=1}^{m}$y(i)log(h(x)) + (1-y(i))log(1 - h(x))] + $\frac{\lambda}{2m}$$\sum_{i=1}^{n}$$\theta$j2 &emsp;&emsp;会发现每一对样本都会为代价函数增添一样东西。 -(ylog(h(x)) + (1 - y)log(1 - h(x))) &emsp;&emsp;总代价函数会将所有训练样本从第一项到第m项求和，先不考虑求和，上面这个式子就代表了每一个单独的训练样本对总体目标函数做的贡献,接下来把假设函数的定义代进去。 -ylog($\frac{1}{1 + e^{\theta^Tx}}$) - (1 - y)log(1 - $\frac{1}{1 + e^{\theta^Tx}}$) &emsp;&emsp;然后我们就可以分情况讨论，先说y = 1的情况。 &emsp;&emsp;这种情况下在上面式子中只有第一项起作用，因为后面那一项中(1-y)等于0了，这时候就化简为了-log($\frac{1}{1 + e^{\theta^Tx}}$)，图像就是上图，从图中可以看出，当z(就是$\theta$Tx)值很大时函数值就会变的非常小，他对代价函数的影响也会很小，这就是为什么当逻辑回归出现y = 1时，$\theta$Tx会被设的非常大，因为他在代价函数中的对应值会非常小。为了构建支持向量机，我们需要从这个代价函数入手，然后进行少量的修改。我们将函数修改成如图所示的样子。&emsp;&emsp;我们在z = 1处取一个点，右边都是平的，左边是一条与逻辑回归幅度相似的直线，如图上的粉色部分。这条粉红的函数就是y = 1时的新代价函数。相似的，y = 0的代价函数如下图所示。&emsp;&emsp;这两个函数分别定为cost1(z)、cost0(z)，下标代表y的取值。接下来开始构建支持向量机，如下图所示的进行替换。得到 min$\frac{1}{m}$ [$\sum_{i=1}^{m}$y(i)(cost1($\theta$Tx(i))) + (1-y(i))(cost0($\theta$Tx(i)))] + $\frac{\lambda}{2m}$$\sum_{i=1}^{n}$$\theta$j2 &emsp;&emsp;支持向量机代价函数的参数略有不同，首先我们要去掉1/m这一项，这样同样能得到$\theta$的最优值，因为1/m只是一个常数。第二个改变就是关于权重，我们的代价函数分为前面那一部分和后面的正则项，我们现在将$\lambda$去掉，然后在前面那一项前面乘一个参数C，其实效果差不多，如果$\lambda$设置为一个大的值，那么正则项权重就大，如果C是一个小值，那么正则项权重也会很大，这只是不同的控制权衡方式。于是我们就得到了支持向量机的代价函数。 min C [$\sum_{i=1}^{m}$y(i)(cost1($\theta$Tx(i))) + (1-y(i))(cost0($\theta$Tx(i)))] + $\frac{1}{2}$$\sum_{i=1}^{n}$$\theta$j2 &emsp;&emsp;最后，与逻辑回归不同的是，支持向量机输出的并不是概率，它的输出是一个直接的预测，结果只有1和0。 大间隔分类器&emsp;&emsp;首先看一下关于支持向量机的代价函数。&emsp;&emsp;接下来要做的事如何让这些代价函数变得更小，如果有一个正样本，y = 1，当且仅当z大于等于1时，有cost1(z) = 0。反之，若y = 0，当且仅当z小于等于-1时cost0(z)函数值为0。总结起来就是。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络参数的反向传播算法]]></title>
    <url>%2F2019%2F10%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[神经网络参数的反向传播算法&emsp;&emsp;这里总结一个算法，能够给有给定的训练集的时候给神经网络训练参数。和之前一样，先从代价函数说起。 代价函数&emsp;&emsp;先说一下分类问题，我们的分类一般有两种，第一种是二元分类，它的输出只有一个结果，而且只有两种情况，就是0和1，在神经网络上的表现就是只有一个输出单元，并且输出一个实数。&emsp;&emsp;第二种是多分类，也就是他有k种不同的输出，如果k=4，那么输出单元将会是四个，h(x)的输出结果将会是一个k维向量（k &gt;= 3时用这个）。&emsp;&emsp;接下来我们来定义代价函数。 J($\theta$) = -$\frac{1}{m}$[$\sum_{i=1}^{m}$y(i) log h$\theta$ + (1 - y(i)) log(1 - h$\theta$(x(i)))] + $\frac{\lambda}{2m}$$\sum_{j=1}^{n}$$\theta$2j &emsp;&emsp;上面这个式子是逻辑回归的代价函数，逻辑回归中，我们会使代价函数最小化，一般是用-1/m乘以代价函数，然后在后面加上正则项。我们在神经网络中使用的代价函数是我们在逻辑回归中使用代价函数的一般形式，式子中不在只有一个逻辑输出单元y(i)，取而代之的是k个逻辑单元，如下公式。 Neural network:h$\Theta$ $\in$ RK &emsp;&emsp;(h$\Theta$(x))i = ith outputJ($\Theta$) = -$\frac{1}{m}$[$\sum_{i=1}^{m}$$\sum_{k=1}^{K}$y(i)k log (h$\Theta$(x(i)))k + (1 - y(i)k) log(1 - (h$\Theta$(x(i)))k)] + $\frac{\lambda}{2m}$$\sum_{l=1}^{L-1}$$\sum_{i=1}^{sl}$$\sum_{j=1}^{sl+1}$($\Theta$(l)ij)2 &emsp;&emsp;神经网络现在输出的是属于RK的一个向量，如果是二元分类的问题，这里的K可能就是上面公式上面地方写的(h$\Theta$(x))i表示了这个输出的第i个元素。这个公式的第一项就是-1/m乘以类似于原来逻辑回归中的代价函数，不过有些部分变成了了k从1到K的所有和，实际上就是K个输出单元之和，第二项类似于正则项，看起来十分复杂，总是在对$\Theta$(l)ij求和，如同罗辑回归，这里也是要去除那些对应于偏差值的项，同理，我们不对第0项求和，因为这些项类似于偏差单元，所以我们不会将这些项加到正则项之中。&emsp;&emsp;这就是应用于神经网络中的代价函数。 反向传播算法&emsp;&emsp;说完代价函数之后，就来说一下使代价函数变小的方法，就是这次总结的重点，反向传播算法。对于代价函数来说，我们需要一个$\Theta$使得J($\Theta$)取到最小值，为了使用梯度下降等算法，我们需要获得输入参数$\Theta$，J($\Theta$)以及它的偏导项$\frac{\partial}{\partial \Theta^{(l)}_{ij}}$J($\Theta$)。代价函数的计算就用上面的公式。&emsp;&emsp;我们来举一个例子。 &emsp;&emsp;先弄一个只有一个训练样本(x,y)的情况。粗略地说一下计算顺序，在向前传播的情况下，它的运算是这样的。 &emsp;&emsp;接下来，我们为了计算导数项，我们会引出一个叫做反向传播的算法。从直观上说，这一个算法是对每一个节点计算一个$\delta$(l)j，这个值代表了第l层的第j个节点的误差，之前我们的a(l)j代表了第l层的第j个节点的激活值，所以说这个$\delta$值就是捕捉了神经节点激活值的误差。我们用上面那一个有四层神经网络的结构做一个例子，我们需要对每一层都求一下$\delta$，那么第四层第j个单元的$\delta$值就是激活值减去真实值。 $\delta$(4)j = a(4)j - yj也可以写成$\delta$(4)j = h$\Theta$(x)j - yj，就是输出值与训练值y之间的差。 &emsp;&emsp;如果把$\delta$、a、y都当作向量的话，我们会得出一个向量化的表达式。 $\delta$(4) = a(4) - y 这里每一个值都是一个向量，维数等与输出单元的数目。 &emsp;&emsp;我们计算出了最后一项误差$\delta$(4)，接下来就要计算前面基层的误差，$\delta$(3)和$\delta$(2)的计算公式是。 $\delta$(3) = ($\Theta$(3))T$\delta$(4) . g’(z(3))$\delta$(2) = ($\Theta$(2))T$\delta$(3) . g’(z(2))这里的.是乘法操作，matlab中a.b表示矩阵a中的元素与矩阵b中的元素按位置依次相乘，得到的结果将作为新矩阵中相同位置的元素。 &emsp;&emsp;($\Theta$(3))T$\delta$(4)是$\Theta$的转置乘以下一层的误差，g’(z(3))则是参数为z(3)的激活函数的导数，如果求出这个导数的话，我们能发现g’(z(3))是等于a(3) .* (1 - a(3))的，过程如下图所示。 &emsp;&emsp;计算到这里就结束了，没有$\delta$(1)，因为输入层是训练集中观察到的，所以没有误差。我们从输出层开始计算$\delta$，然后传给上一层，来计算第三层隐藏层的误差，接着再往前一步计算第二层的误差，相当于把第三层的误差反向传播给第三层，然后再传到第二层，这就是反向传播的意思。&emsp;&emsp;我们可以证明我们需要的偏导数项恰好等于。 $\frac{\partial}{\partial \Theta^{(l)}_{ij}}$J($\Theta$) = a(l)j $\delta$(l+1)i (这里先不考虑正则化项) &emsp;&emsp;这个证明的过程出奇的复杂，所以就先不证明了。我们求出的这些$\delta$项可以特别快速的为我们求出偏导数项，然后我们再捋一捋反向传播算法，看一看在有大量数据情况下又是什么样子。&emsp;&emsp;假设我们有m给样本的训练集，我们首先设置$\Delta$(l)ij = 0（所有的l、i、j，$\Delta$是$\delta$的大写）我们用它来计算偏导项，这些$\delta$项会被作为累加项缓慢增加，然后算出偏导数，接下来遍历训练集，过程如下。 &emsp;&emsp;首先我们遍历训练集，然后设定输入层激活函数x(i)（每个样本的输入值），接下来用正向转播来计算下面几层的激活值，接着用这些样本的y(i)（每个样本的输出值），计算输出值的误差项，然后用反向传播算法计算每一层的误差值，直到第二层最后使用$\Delta$来对偏导数项求和（如图中最后一行），如果向量化对话就是$\Delta$(l) := $\Delta$(l) + $\delta$(l+1) (a(l))T，在for循环完成之后，我们还要进行一个计算。 D(l)ij := $\frac{1}{m}$ $\Delta$(l)ij + $\lambda$ $\Theta$(l)ij if j $\neq$ 0D(l)ij := $\frac{1}{m}$ $\Delta$(l)ij if j = 0 &emsp;&emsp;j = 0的时候对应偏差项，所有j = 0时没有进行正则化。最后$\frac{\partial}{\partial \Theta^{(l)}_{ij}}$J($\Theta$) = D(l)ij，过程不证明，自此我们就可以使用梯度下降了。 梯度检测&emsp;&emsp;反向传播由于非常复杂，所以有一些非常不好的特性，就是中间非常容易出bug，有时候看起来能够正常运行，并且梯度下降的时候，代价函数也在不断减小，但是最后得到的结果却比无bug的情况下高出一个量级。所以我们要做梯度检验。 &emsp;&emsp;假设有一个代价函数J($\Theta$)，并且有一个$\theta$在x轴上，我们对函数在$\theta$处求导，平常我们都是用微分的思想来对函数求导，就是取前后非常小距离的点所在的函数值连成的直线的斜率作为该点的斜率。 &emsp;&emsp;我们的导数公式是。 $\frac{d}{d\Theta}$ J($\Theta$) $\approx$ $\frac{J(\Theta+ \varepsilon)-J(\Theta- \varepsilon)}{2\varepsilon}$ &emsp;&emsp;通常情况下$\varepsilon$的取值是10-4。&emsp;&emsp;我们刚才只讨论了$\theta$是实数的情况，现在要考虑一下$\theta$是向量参数的情况，假设$\theta$是一个n维向量，我们可以用类似的思想去求所有的偏导数项。 &emsp;&emsp;这些等式能够从数学上去估算代价函数关于任何参数的偏导数，我们可以用这种方法完成在神经网络中关于所有参数的偏导数计算，然后可以和在反向传播中得到的梯度进行比较，以达到验证的目的。在验证的时候，首先利用反向传播算法先求得一个偏导数，然后使用梯度检验求出来一个值，与反向传播得出来的值比对，确定他们的值相似（最好只有几位小数差距），最后在训练网络的时候，关闭掉梯度检验，因为梯度检验的计算量很大，非常慢，所以一旦确定反向传播没有错误，就要要关闭梯度检验。 随机初始化&emsp;&emsp;当执行梯度下降算法时，我们需要为变量$\Theta$选取一些初始值。在设置初始值时，有一种想法时将所有的$\Theta$设置为0，这个想法在逻辑回归中是适用的，但是在神经网络中，这起不到任何作用，我们假设一个神经网络进行举例。 &emsp;&emsp;假设将所有的$\Theta$都设置为0，这就意味着图中输入层向前传播时两条权重相等，这就代表了隐藏单元a1、a2成了同一个函数，对所有的训练样本都会得出a1 = a2，由于这些值是相同的，所以说我们得到的$\delta$值也是相同的，这样得出的偏导数也是相等的，在进行梯度下降时，我们也会得出相等的$\Theta$值，然后在计算隐藏函数的时候a1和a2又是一样的，这就意味这这个神经网络算不出什么特征，因为每一个隐藏单元都在算同一个特征，这是一种高度冗余，这样的话最后的逻辑单元只能得到一种特征，这也就失去了神经网络最初的意义。所以我们这里要进行随机初始化。&emsp;&emsp;刚才那种权值一样的情况被称作对称权重问题，随机初始化就是解决这个问题的方法，所以对每一个$\Theta$值初始化为一个在-$\varepsilon$到$\varepsilon$之间的随机值（这里的$\varepsilon$和之前的$\varepsilon$意义不一样）。之后，就可以进行下一步的反向传播与梯度检验了。 总体流程&emsp;&emsp;在我们开始训练一个神经网络的时候，我们先要选择一个架构。 &emsp;&emsp;首先输入单元的个数取决于我们特征集的维数，如果是多分类问题，那么输出层单元的个数由所要区分的类别数确定，如果输出有多个，那么就将其变成向量输出，如果有是个分类，而我们的预测结果是第五个分类，那么，不能写成5，它的输出是一个向量，这个向量的第五个位置是1，其余位置是0。$$y =\left[ \begin{matrix} 0\\ 0\\ 0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ \end{matrix}\right]$$ &emsp;&emsp;输入输出单元的个数确定比较明确，而对于隐藏层单元个数和层数来说，一般默认是一层，或者不止一层，但是每一层隐藏单元个数相同，和上图一样。按常理来说隐藏单元越多越好，但是隐藏单元越多，计算量越大。一般每一层隐藏单元的数量要与特征纬度相匹配，可以相同，也可以大几倍。&emsp;&emsp;接下来说一下训练神经网络所需要的步骤。&emsp;&emsp;第一步是构建神经网络，然后随机初始化权重。&emsp;&emsp;第二步是执行向前传播算法，就是任意输入一个x(i)，然后得到一个输出值h$\Theta$(x(i))。&emsp;&emsp;第三步通过代码计算代价函数J($\Theta$)。&emsp;&emsp;第四步执行反向传播算法，算出偏导数$\frac{\partial}{\partial \Theta^{(l)}_{ij}}$J($\Theta$)。&emsp;&emsp;第五步使用梯度检验检查偏导数项，然后检验完之后记得关闭梯度检验。&emsp;&emsp;最后使用优化算法，比如梯度下降或其他方法，将这些算法与反向传播结合，计算偏导数的值，用来优化权重。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络学习]]></title>
    <url>%2F2019%2F09%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[神经网络学习&emsp;&emsp;在学习神经网络之前，这个名词经常在各种场合被提起，我对他的认知也只限于直到他跟大脑有关，至于有什么关系我还真不清楚，这一部分我也是花了不少时间去理解，从神经元结构开始，到最简单到一个模型，初步明白了它的概念以及他所解决的在线性回归和逻辑回归中的痛点。 神经元&emsp;&emsp;神经学家曾经将一个动物的听觉神经切断，并且将眼睛的信号传递给视觉神经，结果是听觉皮层学会了去看。也就是说无论将信号接入大脑皮层的哪一部分，大脑都会去处理它，也就是说大脑并没有几十或者上千的算法去驱动人类的活动，而是通过一个学习算法去处理这些外部数据，所以科学家们着眼于了大脑用来运算的细胞————神经元。&emsp;&emsp;顾名思义，神经网络就是模仿了大脑中的神经元。&emsp;&emsp;上图在高中生物中经常能看到，左边细胞体上有树突，右边突出的部分是轴突。树突有很多输入通道通往细胞体，而后面的那一条轴突像是一条输出通道，将信息继续传递给后面的神经元，看起来非常像一个计算单元，由树突接收一定数目的信息，通过细胞核的计算（可能吧，是不是细胞核计算的我也不知道），计算结果最终由轴突来输出（动作电位传递电信号，然后转化为化学信号）到下一个计算单元。而神经网络模型就是以此为基础。 神经网络模型&emsp;&emsp;在人工神经网络中，我们也是用一个非常简单的模型模拟神经元。如下图所示。 &emsp;&emsp;这是一个逻辑单元，中间的黄圈类似于细胞体，那么x1、x2、x3就类似于从树突（输入通道）接收的信息，通过神经元计算后由输出通道输出结果。这里的h$\theta$(x) 表示。 h$\theta$(x) = $\frac{1}{1 + e^{-\theta ^Tx}}$ &emsp;&emsp;这个公式在罗辑回归中出现过，后面再说他们之间的关系。这个公式中的x和$\theta$是我们的参数向量。$$x =\left[ \begin{matrix} x0\\ x1\\ x2\\ x3 \end{matrix}\right]$$$$\theta =\left[ \begin{matrix} \theta 0\\ \theta 1\\ \theta 2\\ \theta 3 \end{matrix}\right]$$ &emsp;&emsp;在输入参数中有一个x0，在上面那张图中并没有画出来，它一般被称作偏置神经元，通常情况下等于1，当然也有其他情况。&emsp;&emsp;在神经网络中有一个叫做激活函数的东西，指的是非线性函数g(z)，就是逻辑回归中的那个g(z)，上面的那个h$\theta$(x)就是参数为$\theta$Tx的激活函数。 g(z) = $\frac{1}{1 + e^{-z}}$ &emsp;&emsp;上面的参数向量$\theta$，我们之前姑且将其称之为模型参数，但是有些文档中将其称之为权重。&emsp;&emsp;现在，我们做了一个单个神经元的模拟，而神经网络即是一组神经元，如下图所示。 &emsp;&emsp;在这个图中，我们加入了隐藏单元x0和a(2)0。在整个模型中，第一列被成为输入层，用来输入模型参数，最后一列被称为输出层，用来输出结果，而中间不管有多少层，都被称为隐藏层（对于函数来说，除了输入和输出，其他值是不可见的）。下面逐步分析这个模型到底做了什么。&emsp;&emsp;在分析模型之前要解释一些专用的记号。 a(j)i ： 第j层第i个神经元的激活项（上图中黄圈部分，激活项代表了由神经元计算后输出的值），上图中的a(2)1就是第二层的第一个激活项$\Theta$(j) ： 这个是权重矩阵，他控制从某一层到下一层的映射。 &emsp;&emsp;上面那张图的整个计算过程如下面所示。 &emsp;&emsp;第一个隐藏单元的计算a(2)1就对应了第一行，他的参数是权重乘以对应参数并求和的线性组合结果，第二个隐藏单元和第三个隐藏单元的计算也是如此。我们的权重矩阵$\Theta$(1)就控制着三个输入单元到三个隐藏单元的映射参数矩阵，在这里我们有三个隐藏单元，每个单元又要接收四个参数（包含偏置单元），所以这里的$\Theta$(1)就是一个3乘4的矩阵。所以一般来说，在网络中第j层有sj个单元，在j+1层有sj+1个单元，那么控制着第j层到第j+1层的映射矩阵$\Theta$(j)就是一个纬度为sj+1 *（sj + 1）的矩阵，（sj + 1）中加的那个1应该是偏置单元a(j)0。&emsp;&emsp;最后，我们有一个输出层h$\theta$(x) 当然在这里我们也能写成a(3)1，因为他是第三层的第一个元素。其计算过程就是上面那一张的最后一个公式，这里的权重矩阵$\Theta$变成了$\Theta$(2)，因为这是第二层到第三层映射的权重矩阵。&emsp;&emsp;这就是一个非常简单的神经网络，它定义了函数h$\Theta$(x)，从输入x到输出y到的映射，其中x被参数化，只要改变$\Theta$，我们就能得到许多不同的假设。以上就是从数学上定义的神经网络。&emsp;&emsp;接下来就要理解一下这个假设的作用，拿下面这个图的例子来说明。&emsp;&emsp;这里要引入另外一些量，我们将第一行公式中的参数设为z(2)1，即： z(2)1 = $\Theta$(1)10x0+$\Theta$(1)11x1+$\Theta$(1)12x2+$\Theta$(1)13x3 &emsp;&emsp;那么原式就变成了 a(2)1 = g(z(2)1)同样还有a(2)2 = g(z(2)2)和a(2)3 = g(z(2)3) &emsp;&emsp;这里的z都是一些线性组合（输入值和加权的线性组合）。在上面图中的公式里，将这些线性组合放在一起，正好是一个矩阵向量乘法$\Theta$(i)x，由此，我们就能将神经网络的计算向量化。我们将x假设为一个向量，其中x0=1。$$x =\left[ \begin{matrix} x0\\ x1\\ x2\\ x3 \end{matrix}\right]$$&emsp;&emsp;再设z(2) $$z^{(2)} =\left[ \begin{matrix} z^{(2)}_1\\ z^{(2)}_2\\ z^{(2)}_3 \end{matrix}\right]$$ &emsp;&emsp;这里的z(2)是一个三维向量。接下来向量化他们的运算。 z(2) = $\Theta$(1)xa(2) = g(z(2)) &emsp;&emsp;需要注意的是因为z(2)是三维向量，激活函数在对其运算时需要对每一项进行运算。着一过程计算出来结果a(2)1、a(2)2、a(2)3将作为新的x1、x2、x3进入下一层继续运算，当然在计算的时候会加入一个a(2)0 = 0作为偏置单元，也就是下一层的x0。直到最终计算出结果h$\Theta$(x)。如此计算h$\Theta$(x)的过程叫做向前传播。&emsp;&emsp;再来看一下这个神经网络，我们把左半边盖住。 &emsp;&emsp;右边的部分看起来很像逻辑回归。我们的输出函数是由s型激活函数给出的。 h$\Theta$(x) = g($\Theta$0a0 + $\Theta$1a1 + $\Theta$2a2 + $\Theta$3a3) &emsp;&emsp;a1、a2、a3的值都是上一层算出来的值。其实这实际上就是逻辑回归，只是输入的这些数值是通过隐藏层计算的。从第一层到第二层的函数是由参数$\Theta$(1)决定，，在神经网络中没有用来输入的x1、x2、x3作为特征，而是用a1、a2、a3作为新的特征，这三个数是他们学习得到的函数输入值，就是由第一层到第二层由$\Theta$(1)决定的那个函数，这个值是自己训练出来的，根据$\Theta$(1)的不同，有时候可以学习到很多复杂的特征，从而得到一个更好的假设函数，比原原始输入的假设更好。当然，神经元的连接方式是可以不同的，如下图所示。 Examples&emsp;&emsp; 举几个例子来看一下神经网络是如何学习复杂的非线性模型。&emsp;&emsp; 假设有两个值x1、x2，分别代表0和1，我们分别放两个正样本和两个负样本，如下图所示。 &emsp;&emsp; 实际上这是一个复杂机器学习的分类问题的一个简化版本。 &emsp;&emsp; 我们想要通过学习一个非线性的判断边界来区分这些正样本和副样本，可以通过神经网络来实现，我们可以把这个样本化简为前面那一张图上只有四个数据的样本。&emsp;&emsp; 我们需要计算目标函数 y = x1 XOR x2(异或运算)或者 y = x1 XNOR x2(同或运算) &emsp;&emsp;在上面那张图中，我们将红叉表示为1，圆圈表示为0，我们会发现用同或运算可能更合理，如下图所示。&emsp;&emsp;接下来我们就用神经网络来模拟一个AND运算。我们假设有两个二进制的数x1和x2，取值范围为{0，1}。目标函数是y = x1 AND x2，我们用一个很小的神经元模型来展示出来。 &emsp;&emsp;上面那一张图本来只有x1和x2，现在在上面加上一个值为1的偏置单元，并且在上面标注了权重，就是他们的系数，也就是$\Theta$。$$\Theta =\left[ \begin{matrix} -30\\ +20\\ +20 \end{matrix}\right]$$ &emsp;&emsp;这就意味着我们的假设模型h$\Theta$(x) = g(-30+20x1+20x2)。接下来将x1、x2的值代入，得到如下表格。&emsp;&emsp;观察结果，这其实就是与逻辑的运算，因为当x1与x2都为1时，运算结果才是1。通过这张真值表，我们就能看出神经网络计算的逻辑函数是如何取值的。&emsp;&emsp;现在，我们再来模拟一下或运算的功能。我们稍微将神经元模型修改一下。&emsp;&emsp;我们修改了一下权重参数，现在我们的假设模型变为h$\Theta$(x) = g(-10+20x1+20x2)，真值表也随之改变。&emsp;&emsp;很轻易能看出这里实现了或的功能。&emsp;&emsp;还有一个就是(NOT x1) AND (NOT x2)逻辑。在上面的逻辑中，我们能够知道想要实现NOT逻辑，需要在x1和x2上放一个比较大的负数作为权值，我们可以建立一个只有一个计算单元的神经网络计算结果，这个逻辑运算结果为真的条件是当且仅当x1 = 0，2 = 0。&emsp;&emsp;接下来就要通过多层的神经网络模型来实现更复杂的逻辑。我们需要将刚才的三个逻辑组合到一个神经网络中。 &emsp;&emsp;第一部分是x1 AND x2运算，第二部分是(NOT x1) AND (NOT x2)运算，第三部分是x1 OR x2运算。下面开始假设神经网络模型。&emsp;&emsp;a(2)1执行AND运算，a(2)2是(NOT x1) AND (NOT x2)运算，a(3)1执行OR运算，这里加了一个偏置单元，各自的运算中也添加了相应的权重，接下来列出真值表。&emsp;&emsp;这里我们就实现了XNOR运算。我们通过增加了一层来计算一个更复杂的非线性函数，这就是神经网络为何能计算复杂的函数。当网络有很多层的时候，第二层是关于输入的一些比较简单的函数，第三层在此基础上引入更加复杂的方程，越往后越复杂，OCR就能够基于此实现。 多元分类&emsp;&emsp;在很多情况下，我们分类情况并不是两种，通常这是一个多分类问题，它的本质是对一对多对拓展，比如在识别某一个物体时，其他所有的物体均被看作负类。比如在交通上，我们想通过摄像头区分行人、汽车、摩托车和卡车这四种类别。我们的对象是四个，所以我们就建立一个有四个输出单元的神经网络。&emsp;&emsp;现在，我们的神经网络的输出是一个有四维向量。现在我们可以用第一个输出单元判断是不是行人，第二个输出单元判断是不是汽车，第三个出单元判断是不是摩托车，第四个输出单元判断是不是卡车。如果这张图片是一个行人的话，我们最终的输出是。$$h_{\Theta}(x) =\left[ \begin{matrix} 1\\ 0\\ 0\\ 0 \end{matrix}\right]$$ &emsp;&emsp;如果是汽车会输出。$$h_{\Theta}(x) =\left[ \begin{matrix} 0\\ 1\\ 0\\ 0 \end{matrix}\right]$$&emsp;&emsp;如果是摩托会输出。$$h_{\Theta}(x) =\left[ \begin{matrix} 0\\ 0\\ 1\\ 0 \end{matrix}\right]$$&emsp;&emsp;以此类推。很容易看出来跟一对多发如出一辙，这四个输出就像四个分类器。这就是神经网络解决多分类的办法。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-正则化]]></title>
    <url>%2F2019%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化过拟合问题&emsp;&emsp;在我们进行线性拟合的过程中，有时候可能拟合的效果比较差，比如说用一个一次函数去拟合一如下图所示的样本，那么我们可以看出该算法并没有很好的拟合训练集，我们将它称之为“欠拟合”，也说他是“高偏差”。 &emsp;&emsp;另一种情况是：我们的特征值过多，拟合出来的函数是一个非常高次的函数（比如说下图的拟合函数是一个四次函数），它完美贴合每一个样本，似乎很好的拟合了训练集，如下图所示。但是他是一条上下波动的曲线，我们无法认为它是一个非常好的模型，我们将这个问题称之为“过拟合”，另一种说法是具有“高方差”。 &emsp;&emsp;对于上面那一个训练集来说，最好的办法就是使用二次函数，让其达到一个刚刚好的状态，如下图所示。 &emsp;&emsp;过拟合问题通常在特征值过多时出现，他非常贴合样本，导致我们的代价函数几乎为0，而且无法泛化到其他新样本中，这里的“泛化”指该模型应用到新的样本中。对于罗辑回归，过拟合问题仍然适用。 &emsp;&emsp;对于上面这一张图而言。从左到右依次对应了“欠拟合”、“正好拟合”、“过拟合”。&emsp;&emsp;下面，我们将通过一种方式去解决过拟合问题，在刚才的问题中，由于我们有直观的图像供我们观察，以便我们能够选取一个合适的多项式函数。但是实际情况不总是这样，我们的特征值可能有很多，而且在这种情况之下，绘图也是一个难点。可视化这种方法很难让我们对特征变量的选取作出决定。&emsp;&emsp;现在，有两种方法让我们解决这个问题。 （1）减少选取变量的数量 &emsp;&emsp;具体操作就是手动剔除那些相关性比较小的变量，保留对模型比较重要的变量，在后期学习模型选择算法，会帮助我们对特征变量进行筛选，这种方法非常有效，但是缺点是在舍弃特征变量的同时，也舍弃了关于某些变量的信息。 （2）正则化 &emsp;&emsp;这里我们选择特征数量不变，而去减少量级的做法来解决过拟合问题。这种方法也十分有效，特别是当我们不想去舍弃某些特征变量时。这篇文章主要就是记录关于正则化的问题。 代价函数&emsp;&emsp;我们拿上图的两个函数说事，第一个函数刚刚好，第二个函数因为太贴合样本而过拟合了。这个时候，我们就可以添加一个惩罚项，使得$\theta$3与$\theta$4变小，这就意味着我们要最小化其均方误差代价函数，接下来我们要对原来的代价函数进行修改。 min$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2+1000$\theta$32+1000$\theta$42 &emsp;&emsp;在这里，1000只是指代一个比较大的数。我们想要使最小化的函数尽可能小的方法只有一个，就是使$\theta$3与$\theta$4尽可能的变小。因为我们加上后面这两项时，会使整个函数变大，所以再次最小化使，我们会使$\theta$3与$\theta$4尽量趋近于0。此时在函数中，$\theta$3与$\theta$4所在的项就仿佛被抛弃一样。这样的话，这个四次函数变得相当于二次函数了。这样以来，我们的模型就能回归到二次函数，而且拟合的更好。&emsp;&emsp;在这个例子中，我们通过加入惩罚项增大两个参数来达到预期的效果，这就是正则化的思想。&emsp;&emsp;这种思想是，如果我们的参数比较小，这意味着一个更简单的假设模型，在上面的例子中，我们通过对$\theta$3和$\theta$4增加惩罚项，使他们都接近于0，从而获得一个更简单的假设模型。通过这种方式，我们也将解决过拟合的问题。&emsp;&emsp;再举一个例子，比如说我们有一个很多特征值的模型（比如说有100个），而且我们并不知道要去对哪一个特征值进行惩罚。所以我们要修改代价函数，来缩小所有的参数，因为我不知道去惩罚哪一项。在原有的公式后面添加一个新的正则项来缩小每一项的值。 J($\theta$)=$\frac{1}{2m}$ [$\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2+$\lambda$$\sum_{i=1}^{m}$$\theta$j2] &emsp;&emsp;这个项从$\theta$1到$\theta$100来缩小每一项的值，按照通常的习惯，我们添加惩罚项是从$\theta$1开始的，而对于$\theta$0，我们并没有去管它。&emsp;&emsp;在这个新的正则化函数中，$\lambda$被称为正则化参数，它的作用是控制两个不同目标之间的取舍。一个是尽可能的拟合训练集，另外一个就是保持参数尽量的小。如果这个参数$\lambda$被设的特别大，那么对$\theta$1到$\theta$n来说惩罚过大，最后所有的参数都趋近于0，此时就相当于舍弃了所有都项，就剩一个$\theta$0预测函数就成了一条直线，此时出现了欠拟合的情况，后面会学习如何选择这个参数，使得结果既不会欠拟合，也不会过拟合。 线性回归的正则化&emsp;&emsp;之前在进行罗辑回归时使用了两种算法，一种是梯度下降，一种是正规方程，现在我们将这两种方式进行正则化。&emsp;&emsp;首先看正则化回归的优化目标。 J($\theta$)=$\frac{1}{2m}$ [$\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2+$\lambda$$\sum_{i=1}^{m}$$\theta$j2] &emsp;&emsp;由前半部分的线性回归和后半部分的正则项组成。我们现在将带着正则项的优化目标代入梯度下降算法。如下图所示。 &emsp;&emsp;其中我们将$\theta$0单独拉出来作为一个更新项，因为我们计算的时候并没有对$\theta$0做任何惩罚。从$\theta$1开始的所有项在梯度下降时全部加入正则项进行最小化，此处可以用微积分证明。在上图中用中括号圈起来的部分，刚好就是J($\theta$)对$\theta$j的偏导数（此处J($\theta$)包含正则化项）。同理，上面那个是关于$\theta$0的偏导。&emsp;&emsp;我们把包含$\theta$j的项全部汇合起来（对$\theta$j进行合并）就能形成下面这个式子。 &emsp;&emsp;对比原式而言，这里出现了一个很有意思的项。 1-$\alpha$$\frac{\lambda}{2m}$ &emsp;&emsp;这一项是一个比1略小的数，因为首先他是一个正数，一般学习率很小，而m会很大，所以$\alpha$$\frac{\lambda}{2m}$就会很小，那么1-$\alpha$$\frac{\lambda}{2m}$只比1小一点点，所以就造成了每次更新的时候$\theta$j变成了$\theta$j乘以一个只比1小一点点的数，后果是把$\theta$j向0的方向缩小了一点点，$\theta$j也变小了一点点，而后面的那一项与我们平常梯度下降更新的一样。所以在梯度下降中，我们做的就是在每一次更新中，将$\theta$j逐步变小。&emsp;&emsp;下面说一下关于正规方程的正则化。&emsp;&emsp;首先建立一个矩阵X。 &emsp;&emsp;其中每一项都是一个单独的训练样本，然后建立一个矩阵y，里面放着训练集中的所有标签。X是一个m x (n+1)维的矩阵。y是一个m维的向量。为了最小化J($\theta$)，我们修改一下原来的正规方程。 $$\theta = (X^TX + \lambda\left[ \begin{matrix} 0 &amp; 0 \\ 0 &amp; 1 \end{matrix}\right])^{-1}X^Ty$$ &emsp;&emsp;在这里的矩阵其实是一个n阶矩阵，在此为了方便就写成了二阶矩阵，该矩阵的对角线除了左上角以外都为1，其余部分皆为0。用这个新的式子就可以得到J($\theta$)的全局最小值，证明就不再写了。 十一月五日补充：我忘写了，逻辑回归也是在公式后面加一个正则化项$\frac{\lambda}{2m}$$\sum_{i=1}^{n}$$\theta$j2。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-Logistic回归]]></title>
    <url>%2F2019%2F07%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Logistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic回归&emsp;&emsp;虽然这个算法叫做Logistic回归，但是他却是一个分类算法。分类算法通常是离散的，它通常只有两个值，0或1，也叫负类或者正类，也有更多的分类，但是这里只考虑两种情况的分类。 &emsp;&emsp;我们先假设有一个数据集，他的取值只有1和0，我们先采用线性回归法方式进行拟合，如图。 &emsp;&emsp;我们可以将分类器的阀值设置为0.5，超过纵坐标0.5的就是正类，低于纵坐标0.5的就是负类。目前看起来线性回归在分类问题上还比较合理，但是我们将x轴延长，再增加几个训练样本。 &emsp;&emsp;仅仅是增加了一个样本，此时我们的线性回归就发生了改变，阀值就不再是0.5了，如果还是0.5，有一些1的值就会被预测为0。所以将线性回归应用于分类并不是一个好主意，另外一个原因是在分类问题中，数据都是离散值，而在线性回归中，很多数据都大于一或小于0，所以我们需要Logistic回归算法，因为它的值一直介于0和1之间。 假设陈述&emsp;&emsp;我们希望我们输出的值在0和1之间。我们提出的线性回归假设是。 h$\theta$(x) = $\theta$Tx &emsp;&emsp;在Logistic回归中，我们将作出一点修改，使得 h$\theta$(x) = g($\theta$Tx) &emsp;&emsp;我们定义g(z)函数，z为实数。 g(z) = $\frac{1}{1 + e^{-z}}$ &emsp;&emsp;这个函数就叫做Logistic函数，Logistic回归这个名字也是由此而来，我们来看一下Logistic函数长什么样子。 &emsp;&emsp;很容易看出这个函数的取值范围在0和1之间，由这个函数，我们可以确认h(x)的取值范围也在0到1之间。但是我们实际上取得的值只可能是1或者0，获取0和获取1只是一个概率问题，当然，正是因为取值只有0和1，所以两个概率之间的和是1。 决策界限&emsp;&emsp;先回忆一下刚才的公式。 h$\theta$(x) = g($\theta$Tx)g(z) = $\frac{1}{1 + e^{-z}}$ &emsp;&emsp;实际上，我们的假设函数输出的是一个概率，我们可以将h(x)&gt;=0.5时取1，h(x)&lt;0.5时取0。观察一下上面的Logistic函数图，可以发现g(z)&gt;=0.5时，z&gt;=0，当g(z)&lt;0.5时，z&lt;0。那么对于假设函数，当g($\theta$Tx)&gt;=0.5时，$\theta$Tx&gt;=0。所以我们想要取到1时，只需要让$\theta$Tx&gt;=0就可以了，取0时同理。&emsp;&emsp;我们假设一个数据集，并假设一个函数。 &emsp;&emsp;我们接着假设$\theta$0 = -3，$\theta$1 = 1，$\theta$2 = 1。这样我们就得到了一个向量。$$\theta =\left[ \begin{matrix} -3\\ 1\\ 1 \end{matrix}\right]$$ &emsp;&emsp;接着我们就可以来判别我们y的取值是1还是0。代入我们的$\theta$值，我们可以得出。 y = 1 if -3 + x1 + x2 &gt;= 0y = 0 if -3 + x1 + x2 &lt; 0 &emsp;&emsp;如果将如上等式在图中展现就是下面这个样子。 &emsp;&emsp;不难看出，一条线将平面分割开来，红线左侧取0，红线右侧取1，而这条线，就被称作为决策边界。决策边界并不是训练集的属性，而是假设本身及其参数的属性，只要给定参数向量$\theta$，他的边界就已经被确定了。 代价函数 训练集：((x(1),y(1)),(x(2),y(2)),……,(x(m),y(m)))$$x \epsilon\left[ \begin{matrix} x^{(0)}\\ x^{(1)}\\ …\\ x^{(n)} \end{matrix}\right]x0 = 1,y \epsilon{0,1}$$h(x) = $\frac{1}{1 + e^{-\theta^Tx}}$ &emsp;&emsp;在之前的学习中，我们的代价函数是这个样子。 J($\theta$)=$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2 &emsp;&emsp;现在我们需要对其进行一些小小的改动。 J($\theta$)=$\frac{1}{m}$ $\sum_{i=1}^{m}$Cost(h$\theta$(x(i)),y)Cost(h$\theta$(x(i)),y) = $\frac{1}{2}$(h$\theta$(x)-y)2 &emsp;&emsp;现在我们得到了代价函数，我们希望通过代价函数找到最小代价，但是原来的梯度下降算法恐怕不能胜任。 &emsp;&emsp;Logistic函数的代价函数图如图所示，可以看出来他有很多的局部最优解，梯度下降算法在这里不太好用，我们无法保证其能收敛到全局最小值，所以我们需要另外找一个代价函数，使其称为弓形函数，就能再次使用梯度下降算法了。接下来我们重新定义Cost函数。 $$Cost(h(x),y) = \begin{cases}-log(h(x)) &amp; if &amp; y = 1\\-log(1-h(x)) &amp; if &amp;y = 0\end{cases}$$ &emsp;&emsp;这个函数看起来很复杂，我们来看看他的图像。&emsp;&emsp;这两段函数取值范围都是0到1，一端取0，一端取正无穷。 化简代价函数与梯度下降&emsp;&emsp;首先看一下Logistic函数的代价函数。 J($\theta$)=$\frac{1}{m}$ $\sum_{i=1}^{m}$Cost(h$\theta$(x(i)),y)$$Cost(h(x),y) = \begin{cases}-log(h(x)) &amp; if &amp; y = 1\\-log(1-h(x)) &amp; if &amp;y = 0\end{cases}$$ &emsp;&emsp;因为y的取值只有0和1，所以我们有办法来化简一下这个函数。 Cost(h(x),y) = -ylog(h(x)) - (1-y)log(1 - h(x)) &emsp;&emsp;由于y值只有1和0，很轻易就能得出这个等式与上面的分段函数完全等效，这样，我们就把上面的函数化简成为更紧凑的形式，Logistic回归的代价函数就如下所示。 J($\theta$)=-$\frac{1}{m}$ [$\sum_{i=1}^{m}$ylog(h(x)) + (1-y)log(1 - h(x))] &emsp;&emsp;为什么选这个代价函数就不在详细赘述了，这是从统计学中的极大似然法得出来的，同时他也是一个凸函数。&emsp;&emsp;得到代价函数后，我们要做的就是缩小他的值，还是使用梯度下降算法，原理还是反复更新每个参数来缩小他的值。下面是模版。 Repeat{ $\theta$j := $\theta$j - $\alpha$ $\frac{\partial}{\partial \theta j}$J($\theta$)}接着替换一下导数项就能得到Repeat{ $\theta$j := $\theta$j - $\alpha$ $\sum_{i=1}^{m}$(h(x) - y)x} &emsp;&emsp;虽然Logistic函数和线性回归的梯度下降算法很像，但是由于假设函数不一样，实际上这两个是完全不同的东西。 高级优化&emsp;&emsp;实际上有好多比梯度想讲算法更高级的算法存在，他们效率高，处理数据量大，唯一缺点就是比梯度下降算法复杂，所以除非数值计算很好，一般不推荐自己去实现它。下面举一个例子。&emsp;&emsp;左侧是一个样例，右侧是其Octive实现的代码，用来计算代价函数。然后可以调用库函数来解决最小化问题。&emsp;&emsp;其中fminunc就是用来最小化的库函数。 多元分类 一对多&emsp;&emsp;之前都是将问题分为两类，现在我们讨论一下关于多种问题的分类。&emsp;&emsp;可能、我们需要一个分类算法将我们的文件进行归类到不同的文件夹里。假设有工作、朋友、家人、爱好四个分类，分别用y = 1、y = 2、y = 3、y = 4，这个例子中，y的取值都是离散值，这是一个多分类问题。多分类情况下的图像可能会是这样。 &emsp;&emsp;之前两个分类的方法对一对多也有效，下面说一下一对多分类的原理，有时候也叫“一对余”方法。假设一个训练集包含三个类别，分别对应着y = 1、y = 2、y = 3。我们要做的就是将这三个数据集划分为三个独立的二元分类问题。我们可以设定一个伪训练集，将类2和类3设定为负类，类别1设定为正类，如图所示。 &emsp;&emsp;我们拟合一个分类器h_$\theta$(1)(x),三角形就是正样本，我们可以用直线来判定边界。接下来我们对下面的两个类别进行同样的处理，分别得到分类器h_$\theta$(2)(x)、h_$\theta$(3)(x)，图像如下所示。 &emsp;&emsp;我们现在得到了三个分类器，我们可以对这三个分类器进行分别训练，这样我们就得到了一个多类别分类器，h(x)来预测y = i的概率。最终我们将数据分别放入三个分类器，最后选取最大的作为预测结果。 max h(i)(x) &emsp;&emsp;在三个中我们选出可信度最高的，效果最好的作为最终结果，这就是多分类问题。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-多变量线性回归]]></title>
    <url>%2F2019%2F07%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[多变量线性回归&emsp;&emsp;在学过了一元线性回归之后，再来看一下另外一种可以适用于多个变量的模型————多元线性回归。 &emsp;&emsp;之前研究的回归模型，假设是研究一个房屋的大小与其价格的关系，但是如果我们可以知道的特征量又多处了许多，比如卧室数量，楼层数以及房屋的年龄。 &emsp;&emsp;现在我们又多出了三个特征量，分别用x1、x2、x3、x4表示，价格依然用y表示。我们用xi来表示第i组数据，使用xi_j表示第i组取值的第j给特征值。 h$\theta$(x) = $\theta$_0 + $\theta$_1x &emsp;&emsp;上面是原来单变量线性回归的时候所使用的通式，不过在这个地方已经不适用了，我们需要一个新的通用公式。 h$\theta$(x) = $\theta$0x0 + $\theta$1x1 + $\theta$2x2 + …… + $\theta$nxn （x0 = 1） &emsp;&emsp;此时X是一组从0开始的n+1维向量，同样的，$\theta$也能被写成和下面一样的向量。 $$X=\left[ \begin{matrix} x1 \ x2 \ x3 \ . \ . \ . \ p \end{matrix}\right]$$ &emsp;&emsp;我们还可以将上面的通式写成下面这种形式，$\theta$的转置乘以X。 h$\theta$(x) = $\theta$0x0 + $\theta$1x1 + $\theta$2x2 + …… + $\theta$nxn=$\theta$TX &emsp;&emsp;这就是多元线性回归，实际上就是使用多个特征值x来预测一个y值。 多元梯度下降法&emsp;&emsp;回顾一下上面的公式。 h$\theta$(x) = $\theta$0x0 + $\theta$1x1 + $\theta$2x2 + …… + $\theta$nxn &emsp;&emsp;我们现在将$\theta$1、$\theta$2、$\theta$3……$\theta$n设为向量$\theta$。那么与单元线性回归非常相似的是，多元线性回归也有一个代价函数。 J($\theta$)=$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2&emsp;&emsp; (这里的$\theta$是一个向量) &emsp;&emsp;有了这个公式，我们就可以得到多元线性回归的梯度下降算法,运算原理和单元的是一样的（这个地方只不过有更多的$\theta$需要求导），具体不再多写。 Gradient descernt:Repeat{ $\theta$j := $\theta$j - $\alpha$ $\frac{\partial}{\partial \theta j}$ J($\theta$0,……,$\theta$n)}(simultaneously update for every j = 0 …… n) 特征缩放&emsp;&emsp;如果一个模型中有多个特征，如果能确保这些不同特征的取值处于一个非常接近的范围，那么梯度下降可以很快的收敛。假设一个房间的大小在0到2000平方英尺范围之内，卧室的数量在0到5的范围之内。那么根据这两个参数画出来的等线图将是一个2000:5的椭圆。 &emsp;&emsp;如果在这张图上使用梯度下降算法的话，可能需要很长时间。有一种有效的方法就是将这些特征值收缩，比如说将上面的平方英尺数除以2000，把卧室数除以5.这样的话我们的等值线会变得很圆，这样执行梯度下降算法会更快。 &emsp;&emsp;这样以来，我们将x1和x2都收缩到了0到1的范围之内。所以在一般情况下，我们使用特征缩放最终都希望将特征值约束到-1到+1范围之内（接近即可）。范围太小也不行。正常情况下，我们会做一个均值归一的操作。 x = $\frac{x - \mu}{S}$ &emsp;&emsp;在这个操作中，$\mu$代表x的均值，S代表x的取值范围，即最大值减去最小值，事实上这些值并不需要非常精确，他只是为了更快的去执行梯度下降算法。 学习率&emsp;&emsp;学习率就是在公式中的$\alpha$。对于不同的模型，最终收敛所需要的迭代次数不一样，也许只有三十步，也许有三百步甚至三百万步。很多情况下是画出迭代曲线来确定函数是否收敛，或者使用自动收敛测试来确定函数是否收敛。可以取一个特别小的值$\varepsilon$，如果下降的幅度小于$\varepsilon$值，那么就认定函数收敛，但是这个值非常难找。所以还是推荐用画图的方式，画图有一点好处是可以看出来梯度下降算法是否在正常的工作，如果发现J($\theta$)一直在上升，那么就说明梯度下降算法出现问题了。一般出现这种问题的原因是因为在初始化的时候，初始值非常靠近最低点，但是此时$\alpha$值太大，导致在梯度下降过程中直接冲过了最低点。 &emsp;&emsp;在下一次的迭代中肯能会再一次冲过最低点，导致函数会一直上升。 &emsp;&emsp;另外一种情况就是$\alpha$值取小了，虽然说$\alpha$变小可以使得函数一直下降，但是$\alpha$值太小就会使得下降速度非常慢。&emsp;&emsp;我们可以尝试不同的$\alpha$值，然后绘制函数曲线，以此来判断$\alpha$值是否满足快速下降。 特征和多项式回归&emsp;&emsp;还拿房子为例，我们多了两个向量值，分别是临街宽度和纵向深度。我们建立一个线性回归模型。 h$\theta$(x) = $\theta$0 + $\theta$1 x 临街宽度 + $\theta$2 x 纵向深度 &emsp;&emsp;当然我们不一定非要将这两个值当作特征值来使用，我们都明白实际决定房子大小的是，拥有土地的大小，实际上是以上两者的乘积。所以我们新构造的特征就是临街宽度乘以纵向深度。 h$\theta$(x) = $\theta$0 + $\theta$1(纵向深度 x 临街宽度)。 &emsp;&emsp;所以有时候定义一个新的特征会得到一个更好的模型。与选择特征有着密切相关的一个概念是多项式回归。 &emsp;&emsp;假设有上面图像上的数据集，可以很轻易的看出直线并不能很好的来拟合这个数据集。所以我们通常会选择二次函数。 h$\theta$(x) = $\theta$0 + $\theta$1x + $\theta$2x2 &emsp;&emsp;当然，二次函数最终还会降下来，不符合我们的规律，所以我们还会选择三次函数来进行拟合。 h$\theta$(x) = $\theta$0 + $\theta$1x + $\theta$2x2 + $\theta$3x3 &emsp;&emsp;所以说我们如何进行多项式拟合，只需要稍微改动一下原来的式子。 h$\theta$(x) = $\theta$0 + $\theta$1x1 + $\theta$2x2 + $\theta$3x3=$\theta$0 + $\theta$1(size) + $\theta$2(size)2 + $\theta$3(size)3x1 = (size)x2 = (size)2x3 = (size)3 &emsp;&emsp;我们将第一个特征值设置为房子的面积，第二个特征值设置为房子面积的平方，第三个特征值设置为房子面积的立方。然后应用线性回归的方法来拟合这个模型。但是如果size的取值是0到1000，那么(size)2的取值就是1到1000000，(size)3的取值就是1到109，数据超级分散，所以特征缩放在这里非常重要。 正规方程&emsp;&emsp;对于某些问题，我们有更好的方法去求最小的$\theta$值。在梯度下降方法中，我们 采用迭代的方式来求去最终结果，而在正规方程中，可以使用解析的方式求解，可一次性求出最小值。&emsp;&emsp;在微积分中，我们有一种方法可以使每一个$\theta$取得最小，就是让代价函数对每一个$\theta$进行偏微分，然后等于0，事实上这是在数学中常见的让函数取得最小值的手段。 J($\theta$)=$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h$\theta$(x(i))-y(i))2$\frac{\partial}{\partial \theta j}$J($\theta$) = …… = 0 （for every j） &emsp;&emsp;下面举一个有四个特征的例子，在最前面添加一列，当作x1。 &emsp;&emsp;我们构建一个矩阵X。$$X =\left[ \begin{matrix} 1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45 \\ 1 &amp; 1416 &amp; 5 &amp; 2 &amp; 40 \\ 1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30 \\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36 \end{matrix}\right]$$&emsp;&emsp;这个矩阵中包含了所有特征变量，我们还将构建一个y向量。$$y =\left[ \begin{matrix} 460\\ 232\\ 315\\ 178 \end{matrix}\right]$$ &emsp;&emsp;最后，我们给出一个使得$\theta$最小化的公式,怎么证明是数学上的东西，就不再给出了。 $\theta$ = (XTX)-1XTy &emsp;&emsp;正规方法不需要特征缩放。与梯度下降相比，梯度下降需要多次迭代而且要选择合适的学习率，而正规方程就非常直接。但是实际上，梯度下降在数据量很大的时候运行十分稳定，而正规方程则不然。所以在数据量非常大时，依然采用梯度下降法，而在数据集比较小的时候，就采用正规方程。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-单变量线性回归]]></title>
    <url>%2F2019%2F07%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[单变量线性回归&emsp;&emsp;机器学习第一坑，跟着网易云课堂上吴恩达的机器学习课程学习，纯英文的，但是有字幕，语速什么的都非常不错。为这个课程写博客的目的只有一个，就是为了记录和总结，不然仅仅是听课的话，听完过一段就忘了，记录下来方面以后复习使用。 模型描述&emsp;&emsp;属于监督模型，在一个模型中，我们有数据集。比如说我们要买房子，我们有大量的房价信息，他们都有两个属性，一个是价格，一个是面积，我们可以根据这两个属性建立一个二维坐标，将我们的数据集放在上面，我们可以使用一条直线大致模拟这些数据的走向，这既是一个模型，根据这个模型，我们就可以根据房子的大小来预测房价。这就是一个很简单的监督学习模型。之所以叫监督学习，是因为对于每一个例子，他都会有一个“确定的答案”。而这个问题也是一个回归的问题，回归就是我们预测一个值的具体输出。另一种监督学习叫做分类，它是用来预测一个值的离散输出。&emsp;&emsp;在监督学习中，我们会有一个数据集，叫做训练集，接下来定义一些使用的符号。 m: 训练样本数量。x: 输入量（输入特征）y: 输出量（预测的目标变量）（x(i),y(i)），这里面的i表示的是坐标，而不是幂。 &emsp;&emsp;监督学习算法大致流程是这样的，我们首先提供一个数训练集给学习算法，学习算法会输出一个函数“h”，称为“假设函数”。 向这个函数输入一个x值，这个函数会输出一个预测的y值，其实就是一个正常的函数。针对这个函数我们有一个通式。 h$\theta$(x) = $\theta$_0 + $\theta$_1x &emsp;&emsp;下面的所有东西都是围绕这一个方程式展开。 代价函数&emsp;&emsp;我们需要求得一组$\theta$_0和$\theta$_1，使得函数与真实情况最符合。我们将这两个$\theta$值称为模型参数。我们要做的就是选择模型参数，在这个模型参数所选择的直线上，h(x)与y的值需要取得最小。 min_size =$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h(x)-y)2 &emsp;&emsp;这里的h(x)就是上面列出来的通式。我们需要找到一对$\theta$_0和$\theta$_1使得这个表达式的值最小。这个函数就是这个线性回归的整体目标函数，我们用J($\theta$_0,$\theta$_1)来表示上面这个函数，称之为代价函数，也被称作代价误差函数或者平方误差代价函数。 J($\theta$_0,$\theta$_1)=$\frac{1}{2m}$ $\sum_{i=1}^{m}$(h(x)-y)2 &emsp;&emsp;从公式中可以看出，J($\theta$_0,$\theta$_1)是一个二次函数，想要它的最小值，就要取得它的最低点。 &emsp;&emsp;实际上，决定我们代价函数的值有$\theta$_0和$\theta$_1两个，所以我们的函数应该是三维的碗状函数，同样的，我们所需要的还是要求得J($\theta$_0,$\theta$_1)的最小值。 梯度下降&emsp;&emsp;这个算法被广泛的应用于机器学习的各个领域，后面好多地方也会用到。算法的思想是先给定$\theta$_0和$\theta$_1的初始值（一般都是0）。接下来我们需要使$\theta$_0和$\theta$_1一点点的改变，来使J($\theta$_0,$\theta$_1)的值变小，直到找到一个最小值或者局部最小值。整个过程就如同在下山一样，如图所示，事实上如果出发点不同的话，我们得到的局部最优解也不一样。 梯度下降算法定义：repeat until convergence{//反复操作，直到收敛 $\theta$j := $\theta$j - $\alpha$$\frac{\partial}{\partial \theta j}$J($\theta$_0,$\theta$_1)&emsp;&emsp;(j = 0,j = 1)}在这里，:=是赋值，=是等于 temp0 := $\theta$0 - $\alpha$$\frac{\partial}{\partial \theta 0}$J($\theta$_0,$\theta$_1)temp1 := $\theta$1 - $\alpha$$\frac{\partial}{\partial \theta 1}$J($\theta$_0,$\theta$_1)$\theta$0 = temp0$\theta$1 = temp1这么做的原因是让数据同步，因为两个数的计算同时需要$\theta$_0和$\theta$_1，如果第一个数直接改变的话，第二个数计算的时候用的就不是原来的值了。 &emsp;&emsp;下面来分析一下这个公式，公式中的$\alpha$被称作学习速率，它决定了$\theta$_0和$\theta$_1的更新幅度。第二部分则是一个偏导数，我们可以以用一张图来看一下公式的运作过程，这里我们暂时将$\theta$_0设置为0，对$\theta$_1求偏导数，求得导数值是一个正的，说明是一个上坡，所以我们要向前移动，移动的距离就是$\alpha$，若果偏导数是一个负值，那么就向后移动，这就是导数项的意义。&emsp;&emsp;对于$\alpha$而言，上面也提到了他是学习速率，这个速率的大小影响着算法的速率，比如说如果$\alpha$很小，那么这个算法每次更新时只会移动一点点，最后会移动到终点，时间太长了。可是如果$\alpha$太大，那么在更新过程中，更新过程中可能造成的问题是无法收敛，也就是无法取到最低点，所以$\alpha$的取值很重要。&emsp;&emsp;我们有一种解决这个问题的办法，第一次我们更新时，可能幅度会很大，第二次更新时，我们的幅度会比第一次的小，随着算法的递进，越接近局部最低点时，更新幅度会越来越小，直到最低点，这样的话速度会快一些，而且能够保证收敛。 线性回归的梯度下降算法&emsp;&emsp;首先回顾一下上面的函数。 梯度下降算法：repeat until convergence{ $\theta$j := $\theta$j - $\alpha$ $\frac{\partial}{\partial \theta j}$ J($\theta$_0,$\theta$_1)&emsp;&emsp;(j = 0,j = 1)}线性假设：h$\theta$(x) = $\theta$_0 + $\theta$_1x平方差代价函数：J($\theta$_0,$\theta$_1) = $\frac{1}{2m}$ $\sum_{i=1}^{m}$ (h(x)-y)2 &emsp;&emsp;首先我们要搞定偏导数,先展开代价函数。 $\frac{\partial}{\partial \theta j}$ J($\theta$_0,$\theta$_1) = $\frac{\partial}{\partial \theta j}$ $\frac{1}{2m}$ $\sum_{i=1}^{m}$ (h(x(i))-y(i))2&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;= $\frac{\partial}{\partial \theta j}$ $\frac{1}{2m}$ $\sum_{i=1}^{m}$($\theta$_0 + $\theta$_1x(i)-y(i))2 j = 0 : $\frac{\partial}{\partial \theta j}$J($\theta$_0,$\theta$_1) = $\frac{1}{m}$ $\sum_{i=1}^{m}$(h(x(i))-y(i))j = 1 : $\frac{\partial}{\partial \theta j}$J($\theta$_0,$\theta$_1) = $\frac{1}{m}$ $\sum_{i=1}^{m}$(h(x(i))-y(i))xi &emsp;&emsp;由此我们可以得到我们的线性回归算法。 repeat until algorithm{ $\theta$0 := $\theta$0 - $\alpha$$\frac{1}{m}$ $\sum_{i=1}^{m}$(h(x(i))-y(i)) $\theta$1 := $\theta$1 - $\alpha$$\frac{1}{m}$ $\sum_{i=1}^{m}$(h(x(i))-y(i))xi} &emsp;&emsp;这就是我所学习的第一个机器学习算法，线性回归的梯度下降算法，也叫Batch梯度下降算法。在线性代数中，还有一种算法可以求解代价函数的最小值而不需要去使用这种迭代算法，但是通常在大数据情况下，我们还是用梯度下降算法来解决这个问题。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2019%2F06%2F18%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[排序算法 输入：一个n个数的序列 a1，a2……an输出输入序列的一个排列a1‘，a2‘……n‘，使得a1‘&lt;=a2‘&lt;=……&lt;=an‘ &emsp;&emsp;简而言之，排序就是将一组对象按照某一种逻辑顺序重新排列的过程，这是一个很古老的算法，同时在各个领域都起到了非常重要的作用，所以在这里对排序算法做一次总结。 排序算法与数据结构&emsp;&emsp;在我们进行练习的时候，通常都是使用一个个的数值进行排序，实际上，在应用中，我们不可能只对一个数值进行排序，而是对一个记录进行排序，排序的依据可能是其中的某一个关键字，除关键字之外，记录还会有卫星数据，所以在排序的过程中，并不是只对那些关键字进行排序，卫星数据也是要一同进行存取的。所以，如果卫星数据过大，在排序过程中挪动数据是比较耗费资源的，这种情况下，重新排列记录的指针数组而不是记录本身是一个更好的选择。&emsp;&emsp;正是这些细节上的选择，区分开了程序与算法之间的不同。算法只会描述一个方法，记录之中的卫星数据大小是不考虑的。所以我们在研究排序问题的时候，假定元素由数字组成，这样在概念上比较直接。但是具体情况之中，一些细节问题可能需要我们使用不同的数据结构来解决，这也是一种挑战。 为何使用排序？&emsp;&emsp;上面也说了，排序在各个领域都起到了非常重要的作用。而且这是一个在计算机领域中非常基础的问题。虽然排序算法在许多语言中都被内置来(甚至连C语言也内置了排序算法)，但是对于一个学习者的角度来讲，排序算法往往能够帮助我们有效的解决其他类似的问题。在跟多领域中，排序往往是第一个需要被解决的问题，而且排序算法多种多样，他们实现优雅高效，具有很深的研究价值。下面我会对我学过对一些排序算法进行总结，尽管插入排序与归并排序在算法分析那里已经总结过了，这里作为一个整体性对总结，我会再次总结。 冒泡排序&emsp;&emsp;冒泡排序是最早c语言程序设计课上学习的一个排序，也是大部分人所接触到的第一个排序，因为非常简单。但是因为它的性能很差，所以很多算法书籍中只会一笔带过，甚至就是作为一个课后习题出现。&emsp;&emsp;冒泡排序是交换排序一种，它的原理是比较相邻两个元素，第一个比第二个大，就进行交换，往后每一对相邻的元素都进行这样的操作，直到最后一个，最后一个就是最大的数，持续这个步骤，直到无法继续交换为止，正如同它的名字一样，最大的值是依次以冒气泡的的方式往上冒出来的。看一下具体的排序过程，以整型数组{84,83,88,87,61}举例(我用ppt画个图)。&emsp;&emsp;首先看第一趟冒泡.&emsp;&emsp;第一趟结束以后，可以看见最大值88已经被移动到了最后，接着第二趟.&emsp;&emsp;第二趟之后，第二大的数字87被移动到了倒数第二个。在看第三趟。&emsp;&emsp;第三趟之后，第三大的数字84被移动到了倒数第三个。&emsp;&emsp;经历了四趟之后，整个数组排序完毕。&emsp;&emsp;看一下冒泡排序的复杂度，最好情况下，即待排数据本身就是正序的，只需要一趟即可完成排序，比较次数为n-1，元素移动次数为0(不需要进行交换)，此时拥有最好的时间复杂度，即O(n);&emsp;&emsp;再看一下最坏的情况，最坏情况即是待排数据是逆序的，此时就需要进行n-1趟排序操作，每次要比较n-i(1 ≤ i ≤ N - 1)次,由于完全逆序，每次比较都需要进行交换，而且每次交换都要移动三次数据(两个数据交换需要三次移动)，那么我们的比较次数为 $\frac{N(N-1)}{2}$=O(N2)，移动次数为 $\frac{3N(N-1)}{2}$=O(N2)，他的最坏时间复杂度是O(N2)，因此，它的平均时间复杂度也是O(N2)。由于其中只有在交换过程中会用到一个单位辅助空间，所以其空间复杂度为O(1)。&emsp;&emsp;冒泡排序是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以相同元素的前后顺序并没有改变，冒泡排序是一种稳定排序算法。下面贴上冒泡排序的C++实现。12345678910111213/*冒泡排序*/void bubbsort(int* a, int length) &#123; int temp; for (int i = 0; i &lt; length; i++)&#123; for (int j = length-1; j &gt; i; j--)&#123; if (a[j]&lt;a[j-1])&#123; temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125;&#125; 选择排序&emsp;&emsp;这也是一个非常简单的排序算法，它的排序规则是这样的，首先找到数组中最小的元素，将它与数组中第一个元素的位置交换，然后再从剩下的元素中选取最小的元素，与数组的第二的元素交换。如此往复，直到整个数组有序，因为其不断在待排序元素中选择最大(小)者，所以起名叫选择排序。大致看一下排序过程，以数组{12，2，16，30，28，10，16，20，6，18}为例。&emsp;&emsp;这个排序很简单，每一趟排序只是在比较当前元素与目前已知的最小元素，每一次排序都会将一个元素放在最终位置，从上图也可以看出来，对角线上的每一个元素都代表一次交换，总交换次数为N，0～N-1的任意第i次交换都会伴随着一次交换与N-1-i次比较，所以总共会有N次交换与(N-1)+(N-1)…2+1=N(N-1)/2~N2/2次比较。&emsp;&emsp;在上面对冒泡排序的分析中，提到了最好运行时间与最坏运行时间，但是选择排序有一个特点，它的运行时间与输入无关，每一遍扫描并不能为下一遍提供什么信息或者遍历，所以无论原始序列有序无序、正序逆序，他们用的时间都一样长！它并不会向其他算法一样依赖初始状态，这有时候是个好事，有时候也是个坏事，它的时间复杂度是O(N2)。&emsp;&emsp;选择排序对数据移动可以说是最少的，每次交换都会改变两个数组元素，一共N次交换，并且交换次数与数组长度呈线性关系，这是一个其他算法都不具备的特征(大部分都是线性对数或者平方关系)，虽然选择排序和冒泡排序的时间复杂度在数量级上是一样的，都是两层循环。但是冒泡排序平均的换位操作比选择排序要多，所以内层循环中，选择排序只有O(N)而冒泡排序有O(N2/2)，所以综合来讲选择排序的时间效率要高于冒泡排序。下面贴出代码。1234567891011121314151617void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*选择排序*/void selectsort(int* a, int length) &#123; for (int i = 0; i &lt; length; i++)&#123;//将a[i]与a[i+1...length]的最小元素交换 int min = i;//最小元素索引 for (int j = i+1; j &lt; length; j++)&#123; if (a[j] &lt; a[min]) &#123; min = j; &#125; &#125; exch(a, i, min); &#125;&#125; 插入排序&emsp;&emsp;这个排序在算法分析中作为例子讲到过，现在再来说一遍。这个排序有点像在打牌的时候整理自己的手牌，我们会将未排序的牌插入到已经排好序的手牌之中，而且为了填补空缺，在牌移动前和移动后位置中间的所有牌会向后移动一位，插入排序算法的思路也是如此。&emsp;&emsp;与选择排序一样，插入排序的索引左侧的元素的排列是有序的，但是在排序结束之前，那些元素的位置并不确定，中间随时可能移动，当索引达到另外一侧时，排序完毕。看一下插入排序的过程。&emsp;&emsp;在这个N*N的轨迹表上很容易看出来交换和比较次数，最坏情况下是对角线一下所有元素都要移动位置(当前插入元素比所有一排序元素都要小),最好情况下根本不需要移动(当前插入元素比所有待排元素都大)，插入排序与选择排序不同，它的排序时间取决于初始元素顺序，比如一个有序数组的排序要比逆序或者乱序数组要快很多。&emsp;&emsp;最坏情况下在对角线以下所有元素都要移动与被比较，所以总共N2/2次比较与N2/2移动，而平均情况下，每次排序需要移动与被比较的元素为对角线一下元素总数的二分之一，那么总共N2/4次比较与N2/4移动。当然，最好情况下，它只需要N次比较与0次交换。总体来说，它的时间复杂度也是O(N2)。&emsp;&emsp;这种排序对于某种非随机数组特别有效，比如对一个有序数组来说，插入排序在发现其是有序之后，它的运行时间就会变成线性的。下面贴出实现。1234567891011121314/*插入排序*/void insertSort(int* a, int length) &#123; int key, i; for (int j = 1; j &lt; length; j++) &#123;//从第二个开始遍历，直到最后一个 key = a[j];//标记当前需要插入的数 i = j - 1;//i作为指针，遍历j之前的数 while (i &gt;= 0 &amp;&amp; a[i] &gt; key) &#123; //开始遍历，如果遍历到的数比key大，就把这个数向后移一位，知道循环完毕或者碰到比key小的数 a[i + 1] = a[i]; i = i - 1; &#125; a[i + 1] = key;//将key插入 &#125;&#125; 希尔排序&emsp;&emsp;在上面的插入排序中，如果原始序列非常大而且是乱序的话，插入排序的速度就会变得非常慢。在插入的过程中，元素只能与相邻的元素交换，于是只能一步一步的移动到最终位置，如果最小元素正好在序列的另一端，那么移动到另外一端就需要N-1次移动。为了加快这一进度，我们需要对插入排序进行改进，这也体现了我们研究初级排序算法的价值。我们让不相邻元素交换以达到局部排序，最后再让元素进行插入排序将局部有序数组排序，这种排序方法叫做希尔排序。&emsp;&emsp;希尔排序的思想是使数组中任意间隔为h的元素有序，称为h有序数组，h有序数组就是h个独立的数组编织在一起形成的数组。从整体数组来看，数组中间每隔h个位置的元素的集合，就是h个独立数组中的一个数组。&emsp;&emsp;对于任意h序列(不是h数组，这里指h的取值序列，最后一个元素是1)，在h很大的时候，我们可以将元素与其间隔非常远的元素进行交换，使得在h取得更小数字的时候排序方便。一般会选取一个初始的h值，N/3也好，N/5也好（N是数组长度)，然后每一次进行h数组进行插入排序之后就递减，一直到1。排序期间，希尔排序会对每一个h独立的子数组进行插入排序，由于子数组是相互独立的，我们只需要将原来插入排序中移动距离由1增加到h，所以希尔排序可以看作是插入排序使用不同增量过程的结果。当h递减到1时，希尔排序就变成了插入排序。&emsp;&emsp;上面几个排序我们都分析了他们的性能，但是对于希尔排序而言，它的性能分析相当复杂，其子数组部分有序成都完全取决于递增序列的选择。而这个选择并不简单，它并非仅仅取决于h的选择，而且与其某些数学性质有关，直到现在也没有证明出来哪一种序列才是最好的选择。&emsp;&emsp;但是事实证明，希尔排序在大型数组的排序中是绝对优于选择排序和插入排序的，并且数组越大，优势越大。只不过对于他性能的认证已经超出我们的范围，唯一能知道的是，它的运行时间达不到平方级别，最坏比较次数也只是与N3/2成正比。 当一个“h有序”的数组按照增幅k排序后，它仍然是“h有序”的。 &emsp;&emsp;希尔排序执行过程如下，h分别取5，3，1。相同颜色代表同一h子数组。&emsp;&emsp;代码如下1234567891011121314151617181920212223#pragma once#include&lt;iostream&gt;using namespace std;void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*希尔排序*/void shellsort(int* a,int length) &#123; int h = 1; while (h &lt; length/3)&#123; h = 3 * h + 1; &#125; while ( h&gt;=1 )&#123; for (int i = h; i &lt; length; i++)&#123; for (int j = i; j &gt;= h&amp;&amp;(a[j]&lt;a[j-h]); j-=h)&#123; exch(a, j, j - h); &#125; &#125; h = h / 3; &#125;&#125; 归并排序&emsp;&emsp;首先对于数组A[1……n] 1.如果n为1，那么数组就是排好序的。2.递归的对A[1到n/2向上取整]的这一部分，以及A[n/2+1向上取整到n]这部分排序3.把排好序的两个表归并 归并过程如图所示实质上就是将两个已排序好对的数组，也就是A[1..N/2]和A[N/2+1..n]，进行归并，得到一个排序好的数组，每一步都是固定的操作与数组长度无关，所以对于总数为n的输入，时间是$\Theta$(n)的。&emsp;&emsp;对于这个递归，我们可以写出一个递推式。 $$T(n) =\begin{cases}Θ(1) &amp; （n=1） \2T(n/2)+ Θ(n) &amp; （n&gt;1）\end{cases}$$ &emsp;&emsp;这是一个树状结构，树的末端时间只有$\Theta$(1)，而树的高度是lgn，叶的节点总数为n，完全扩展的递归树有lgn+1层，每层贡献总代价$\Theta$(n)，如果计算总数就是$\Theta$(n)lgn+$\Theta$(n)，根据渐进的思想，最后结果就是$\Theta$(nlgn)，考虑渐进，他比$\Theta$(n2)快,在数据足够大的情况下，归并排序将优于插入排序，差不多n大于30归并就更快了。下面是对《算法导论》中归并排序的C++实现，该算法使用了自底向上归并，并且使用了辅助数组。123456789101112131415161718192021222324252627282930313233const int N=204800;void Merge(int *arr,int p,int q,int r)&#123; int n1=q-p+1;//左数组长度 int n2=r-q;//右数组长度 int left[n1+1],right[n2+1];//开辟新的左右数组 for(int i = 0; i !=n1; ++i)&#123; left[i]=arr[p+i];//为左数组赋值 &#125; left[n1]=N;//左数组“哨兵” for(int j = 0; j!= n2; ++j)&#123; right[j]=arr[q+j+1];//为右数组赋值 &#125; right[n2]=N;//右数组“哨兵” int i=0,j=0; for(int k = p; k !=r+1; ++k)//将左右数组归并至原数组&#123; if(left[i]&gt;right[j])&#123; arr[k]=right[j]; ++j; &#125;else&#123; arr[k]=left[i]; ++i; &#125; &#125;&#125;void MergeSort(int *arr,int p,int r)&#123; //分治法，将数组分割，将复杂问题化简为数个简单问题 if(p&lt;r)&#123; int q=(p+r)/2;//数组分割标记，中间下标 MergeSort(arr,p,q);//分割左边数组 MergeSort(arr,q+1,r);//分割右边数组 Merge(arr,p,q,r);//进行归并排序 &#125;&#125; 快速排序&emsp;&emsp;快速排序是使用最广泛的排序算法，甚至大部分语言内置的库中使用的排序算法默认使用的就是快速排序。原因很简单，一个是因为它的实现很简单，而且平均性能非常好，一般情况下比其他算法要快得多。快速排序是一种原址排序，而且在长度为n的数组所需的时间和nlgn成正比。&emsp;&emsp;快速排序是基于分治思想的，所以它的算法过程也是经历三个阶段。 1.分解：数组A[p,q]被划分为左子数组A[p..s-1]与右子数组A[s+1..q]，使得A[p..s-1]的每一个元素都小于A[s]，使得A[s+1..q]中的每一个元素都大于A[s]，A[s]所在的位置就是它最终在的位置。2.解决：通过递归的调用快速排序，对子数组A[p..s-1]和A[s+1..q]继续分解，重复执行分解过程，直至不能分解。3.合并：由于我们使用的是原址排序，所以在递归执行到最底层的时候，数组A[p,q]就已经被排序完毕了，并不需要合并操作。 &emsp;&emsp;快速排序最重要的就是数组划分的过程，这一过程实现了原址重排。&emsp;&emsp;过程也比较简单，首先我们选择一个主元，一般为了方便我们都会选第一个或者最后一个，这个主元便是我们数组划分的依据，我们设这个主元为v，设这个数组的上界为hi，下界为lo。为了方便，在这里我们的v就随意的设成第一个元素即A[lo]，我们使用标记i、j，分别从A[lo+1]和A[hi]开始（也就是两端，不从A[lo]开始是因为A[lo+1]是主元），逐步从中间靠拢。如果出现A[i] &gt; v,i停止向后靠拢，出现A[j] &lt; v,j停止向前靠拢，二者同时停止时，就将A[i]与A[j]互换位置，然后继续向中间靠拢，重复上面的操作，直到i与j相遇，然后将v放在这个位置，这是就满足了主元前面都比主元小，主元后面都比主元大，同时主元也为更深层次的数组划分提供依据，而且这个数组被遍历了一次，所以一次划分的时间复杂度是$\Theta$(n) （n = hi - lo + 1）。下面是划分过程。 &emsp;&emsp;注意，这里只是提供一种快速排序的数组划分方法，这个方法并不惟一，不同教材上写的也不尽相同，但是划分思路是一样。 &emsp;&emsp;在性能方面，快速排序的运行时间主要取决于数组划分是否平衡，是否平衡划分又取决于用于划分的元素，即主元。如果划分平衡的话，它的性能相当于归并排序，如果不平衡的话，它的效率就接近于插入排序了。&emsp;&emsp;如果是在最坏情况下划分的话，即一子数组被分到了n-1个元素，一个数组被分到了0个元素，这种划分是最不均衡的。那我们就假设每一次划分都会遇见这一种情况，划分操作的复杂度为$\Theta$(n)。一个数组为0时，递归会直接返回，所以 T(0) = $\Theta$(1)。我们可以得到算法的递归式。 T(n) = T(n - 1) + T(0) + $\Theta$(n) = T(n - 1) + $\Theta$(n) &emsp;&emsp;我们将每一层的代价累加，就能得到一个算术级数，其结果是$\Theta$(n2)。所以在最坏情况下，快速排序的效率和插入排序一样，就算输入数组已经完全有序，它的效率还是$\Theta$(n2)，而这时候插入排序已经就是$\Theta$(n)了。 &emsp;&emsp;再看看最好的情况，最平衡的划分就是把主元去掉然后对半分了，两个子数组的大小都不会大于n/2。一个子数组规模为$\lfloor$n/2$\rfloor$，而另一个子数组的规模就是$\lceil$n/2$\rceil$-1.我们可以得到一个递归式。 T(n) = 2T(n/2) + $\Theta$(n) &emsp;&emsp;根据主定理，这个递归式的解是T(n) = $\Theta$(nlgn)。这样的划分可以使渐进时间更快。&emsp;&emsp;快速排序算法被广泛使用的原因就是它的平均运行时间总是接近于它的最好情况的。假设产生一个9:1的划分，那么它的递归式就是。 T(n) = T(9n/10) + T(n/10) + cn我们显示的写出$\Theta$(n)所隐含的常数c &emsp;&emsp;如果是一棵树的话，这棵树的每一层代价都是cn。这棵树在深度为log10n = $\Theta$(nlgn)地方就能达到递归边界，而我们的递归的终止是在深度为log10/9n = $\Theta$(lgn)的地方。在达到第一个递归边界与递归终止之前，每层代价至多为cn。所以说它的总代价就是O(nlgn)。即使是9:1这种非常不规则的划分，它的运行时间居然和最好运行时间是一样的。事实上，即便是99：1，它的时间复杂度还是这样。 任何一种常数比例划分都会产生深度为$\Theta$(lgn)都递归树，每一层代价都是O(n)。所以只要划分是常数比例，它的运行时间总是O(nlgn)。 &emsp;&emsp;下面给出快速排序的实现，这个实现是由E.W.Dijkstra（就是那个图论中求最短路径的Dijkstra算法的发明者）解出的三项切分的快速排序，我们在小于和大于的基础上引进一个等于，解决了重复排序的问题，使得其在重复元素很多的应用中更加快速。123456789101112131415161718192021222324252627#pragma once#include&lt;iostream&gt;using namespace std;void exch(int* a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125;/*三向切分快速排序*/void quicksort(int*a, int lo, int hi) &#123; if (hi &lt;= lo)return; int lt = lo, i = lo + 1, gt = hi; int v = a[lo]; while (i &lt;= gt)&#123; if (a[i] &lt; v) &#123; exch(a, lt++, i++); &#125; else if(a[i] &gt; v)&#123; exch(a, i, gt++); &#125; else&#123; i++; &#125; &#125; quicksort(a, lo, lt - 1); quicksort(a, gt+1, hi);&#125; 堆排序&emsp;&emsp;首先来说一下堆是个什么东西，它也叫二叉堆，一般用数组实现，从逻辑结构上讲类似一棵完全二叉树，每个节点对应数组中的元素，除了最后一层之外，这个树是完全充满的。在数组中，我们可以通过一些简单的计算来算出一棵树的父节点以及左右叶子结点。 int parent(int i) { return i / 2;}int left(int i) { return 2 i;}int right(int i) { return 2 i + 1;} &emsp;&emsp;二叉堆分两种，一种叫最大堆，一种叫最小堆，他们的共同点是都满足堆的性质，即是一个完全二叉树。最大堆中，对于某一个节点i来说，它的大小必须不能大于它的父节点。而在最小堆中，任意一个节点不能小于它的父节点。直观来讲，在层序上，从下往上呈现上升（下降）的趋势，根节点是最大（小）的元素。既然堆是一种完全二叉树，那么n个节点的完全二叉树的高度就是$\Theta$(log2n)，基本操作时间与高度成正比，所以时间复杂度也是O(log2n)。&emsp;&emsp;由于在堆排序中可能会有元素变动之后，堆就不符合最大堆的性质，或者整个数组输入的时候就不符合最大堆的性质。这个时候我们需要让某些元素在堆中逐级下降，使其能够满足最大堆的性质，这个过程就是堆的维护。堆的维护是一个递归的过程，每次递归的操作，就是将父节点i与其左右节点相比较，如果i是最大的，那么这个子树必然满足最大堆的性质，如果最大的点不是i，那么i节点与那个最大的元素交换，递归完毕之后，就能够形成一个最大堆。这一过程中，它的时间代价有调整父节点与其左右关系节点的代价，是$\Theta$(1)。再算上时间代价每个孩子的子树大小至多为2n/3（底层恰好半满就是最坏情况）。我们可以得到一个递归式。 T(n) = T(2n/3) + $\Theta$(1) &emsp;&emsp;使用主定理解一下，能得到T(n) = O(log2n)。所以说，对于一个堆的维护来说，它的时间复杂度与其树的高度有关。&emsp;&emsp;接下来要执行建堆过程，数组元素从$\lfloor$n/2$\rfloor$+1到n都是叶子结点，而叶子结点所形成的堆只有一个元素，不需要调整，那么我们就需要对非叶子结点进行最大堆的调整。我们从$\lfloor$n/2$\rfloor$开始一直到1，逐个调用堆的维护算法，使其形成最大堆。由于过程复杂，时间复杂度的推导就不再写了，把无序数组构建为一个最大堆的时间是线行的，也就是O(n)。&emsp;&emsp;在有了堆的维护和堆的调整之后，就可以进行堆排序算法了，它的本质是一种选择排序。首先将数组初始化为最大堆，这时候根节点就是最大的，然后将根节点与最后一个节点交换，此时最后位置这个节点就需要被排除出堆了，因为他目前已经到了它最终所在的位置（这一操作可以通过n-1来实现）。这个时候的堆可能已经不满足最大对的性质了，所以要进行一次调整，使其继续满足最大堆的特性。然后重复上面的操作，直到最后就剩一个节点。此时这个数组已经被排序了。这一过程非其实就是一个选择的过程，通过最大堆找到堆中最大的元素，将其挑出，然后再次调整为堆大堆，就能够挑出第二大的元素，直到最后就能将所有元素排序，下面是堆排序的过程，并没有排完，但是后面的过程是一样的。 &emsp;&emsp;堆排序过程中，每一次调用建堆的时间复杂度是O(n)，n-1次调用堆维护，每次时间为O(log2n)，所以对排序的时间复杂度为O(nlog2n)。下面是实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#pragma once#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int parent(int i) &#123; return i / 2;&#125;int left(int i) &#123; return 2 * i;&#125;int right(int i) &#123; return 2 * i + 1;&#125;void Maxheapify(vector&lt;int&gt;&amp; A, int i, int length) &#123; int largest; int l = left(i);// 左子节点坐标 int r = right(i);// 右子节点坐标 if (l&lt;length &amp;&amp; A[l]&gt;A[i]) &#123;//用两个判断筛选当前节点与左右子节点的最大值 largest = l; &#125; else &#123; largest = i; &#125; if (r&lt;length &amp;&amp; A[r]&gt;A[largest]) &#123; largest = r; &#125; if (largest != i) &#123;//当前节点与除本身以外的最大值交换 int k = A[i]; A[i] = A[largest]; A[largest] = k; Maxheapify(A, largest, length);//继续向下重复此操作 &#125;&#125;void buildmaxheap(vector&lt;int&gt;&amp; A) &#123;//建立最大堆 int length = A.size(); for (int i = A.size() / 2; i &gt;= 1; i--)&#123; Maxheapify(A, i, length); &#125;&#125;/*堆排序*/void heapsort(vector&lt;int&gt;&amp; A) &#123; buildmaxheap(A); int length = A.size(); for (int i = length - 1; i &gt;= 2; i--)&#123; int k = A[i]; A[i] = A[1]; A[1] = k; length -= 1; Maxheapify(A, 1, length); &#125;&#125; 基数排序&emsp;&emsp;这是一种原来在卡片机上的一种算法，这种算法是按照每一位进行排序的，每一位的取值范围就是0-9这10个数字，每一个数字都代表一个容器。如果按照常规想法都话，一般从在高位开始排序，先按照最高位将数字插入0-9之间的容器中，然后将每个容器中的数据按照第二位再进行这样的排序，这是一个递归操作，直到最后。但是这么做的话一次只能排一个容器里的数据，当一个容器在排序的时候，其他容器都是未使用状态，多出来了许多不必要的临时储存空间，所以这种方式在很久以前内存紧张的时候并不是一个好的算法。&emsp;&emsp;而基数排序则是选择从最低有效位开始进行排序，然后合并到一起。从第一位开始，所有第一位是0的数字排在所有第一位是1的数字之前，所有第一位是1的数字排在所有第一位是2的数字之前……然后对第二位进行重复操作，直到最高位，过程如图所示。&emsp;&emsp;基数排序的算法就是这么直观，但是它的算法效率又是怎样的呢？ 给定n个d位数，其中每一个数位有k个取值，如果基数排序使用的稳定排序方法耗时$\Theta$(n+k)，那么他就可以在$\Theta$(d(n+k))时间内排好序。 &emsp;&emsp;当每一位数字都在0到k-1之间，并且这个k不太大时，可以使用计数排序（不想在写这个计数排序了，这是一个区间范围内的整数排序，它的时间代价是$\Theta$(n+k)，十进制的基数排序中正好k就等于10，很适合采用计数排序对每一位进行排序。对于n个d位数字来说，每一轮排序耗费$\Theta$(n+k)，一共有d轮，所以基数排序总排序时间为$\Theta$(d(n+k))。 对于给定n个b位数的和任何正整数r&lt;=b，如果基数排序使用的稳定排序算法对数据取值区间是0到k到输入进行排序耗时$\Theta$(n+k)，那么他就可以在$\Theta$((b/r)(n+2r))将这些数排好序。 &emsp;&emsp;证明就不写了，脑壳疼……&emsp;&emsp;至于这个选择用不用，我觉得还是别用，它的运行实现可能看上去比其他排序好一些，但是在渐进记号后面的常数因子却大的多，并且他没有快速排序那样可以有效使用硬件缓存，而且他也不是原址排序。具体实现如下。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#pragma once#include &lt;iostream&gt;using namespace std;/*基数排序Input: 数组A[l,h]； 数组中最大元素的位数d，例如最大数为999，则d为3； 进制数k，如果是10进制数，k为10；Output:排序好的数组；Others：对数字1234来说，预定第0位为4，第1位为3，依次类推；*/bool radixsort(int A[], int l, int h, int d, int k) &#123; if (NULL == A || l &gt; h) return false; int size = h - l + 1; int* counts = new int[k];//辅助数据 int* temp = new int[size];//用于存储重新排序的数组 int index; int pval = 1; //依次处理不同的位 for (int i = 0; i &lt; d; i++) &#123; //counts数组清零 for (int j = 0; j &lt; k; j++) counts[j] = 0; for (int j = l; j &lt;= h; j++) &#123; index = (int)(A[j] / pval) % k; counts[index]++; &#125; //计算累加频数 for (int j = 1; j &lt; k; j++) counts[j] = counts[j] + counts[j - 1]; //使用倒数第i+1位数对A进行排序 for (int j = h; j &gt;= l; j--) &#123; index = (int)(A[j] / pval) % k; temp[counts[index] - 1] = A[j]; counts[index]--; &#125; //将按第i为数排序后的结果保存回数组A中 for (int j = 0; j &lt; size; j++) A[j + l] = temp[j]; //更新pval pval = pval * k; &#125; delete[] counts; delete[] temp;&#125; 算法比较&emsp;&emsp;这八种排序算法具体该选择哪一种，很大程度上取决于实际应用场景。下面用一张表来总结以上九种（快速排序分为普通的和三向切分的）排序算法。 算法 是否稳定 是否为原址排序 时间复杂度 空间复杂度 备注 选择排序 否 是 N2 1 插入排序 是 是 介于N和N2之间 1 与元素初始状态有关 希尔排序 否 是 近似于Nlog2N和N6/5 1 目前没有证出来 快速排序 否 是 Nlog2N log2N 有大概率取得最好运行状态 三向切分快速排序 否 是 介于N和Nlog2N之间 log2N 同时取决于概率和元素分布 归并排序 是 否 Nlog2N N 堆排序 否 是 Nlog2N 1 基数排序 是 否 d(N+k) (N+k) 受隐藏常数因子影响太大]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[禹州实训-宠物医院-后端]]></title>
    <url>%2F2019%2F05%2F22%2F%E7%A6%B9%E5%B7%9E%E5%AE%9E%E8%AE%AD-%E5%AE%A0%E7%89%A9%E5%8C%BB%E9%99%A2-%E5%90%8E%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[禹州实训-宠物医院(后端)&emsp;&emsp;今年实训项目是一个只有增删改查的管理系统，没啥难度，后端基本都是体力活，所以SpringBoot加Spring-data-jpa写这个最爽了，前端打算使用Angular和Angular Material构建一下(不想用jsp那种破烂了)，前端东西有点多，所以分开记录(前端太无聊，鸽了)。so，这就是一个前后端分离的项目了，后端目前就只有一个写接口的任务。 项目分析&emsp;&emsp;没啥好分析的，照着老师给的文档来吧。 “爱心”宠物诊所的职员需要使用系统提供的如下功能：浏览诊所的兽医以及他们的专业特长；浏览宠物的主人（即诊所的客户）的相关信息；更新宠物的主人的相关信息；向系统中增加一个新客户；浏览宠物的相关信息；更新宠物的相关信息；向系统中增加一个新宠物；浏览宠物的访问历史记录；向宠物的访问历史记录添加一次访问；此外，诊所的职员在使用系统提供的上述功能之前需要进行登录。当职员不需要使用系统的上述功能时，也可退出系统。 数据库建立&emsp;&emsp;根据提供的ER图，一共需要七张表，其中一张表独立，两张表有多对多关系，四张表相互之间有一对多或者多对一关系，但是经过考虑我决定两张具有多对多关系的表在数据库中依然保持其多对多关系，而另外四张需要外键连接的表在数据库中不再保持外键关系，所有外键都在Service层进行处理，具体关系如图所示。 项目配置&emsp;&emsp;SpringBoot怎么创建就不废话了，先说一下主要的依赖。由于不用写jsp，所有就不需要使用jsp的依赖，那么还剩下的依赖有。 数据库驱动数据源SpringMVCSpring-data-jpa 这四个依赖就够了，所以pox.xml里面就写上1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.edu&lt;/groupId&gt; &lt;artifactId&gt;zzuli&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;zzuli&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;11&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springmvc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--数据源--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; &emsp;&emsp;依赖写完之后，就开始写配置文件，配置文件还是使用yaml格式123456789spring: jpa: database: mysql properties: hibernate: hbm2ddl: auto: update show_sql: true format_sql: true 正常情况下配置文件里应该还有数据源配置，但是不知道为什么这一回写在配置文件中没有用了，所以单独写一个配置类Config.java123456789101112131415161718192021package cn.edu.zzuli;import com.alibaba.druid.pool.DruidDataSource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @author 程佩 * @datatime 2019/5/14 11:04 */@Configurationpublic class Config &#123; @Bean public DruidDataSource druidDataSource() &#123; //Druid 数据源配置 DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName("com.mysql.cj.jdbc.Driver"); dataSource.setUrl("jdbc:mysql://你的ip/pet?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true");//mysql8.0新写法 dataSource.setUsername("root"); dataSource.setPassword("123456”); //初始连接数(默认值0) dataSource.setInitialSize(8); // 至此，所有项目需要的依赖和配置已经设置完毕。 Entity类设计&emsp;&emsp;这些类中包含了所有数据库需要的实体类，对应那七张表。本来这个包准备起名叫domain呢，很大程度上成了一个行业的约束，domain就代表了数据库映射。但是前几天看了一篇文章指出，在Java传统web开发中，通常我们写的所谓domain都是一些贫血模型，并没有领域模型的行为，所以这些类都只能是一个entity对象，并非领域对象，所以我把包名改成了entity。 Employ类&emsp;&emsp;这个类中包含员工信息，里面有一个主键自增长的id，一个名字，一个密码，下面是代码，get和set方法省略。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import java.io.Serializable;/** * @author 程佩 * @datatime 2019/5/14 10:12 */@Entity@Table(name = "Employ")public class Employ implements Serializable &#123; @Id//主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//自增长 @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "password") private String password; @Column(name = "isactive") private Integer isactive;//这个目前没用 /*getter and setter*/&#125; &emsp;&emsp;全部采用注解的方式，下面所有的表都是这样。 Specialties与Vets类设计&emsp;&emsp;这两个类分别是特长类和兽医类，因为存在多对多关系，所以放在一起。其中兽医类维护两个实体类之间的关系。123456789101112131415161718192021222324package cn.edu.zzuli.domain;import com.fasterxml.jackson.annotation.JsonIgnoreProperties;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 14:30 */@Entity@Table(name = "vets")public class Vets &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @JsonIgnoreProperties(value = "vetsList") @ManyToMany//多对多映射 @JoinTable(name = "vets_specialties",joinColumns = @JoinColumn(name = "vets_id"), inverseJoinColumns = @JoinColumn(name = "specialties_id"))//外键关联 private List&lt;Specialties&gt; specialtiesList; /*getter and setter*/&#125; &emsp;&emsp;主要记录一下@JsonIgnoreProperties(value = &quot;vetsList&quot;)这一句，因为不加这一句的话，在多对多查询中，两个类中的成员变量都会有对方的列表，这样会造成无限循环查询，加上这一句是为了避免内层再次查询，相应的，特长类中也会有这一行注解，下面是特长类的代码。12345678910111213141516171819202122package cn.edu.zzuli.domain;import com.fasterxml.jackson.annotation.JsonIgnoreProperties;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 14:32 */@Entity@Table(name = "specialties")public class Specialties &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @JsonIgnoreProperties(value = "specialtiesList") @ManyToMany(mappedBy = "specialtiesList") private List&lt;Vets&gt; vetsList; /*getter and setter*/&#125; Owner、Pet、Type、Visits类设计&emsp;&emsp;这四张表分别对应主人信息、宠物信息、宠物类型、病历信息，因其存在外键关系，所以这四个表放一起讲。&emsp;&emsp;Owner之中有主键、主人姓名、电话、家庭住址四个信息，所以实体类很简单。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 11:18 */@Entity@Table(name = "owner")public class Owner &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "address") private String address; @Column(name = "telephone") private String telephone; /*getter and setter*/&#125; &emsp;&emsp;Pet比较复杂，因为他跟两个表之间有这多对一关系，所以要记录关联信息，为了方便查询，我不止关联了主键信息，还关联了一些必要的名称信息。123456789101112131415161718192021222324252627package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.Date;/** * @author 程佩 * @datatime 2019/5/14 11:40 */@Entity@Table(name = "pet")public class Pet &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; @Column(name = "brith_date") private String brithDate; @Column(name = "type_id")//与type关联 private Integer typeId; @Column(name = "type_name") private String typeName; @Column(name = "ownerId")//与owner关联 private Integer ownerId; @Column(name = "owner_name") private String ownerName; /*getter and setter*/ &emsp;&emsp;Type十分简单，除了主键之外只有一个name信息。123456789101112131415161718package cn.edu.zzuli.domain;import javax.persistence.*;import java.util.List;/** * @author 程佩 * @datatime 2019/5/14 11:35 */@Entity@Table(name = "type")public class Type &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "name") private String name; /*getter and setter*/&#125; &emsp;&emsp;Visits和Pet之间存在多对一关系。12345678910111213141516171819202122package cn.edu.zzuli.domain;import javax.persistence.*;import javax.xml.crypto.Data;/** * @author 程佩 * @datatime 2019/5/14 14:22 */@Entity@Table(name = "visits")public class Visits &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Integer id; @Column(name = "visit_date") private String visitDate; @Column(name = "description") private String description; @JoinColumn(name = "pet_id") private Integer petId; /*getter and setter*/&#125; 项目将一启动，这七个表就会被创建了。 JPA设计&emsp;&emsp;JPA使用非常简单，只要继承JpaRepository接口就能使用一套已经写好的增删改查接口，但是自动生成可能无法满足自己的需求，所以需要在里面再加几个方法。(由于急着玩游戏学习，就不写基类JPA，反正就继承这一个接口，分页什么的扔前端了) PetJPA&emsp;&emsp;再这个接口中我想添加根据类型ID查询宠物接口、根据主人ID查询宠物接口，根据主人的ID更改所有属于该主人的宠物中的主人姓名的接口，在没有使用QueryDSL的时候，我觉得最好在接口中自己用HQL或者原生SQL编写接口、需要注意的是在修改和删除操作的时候要加上事务。修改操作的时候要加@Modifying注解。123456789101112131415161718192021package cn.edu.zzuli.jpa;import cn.edu.zzuli.domain.Pet;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Modifying;import org.springframework.data.jpa.repository.Query;import javax.transaction.Transactional;import java.util.ArrayList;/** * @author 程佩 * @datatime 2019/5/14 14:47 */public interface PetJPA extends JpaRepository&lt;Pet,Integer&gt; &#123; @Query("select pet from Pet pet where pet.typeId = ?1") public ArrayList&lt;Pet&gt; getPetByType(Integer id); @Query("select pet from Pet pet where pet.ownerId = ?1") public ArrayList&lt;Pet&gt; getPetByOwner(Integer id); @Transactional @Modifying @Query("update Pet pet set pet.ownerName = ?1 where pet.ownerId = ?2") public void updatePetByOwner(String name, Integer id);&#125; 下面只写添加的方法，不再写整个接口了。 OwnerJPA&emsp;&emsp;OwnerJPA只需要添加一个模糊查询接口。12@Query(value = "select o from Owner o where o.name LIKE CONCAT('%',?1,'%') ")public ArrayList&lt;Owner&gt; selectByNamelike(String name); EmployJPA&emsp;&emsp;EmployJPA里面只需要一个根据姓名查询，但是这里就不需要写查询语句了，jpa接口中是可以根据接口名称来自动实现接口，比如下面这个写法，就能自动解析为查询语句，不过只适用于简单查询，复杂查询写法太过繁琐。1Employ findByName(String name); &emsp;&emsp;SpecialtiesJPA和VetsJPA都有这个方法，就不重复写。 VisitsJPA&emsp;&emsp;VisitsJPA里面需要一个根据宠物ID查询宠物信息的方法，如下所示。12@Query("select v from Visits v where v.petId = ?1") public ArrayList&lt;Visits&gt; getListByPet(Integer id); &emsp;&emsp;数据库操作已经编写完毕，接下来就是编写Service接口了。 Service接口设计&emsp;&emsp;关于Service层的设计也很简单，俗话说代码就是最好的注释，所以这里我并没有写太多注释，只放接口（从接口名完全可以看出来接口的功能），实现类放在github上。 EmployService123456789/** * @author 程佩 * @datatime 2019/5/14 16:28 */public interface EmployService &#123; public Employ SearchEmploy(Employ employ); public boolean Verify(Employ employ); public Employ Logon(Employ employ);&#125; OwnerService12345678910111213/** * @author 程佩 * @datatime 2019/5/14 18:30 */public interface OwnerService &#123; public ArrayList&lt;Owner&gt; ownerList(); public ArrayList&lt;Owner&gt; searchByName(String name);//模糊查询 public Optional&lt;Owner&gt; searchById(Integer id); public Owner addOwner(Owner owner); public Owner updataOwner(Owner owner); public Owner deleteOwner(Owner owner); public void deleteOwnerById(Integer id);&#125; PetService12345678910111213/** * @author 程佩 * @datatime 2019/5/14 20:45 */public interface PetService &#123;public Pet addPet(Pet pet);public void deletePet(Integer id);public Pet uptataPet(Pet pet);public ArrayList&lt;Pet&gt;getPetList();public Optional&lt;Pet&gt; getPetById(Integer id);public ArrayList&lt;Pet&gt; getPetByType(Integer id);public ArrayList&lt;Pet&gt; getPetByOwner(Integer id);&#125; TypeService12345678910/** * @author 程佩 * @datatime 2019/5/14 20:19 */public interface TypeService &#123; public Type add(Type type); public void delete(Integer id); public Type updata(Type type); public ArrayList&lt;Type&gt; getlist();&#125; Vets_SpService&emsp;&emsp;这个接口要说明一下，因为Specialties与Vets两个表是多对多关系，操作时通常为关联查询，为了方便起见，就把他们两个的服务接口放在一起。1234567891011/** * @author 程佩 * @datatime 2019/5/15 11:02 */public interface Vets_SpService &#123; public ArrayList&lt;Vets&gt; getVetsList(); public ArrayList&lt;Specialties&gt; getSpecialtiesList(); public void insert(SPSet set) throws Exception; public Optional&lt;Vets&gt; getVets(Integer id); public Optional&lt;Specialties&gt; getSpecialties(Integer id);&#125; VisitsService123456789101112/** * @author 程佩 * @datatime 2019/5/15 9:22 */public interface VisitsService &#123; public Visits add(Visits visits); public void delete(Integer id); public Visits updata(Visits visits); public ArrayList&lt;Visits&gt; getlist(); public ArrayList&lt;Visits&gt; getListByPet(Integer id); public Optional&lt;Visits&gt; getVisitById(Integer id);&#125; Optional&emsp;&emsp;在上面的接口中，可以看见很多返回Optional的接口，因为Spring-data-jpa在进行单个对象查询的时候不会只返回对象，而是返回了一个Optional类，这是一个再jdk8中引入的很有趣的特性，它的存在就是为了解决令许多java程序员头疼的的空指针异常。本质上，这是一个包含有可选值的包装类，这意味着 Optional 类既可以含有对象也可以为空。可以说是java迈向函数式编程的很强劲的一步，不过在这里不是重点，不多说。 Controller层设计&emsp;&emsp;这一层是直接和前端进行交互的，比较重要。但是这个项目本身并没有什么东西，所以Controller层设计还是很简单的。 EmployController12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * @author 程佩 * @datatime 2019/5/14 16:51 */@RestController@RequestMapping(value = "/employ")public class EmployController &#123; @Autowired private EmployService employService; /** *@Author:程佩 *@Description:验证用户信息 *@Date:2019/5/14_17:19 */ @CrossOrigin @RequestMapping(value = "verify",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; Verify(@RequestBody Employ employ)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); boolean flag = employService.Verify(employ); if (flag)&#123; map.put("state","1"); map.put("message","登陆成功！"); &#125;else &#123; map.put("state","0"); map.put("message","密码或账号不正确！"); &#125; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; /** *@Author:程佩 *@Description:注册账号 *@Date:2019/5/14_17:23 */ @CrossOrigin @RequestMapping(value = "logon",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; LogOn(@RequestBody Employ employ)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); Employ searchEmploy = employService.SearchEmploy(employ); if (searchEmploy == null)&#123; employService.Logon(employ); map.put("state","1"); map.put("message","注册成功！"); &#125;else &#123; map.put("state","0"); map.put("message","账号已存在！"); &#125; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125;&#125; &emsp;&emsp;说一下这个ResponseEntity，其实本来直接使用HashMap也行，因为做post提交这种非幂等操作，每次操作结果都不一样，反正返回一个状态让前端知道后面操作是个什么情况就行，但是我在浏览Spring文档的时候发现了这么一个类，仔细看了看才知道它可以返回一整个http响应，包含状态码、头部信息以及相应体内容，显然比只返回一个json串更合理，索性就用它了，具体怎么用在代码里体现的也有。&emsp;&emsp;再说一下@CrossOrigin，这个注解可能在以前很少遇到，因为之前的jsp开发，静态资源和后端代码耦合比较严重，都放在一个服务器上。但是在前后端分离的情况下，由于不在一个服务器上，就会产生跨域问题，所谓跨域也就是浏览器对于同源策略（就是同一个域）的限制，浏览器会阻止服务器（域）之间的交流，协议、域名、端口其中之一不同就是跨域，一种解决办法就是使用nginx做反向代理，强行使两个服务器在同一个域中，但是这个有点麻烦（这个项目太小了，没有必要）。另一个方法就是Spring中给的@CrossOrigin注解，原理就是向请求中添加Access-Control-Allow-Origin等信息解决跨域问题。&emsp;&emsp;下面的Controller类如果没有特别的地方就不再做详细说明，只贴代码，因为方法名已经把该接口的作用描述得够清楚类了，多写也无益。 OwnerController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * @author 程佩 * @datatime 2019/5/14 19:06 */@RestController@RequestMapping(value = "/owner")public class OwnerController &#123; @Autowired private OwnerService ownerService; @CrossOrigin @RequestMapping(value = "getlist",method = RequestMethod.GET) public ArrayList&lt;Owner&gt; getOwnerList()&#123; return ownerService.ownerList(); &#125; @RequestMapping(value = "getlistbyname",method = RequestMethod.GET) public ArrayList&lt;Owner&gt; getOwnerListByName(String name)&#123; return ownerService.searchByName(name); &#125; @CrossOrigin @RequestMapping(value = "getbyid/&#123;id&#125;" ,method = RequestMethod.GET) public Optional&lt;Owner&gt; getOwnerById(@PathVariable("id") Integer id)&#123; return ownerService.searchById(id); &#125; @CrossOrigin @RequestMapping(value = "add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; addOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.addOwner(owner); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updateOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.updataOwner(owner); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "delete",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deleteOwner(@RequestBody Owner owner)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.deleteOwner(owner); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "deletebyid/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deleteOwnerById(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; ownerService.deleteOwnerById(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125;&#125; PetController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * @author 程佩 * @datatime 2019/5/15 8:39 */@RestController@RequestMapping(value = "/pet")public class PetController &#123; @Autowired private PetService petService; @CrossOrigin @RequestMapping(value = "add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; addPet(@RequestBody Pet pet)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.addPet(pet); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "delete/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; deletePet(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.deletePet(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updataPet(@RequestBody Pet pet)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; petService.uptataPet(pet); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "getlist" ,method = RequestMethod.GET) public ArrayList&lt;Pet&gt; getPetList()&#123; return petService.getPetList(); &#125; @CrossOrigin @RequestMapping(value = "get/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Pet&gt; getPetListById(@PathVariable("id") Integer id)&#123; return petService.getPetById(id); &#125; @CrossOrigin @RequestMapping(value = "getbyowner/&#123;id&#125;",method = RequestMethod.GET) public ArrayList&lt;Pet&gt; getPetListByOwner(@PathVariable("id") Integer id)&#123; return petService.getPetByOwner(id); &#125; @CrossOrigin @RequestMapping(value = "getbytype/&#123;id&#125;") public ArrayList&lt;Pet&gt; getPetListByType(@PathVariable("id") Integer id)&#123; return petService.getPetByType(id); &#125;&#125; TypeController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author 程佩 * @datatime 2019/5/14 20:29 */@RestControllerpublic class TypeController &#123; @Autowired private TypeService typeService; @RequestMapping(value = "/type",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; add(@RequestBody Type type)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.add(type); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "/type/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; delete(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.delete(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @RequestMapping(value = "/type",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updata(@RequestBody Type type)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; typeService.updata(type); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/type",method = RequestMethod.GET) public ArrayList&lt;Type&gt; get()&#123; return typeService.getlist(); &#125;&#125; &emsp;&emsp;关于TypeController(宠物类型控制器)，我写了一个符合rest(表现层状态转换)规范的接口，通过一个统一的接口/type用来通讯，使用GET、POST、PUT、DELETE对资源进行交互。 Vets_SpController1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 程佩 * @datatime 2019/5/15 11:39 */@RestController@RequestMapping(value = "/vetsp")public class Vets_SpController &#123; @Autowired private Vets_SpService service; @CrossOrigin @RequestMapping(value = "getvetslist",method = RequestMethod.GET) public ArrayList&lt;Vets&gt; getVetsList()&#123; return service.getVetsList(); &#125; @RequestMapping(value = "getsplist",method = RequestMethod.GET) public ArrayList&lt;Specialties&gt; getSpecialtiesList()&#123; return service.getSpecialtiesList(); &#125; @RequestMapping(value = "getvetbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Vets&gt; getVetsById(@PathVariable("id") Integer id)&#123; return service.getVets(id); &#125; @RequestMapping(value = "getspbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Specialties&gt; getSpById(@PathVariable("id") Integer id)&#123; return service.getSpecialties(id); &#125; @CrossOrigin @RequestMapping(value = "/insert",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; insert(@RequestBody SPSet set)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; System.out.println(set.getVname()+" "+set.getSname()); service.insert(set); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125;&#125; VisitsController123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@RestController@RequestMapping(value = "/visits")public class VisitsController &#123; @Autowired private VisitsService visitsService; @CrossOrigin @RequestMapping(value = "/add",method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; add(@RequestBody Visits visits)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.add(visits); map.put("state","1"); map.put("message","添加成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","添加失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/delete/&#123;id&#125;",method = RequestMethod.DELETE) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; delete(@PathVariable("id") Integer id)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.delete(id); map.put("state","1"); map.put("message","删除成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","删除失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/updata",method = RequestMethod.PATCH) public ResponseEntity&lt;Map&lt;String,Object&gt;&gt; updata(@RequestBody Visits visits)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;(); try &#123; visitsService.updata(visits); map.put("state","1"); map.put("message","更新成功!"); &#125;catch (Exception e)&#123; e.printStackTrace(); map.put("state","0"); map.put("message","更新失败!"); &#125;finally &#123; return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(map, HttpStatus.OK); &#125; &#125; @CrossOrigin @RequestMapping(value = "/getlist",method = RequestMethod.GET) public ArrayList&lt;Visits&gt; getlist()&#123; return visitsService.getlist(); &#125; @CrossOrigin @RequestMapping(value = "/getlistbypet/&#123;id&#125;",method = RequestMethod.GET) public ArrayList&lt;Visits&gt; getlist(@PathVariable("id") Integer id)&#123; return visitsService.getListByPet(id); &#125; @CrossOrigin @RequestMapping(value = "/getbyid/&#123;id&#125;",method = RequestMethod.GET) public Optional&lt;Visits&gt; getbyid(@PathVariable("id") Integer id)&#123; return visitsService.getVisitById(id); &#125;&#125; 结尾&emsp;&emsp;总体来说这就是一篇流水账……源代码开放在github上-&gt;地址。还有，这回禹州的网没以前好，打星际还凑合。餐厅里唯一的一家杂炣没了，比较难受。银梅可乐挺好喝。没了。]]></content>
      <categories>
        <category>禹州实训</category>
      </categories>
      <tags>
        <tag>禹州实训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker部署静态网页]]></title>
    <url>%2F2019%2F04%2F28%2F%E4%BD%BF%E7%94%A8docker%E9%83%A8%E7%BD%B2%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[使用docker部署静态网页&emsp;&emsp;Docker是一个使用Go语言开发的容器引擎，可以将应用以及依赖环境打包进容器中，容器是可以移植到任何操作系统上(一般是Linux)，容器与容器之间是完全隔离的，容器之间不会有任何接口。&emsp;&emsp;别的也就不多说了，反正网上一大片讲Docker的好处，这次笔记主要是记录一下使用Docker部署一下一个简单的静态网页，开发环境是MacOS，Docker已下载好，既然是部署静态网页，那么首选必然是Nginx，这个服务器既轻，并发量又大。 1. 获取Nginx&emsp;&emsp;Docker官方已经准备好了十万多个模板镜像，就在模板镜像库里，Nginx必然也会在模板镜像库中存在，所以没必要自己下载Nginx去安装，配置好Docker之后直接执行指令。 docker pull nginx 简单粗暴，直接从镜像库中拉出Nginx。 2. 启动镜像&emsp;&emsp;依旧只有一条指令。 docker run -p 8080:80 -d nginx &emsp;&emsp;这里使用的docker run命令相当于先执行了docker create指令，后执行了docker start指令，可以说是先创建再执行的指令。&emsp;&emsp;要注意的是，里面的80:80代表将虚拟机(镜像)的80端口(Nignx使用的是80端口)映射到本机的8080端口，也可以映射到其他端口，冒号前面是本机端口，冒号后面是虚拟机端口，这个参数是必须有的(没有的话访问不到虚拟机)。&emsp;&emsp;可以使用docker ps指令去查看目前正在运行的虚拟机，如下图所示。&emsp;&emsp;图中显示了正在运行的虚拟机，可以看见主机的8080端口映射进了Docker的Nginx虚拟机的80端口，但是还可以看见主机的80端口也被映射了进来，这个是另一个东西，一会会说到。这回已经可以在浏览器访问本机的8080端口了，不出意外的话就可以看到熟悉的welcome to nginx!我的虚拟机里已经扔进去东西了，所以就不截图了。 3. 部署静态文件&emsp;&emsp;有两种方法把静态文件扔进容器中，一个是通过命令行。 docker cp index.html ec8aca4bc4f2://usr/share/nginx/html 利用cp指令将当前已经写好的文件扔进容器中nginx的html文件中，ec8aca4bc4f2是容器的一个标签，可以在刚才查看容器的地方找到。执行 docker exec -it ec8aca4bc4f2 bash 可以将命令行转到容器的伪终端中，能够查看容器中是否存在刚刚拷贝进去的静态文件。退出的时候用Ctrl+P+Q，不然退出时会把容器关闭。(这个方法网上说管用，但是不知道为啥我这里没用)&emsp;&emsp;另外一个方法就是采用文件映射，使用 docker run -p 80:80 -d -v $PWD/html://usr/share/nginx/html nginx 这个的意思是将主机目录映射到容器，我是将当前路径下(此时命令行所在路径)的html文件夹映射到虚拟机中/usr/share/nginx/html路径中，刚才截图中的第二个运行的另一个映射到80端口的东西就是他了，这样我们修改本地当前目录的html文件夹就相当于修改对应映射路径的文件夹了。 4. 使用Dockerfile构筑镜像&emsp;&emsp;这个其实也是部署静态文件的一种方法，不过我觉得比较具有代表性，就单独写一下。&emsp;&emsp;我们可以使用Dockerfile配置文件定制镜像，这个方法是我最推荐的，这里我用官方提供的nginx为模板来为我的Angular项目定制一个镜像。首先先用ng build -prod(react和vue的是啥我不知道)为项目生成dist文件夹，在dist文件夹下(因为项目在这里面，Dockerfile需要被导入文件在同一目录下)创建Dockerfile文件。 FROM nginx:1.15.9-alpineCOPY angular-routing-demo /usr/share/nginx/html/EXPOSE 80CMD [“nginx”, “-g”, “daemon off;”] 大致意思就是把nginx:1.15.9-alpine作为基础镜像，把文件扔到对应路径中，监听容器内的80端口，并且指定容器启动时的默认指令(这里设定的是Nginx的启动指令)。随后执行 docker build -t angular-demo . &emsp;&emsp;注意，指令最后面有一个”.”，我就是因为这个东西被坑了几个小时，执行完指令后，执行docker images指令就可以看到我们定制的镜像了。 随后使用刚才的创建并启动容器的指令，打开浏览器，访问localhost:8080(我把本地8080端口映射进容器的80端口了)，就能够直接访问到我的项目。&emsp;&emsp;Docker最大的好处就是任何环境下都能跑，不需要考虑环境，做到生产环境与开发环境一致，接下来就把我新建的容器移植到Windows下试试，执行 docker save -o angular-demo.tar angular-demo 这个指令是将指定镜像进行打包，如果要打包容器的话需要把save改成export。通用格式是 docker save -o dnsmasq.docker image_name -o指令是输出，dnsmasq.docker 是打包出来的文件，image_name是镜像名。上一条指令执行完之后会在执行指令的路径下生成angular-demo.tar文件，将这个文件拷贝到Windows下，其实完全可以上传到Docker Hub或者自己在服务器创建的本地仓库中，再用其他电脑拉下来，也省了用u盘了，不过重点不在这，不讨论太多。&emsp;&emsp;在Windows下使用命令行(貌似有图形化界面可以用，但是我喜欢命令行)，执行指令 docker load -i angular-demo.tar Windows上的Docker就会加载这个镜像，接着再使用上面的创建并启动命令，然后访问localhost:8080 完美运行，期间没有任何改动。 5. 最后&emsp;&emsp;本来只是为了Angular和SpringBoot移植用的，因为开发环境与生产环境肯定不是一样的，想起来我寒假的时候还在我的Mac里装了一个这玩意，而且还挺好用，于是决定入这个坑，往后要好好总结一下关于Docker的学习过程。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Django-rest-framework开发总结-项目搭建]]></title>
    <url>%2F2019%2F02%2F28%2FDjango-rest-framework%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93-%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Django-rest-framework开发总结-项目搭建写在前面&emsp;&emsp;我第一次开发web应用用的就是Django，甚至比最熟悉的Java还早，目前开发了两个应用，一个报名系统，一个考试系统，不敢说对这个框架很熟悉吧，但是也是有一些了解的，毕竟踩过不少坑，一个bug改三天的事也经常干，但是自从开了博客以来却没有总结过任何关于Django的文章，借着这次开发的机会总结一下，就当是做笔记了。这一回开发的并不是普通的Django应用，而是仅仅提供rest风格数据接口的服务接口的应用，这一过程中除了后台展示界面以外，我并没有参加关于任何前端的开发，目前来讲这也是一种国际上的主流用法，也是前后端分离的大趋势，有利于微服务的构建。 Django-rest-framework Django REST framework is a powerful and flexible toolkit for building Web APIs.Some reasons you might want to use REST framework: The Web browsable API is a huge usability win for your developers. Authentication policies including packages for OAuth1a and OAuth2. Serialization that supports both ORM and non-ORM data sources. Customizable all the way down - just use regular function-based views if you don’t need the more powerful features. Extensive documentation, and great community support. Used and trusted by internationally recognised companies including Mozilla, Red Hat, Heroku, and Eventbrite. &emsp;&emsp;这是Django-rest-framework官网给出的描述，大概意思就是这是一个构造webapi的应用，可以将数据序列化传输。虽然本身Django也有序列化的功能，但是并不怎么好用，接下来就来记录一下怎么搭建一个Django-rest-framework应用。&emsp;&emsp;我用的是macOS系统，工具用的是Pycharm，不过无所谓，反正python是一个跨平台语言，一切第三方库都是用Pycharm中的工具安装(其实还是pip，只不过是可视化界面安装)。 首先打开工具，选择Django工程，右边设置项目名称，选择环境，默认就是创建一个虚拟环境，这个一般不用动，这个滚动条往下拉，有一个Application name，这个是在使用Pycharm创建Django的时候可以直接创建一个app,这里面是填名字的，如果不填就不会创建，开始Django应用之后可以使用命令行创建，Template language是选择模板语言，因为我是创建webapi应用，所以这个不用管，然后点击create，开始下载Django，如果下载不成功可能是网速或者pip版本过低导致，换个网或者升级一下pip就行。&emsp;&emsp;首先在开始之前，我们要先干一件事情，因为pip默认是从外网服务器下载第三方包，所以一般比较慢，特别是用校园网有时会有波动导致无法连接，所以我们要先更改一下数据源，推荐使用清华大学或者豆瓣的数据源。直接点开左上角的Pycharm(windows上是file)，点Perference(windows是setting)，到下面这个界面。点开本项目的解释器(Interpreter)，可以看将当前Django在我们的环境之中，然后点下面的小加号，弹出的界面是搜索并安装第三方库的，先不管这个，点开下面的Manage Respositories，点里面的加号，添加https://pypi.tuna.tsinghua.edu.cn/simple/，添加清华的源，再下载的时候就是用清华的镜像源下载，速度比较快一些。然后在刚才那个搜索界面搜索并安装djangorestframework。然后再setting.py文件里向INSTALLED_APPS中添加&#39;rest_framework&#39;。&emsp;&emsp;虽然我对这个很熟悉，但是还是得捋一遍项目结构 &emsp;&emsp;Demo是项目文件，是这个工程的主干，__init__.py不用说了，所有python包的标识符，setting.py里面有各种配置，urls.py是路由文件，wsgi.py是跟服务器相关的，而下面的StudentApp是我创建的一个app，migration文件夹下会存放数据库的迁移文件，admin.py是Django默认提供了一套模板，用户可以将自己的模型用这个文件来注册，apps.py是对这个app的一些设置，model.py里面是模型类，可以直接迁移生成数据库。test.py是测试(我没用过)，views.py是视图，向外提供数据接口的。urls.py 和serializers.py是我自己建的，前一个是为了代码整洁建立的二级路由，后一个是将模型类数据序列化用的。 数据库配置&emsp;&emsp;在app文件夹中的__init__.py文件中添加12import pymysqlpymysql.install_as_MySQLdb() 这就相当于批量导入了，用来链接数据库。接着还要在setting.py中配置一下数据库，因为默认写的配置是sqllite3，但是我们需要mysql，所以要改一下。1234567891011DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'rest', 'USER':'root', 'PASSWORD':'******', 'HOST':'localhost', 'PORT':'3306', &#125;&#125; 在model.py中弄一个很简单的模型类1234567class Student(models.Model): name=models.CharField('姓名',max_length=100,default='no_name') sex=models.CharField('姓别',max_length=50,default='男') sid=models.CharField('学号',max_length=100,default='0') def __unicode__(self): return '%d: %s' % (self.pk,self.name) 嗯，不能再简单了，但是重点不在这里，在下面命令行里执行两条命令python manage.py makemigrations和python manage.py migrate，顺序不能反，前一个是在migrattions生成迁移文件，后一个是执行迁移,生成对应数据表。对了，当前django版本是2.17，不知道是哪个版本开始只支持mysql5.7以上的版本，5.7之前的版本生成的迁移文件没法执行。 序列化与反序列化&emsp;&emsp;然后开始序列化这一个模型类。1234567from rest_framework import serializersfrom .models import *class StudentSerializers(serializers.ModelSerializer): class Meta: model=Student#指定模型类 fields=('pk','name','sex','sid',) 创建一个StudentSerializers继承serializers.ModelSerializer，下面可以指定模型类，这样一配置，框架就会直接将数据库中的数据序列化为json格式传出，也可以接收json格式存入数据库，fields可以指定序列化哪一部分，如果向全部序列化的话直接等于&#39;__all__&#39;。 viewsets&emsp;&emsp;接着在views.py文件夹中写入1234567class StudentViewSet(viewsets.ModelViewSet): #指定结果集并排序 queryset = Student.objects.all().order_by('-pk') #指定序列化的类 serializer_class = StudentSerializers #任何用户可接入 authentication_classes = (AllowAny,) 只有这短短几行，但是现在已经完成了一整套的rest接口。关键的一点就是我这个StudentViewSet所继承的viewsets.ModelViewSet，这个类中包含了所有的接口实现，已经有了一套rest的api(不想使用他生成的接口，可以使用APIView自己写，还是原来的路由语法)，ModelViewSet中包含了：1234567891011121314151617def list(self, request): passdef create(self, request): passdef retrieve(self, request, pk=None): passdef update(self, request, pk=None): passdef partial_update(self, request, pk=None): passdef destroy(self, request, pk=None): pass 分别对应各种操作可以被重写，也可以添加，比如123456789def create(self, request, *args, **kwargs):#如果对面传过来的json串没有值的话，就是一个字符串，如果有值的话，会被作为json解析 '''提交数据验证并保存''' data = request.data anwser = NewTownRang.newtown(data) serializer = NewtownSerializer(data=anwser) if serializer.is_valid(raise_exception=True):#验证表单，如果错误会返回404 serializer.save() return Response(anwser,status=HTTP_200_OK) return Response(serializer.error_messages,status=HTTP_400_BAD_REQUEST) 这是重写了create方法，用来接收提交的数据，注释是我遇见的坑，如果对面传过来的是‘{“id”:1}’,这样的数据，我这边接收到的就是json格式，会解析为字典，如果对面传过来的是‘{“id”:}’，这个数据就是一个字符串。如果保存数据的话，第六行是必须要加的，因为数据要验证准确性，返回的数据也都是json格式，在python里就是字典。12345678@action(detail=True,methods=['get'])def getListById(self,request, *args, **kwargs): '''根据用户的编号(非学号，用户的pk，不是实验的pk)获取其所有信息''' pk = kwargs['pk'] serializer = NewtownSerializer(NewTown.objects.filter(user_id__exact=pk),many=True) data = serializer.data print(data) return Response(data,status=HTTP_200_OK) 这是增加一个方法，注意这个地方上面的注解原来是另外一个写法，网上大多数流传的也是那个方法，不过目前已经被官方定义为过时了，@action是最新的写法，里面有几个参数，detaill如果设置为true，那么生成url的时候会多一个/{id}/，这个是可以接收主键，methods是规定了这个api的请求方式，原来的主键是通过参数中有一个ip获取，不过目前是通过多参数列表获取pk，序列化的时候many=True必须要带，因为一般查询到的数据都是多条。 路由配置&emsp;&emsp;如果是用这个生成的话，就不需向之前一样写路由语法了，直接向app中自己建的urls.py文件夹中添加12345678from rest_framework import routersfrom StudentApp import viewsroute = routers.DefaultRouter()route.register(r'student',views.StudentViewSet)#url请求末端必须以反斜杠结尾urlpatterns = route.urls 这么做就相当于将StudentViewSet中所有的方法都放在了/student/路由下，接着在主文件中的urls.py添加1path('',include('Experimentation.urls')), 现在这个项目就能访问了，如果是在本机访问，只需要点右上角绿色的小箭头运行这个Django项目就行了，如果向通过ip访问的话，需要在setting.py中设置ALLOWED_HOSTS = [&#39;*&#39;]，这个是设置是为了限定请求中的host值，以防止黑客构造包来发送请求.只有在列表中的host才能访问。如果是开启DEBUG模式的时候默认的是本机url地址，就是127.0.0.1或者localhost，如果关闭DEBUG模式然后正式上线的话，这个值必须配置，在这里为了方便使用了通配符*，也就是所有host都可以访问。正式上线严禁使用通配符‘*’，而且必须关掉DEBUG模式(DEBUG = False)。大概就是设置成这样。123456ALLOWED_HOSTS = [ '.example.com', '.zzuli.com',]DEBUG = False 然后可以在命令行中运行python manage.py runserver 0.0.0.0:8000,这样就可以让其他主机使用ip访问数据接口。&emsp;&emsp;接下来用文档生成工具查看一下接口是否生成。&emsp;&emsp;可以看到一个非常标准的增删改查接口已经生成，先写这么多吧，坑还有很多，以后有时间慢慢写。]]></content>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2F2019%2F02%2F18%2F%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[哈希表&emsp;&emsp;在学习Java和Python的时候经常使用的一种数据结构，像HashMap和字典都是封装好的哈希表，特点是他是一种动态储存结构，储存格式为key-value形式，支持INSERT、SEARCH、DELETE、UPDATE等一系列操作，他们的底层都是由各个程序语言的编译器维护出来了一个符号表，其中每个元素的key为任意字符串，与不同语言中现哈希表的实现的key对应。虽然看起来没什么，但是哈希表是实现字典操作的一个非常有效的数据结构，最坏情况下的搜索时间也与链表相同，也就是说他的最坏运行时间是O(n)，然而实际上在合理假设的情况下，哈希表的查找时间甚至达到了O(1)。 &emsp;&emsp;哈希表这种有key 的结构非常像数组中的下标，但是哈希表可以说是数组的推广。普通数组可以直接通过下标进行直接寻址，花费O(1)的时间就可以找到要查找值所在的位置。&emsp;&emsp;但是实际上很少存在这种一个萝卜一个坑的情况，关键字可能有时候并不能满足储存数，这个时候就需要哈希表来替代直接寻址表，哈希表使用后一个长度与实际储存的关键字数目成比例的数组来储存，哈希表中key与value并不是直接对应，而是根据key来计算下标，不过有些key可能会算出来同样的值，后面会说一个连接法来解决这个冲突问题，下面来仔细总结一下这一块内容。 1. 直接寻址表&emsp;&emsp;当key的范围比较小时，我们可以采取一个萝卜一个坑的策略，使用直接寻址法，简单粗暴。&emsp;&emsp;假设现在有一个动态集合，每个元素都是取自于全域U={0，1，2，3，…n}中的一个关键字，n不是很大，而且每个元素的关键字都不一样。我们可以很轻易的用一个数组来模拟一个直接寻址表，记为T[n+1]。其中每一个值对应的位置可以作一个槽，T就相当于全域U，下标为1的槽中就会插入key值为1的元素，下标为1的槽中就会插入key值为1的元素，依此类推。对应关系如图所示。&emsp;&emsp;基于直接寻址表的字典就很简单了，基本就类似于数组一样的操作，这里就贴出《算法导论》上的伪代码，不用C++重写了。12345678DIRECT-ADDRESS-SEARCH(T,k) return T[k]DIRECT-ADDRESS-INSERT(T,x) T[x.key]=xDIRECT-ADDRESS-INSERT(T,x) T[x.key]=NULL 这三种操作每一步耗费的时间都只有O(1)。&emsp;&emsp;有些直接寻址表并不像图中那样把数据放入外部对象中，再由表中指针指向该对象，而是直接把对象放进槽中，节省一部分空间。我们可以使用一个关键字表明这个槽是一个空槽，如果是数组这种结构我们就可以不需要关键字，直接使用下标来获取元素，但是需要用其他方法来确定某个槽为空。 2. 哈希表&emsp;&emsp;其实翻译过来叫散列表，但是说哈希表习惯了，就叫哈希表吧。&emsp;&emsp;直接寻址法有一个很大的缺点，如果你的全域U很大，甚至超过了电脑当前可用内存的范围，这时候划出一个大小为U的数组T不太现实。有时候可能全域U的范围很大，但是实际储存的值却不多，那剩下的都是空值，那这样又会造成内存的极大浪费。&emsp;&emsp;当字典中所有的关键字K与所有可能的关键字全域U小很多时，哈希表所需要的储存空间比直接寻址表要少许多，相应的我们的储存需求也降低到了 $\Theta$(|K|)，但是查找一个元素的时间依旧是只有O(1)，但是这是相对于平均情况来讲，而对于直接寻址法来说，这只是最坏时间，我们在这里相当于用时间来换取空间了。&emsp;&emsp;在直接寻址表中，我们将关键字为k的值放在槽k中，而在哈希表中，我们将值放在槽h(k)中，这里我们使用了哈希函数h来对k值进行计算求得槽的位置，通过这种方式，h可以讲全域中U中所有的值通过计算映射到哈希表T中的槽位上,h(k)就是k的哈希值，哈希函数缩小了原本数组所需的下标范围，把|U|的大小缩减到了实际关键字数量K的大小。如图所示。 2.1 链接法&emsp;&emsp;虽说缩小了全域的范围，但是哈希函数并不能保证每个k值经过计算后都不相同，很有可能如上图所示，k2与k5经过哈希函数计算后被映射到了同一个槽中，一种解决办法就是让哈希函数尽可能的“随机”，但是无论哈希函数怎么“随机”，全域U的大小比数组T的大小大，至少有两个或者多个关键字的哈希值相同，想要避免冲突是不可能的，一方面是使用更好的哈希函数使哈希值尽量随机，另外还有几种方法，先说链接法。&emsp;&emsp;在链接法中，每一个槽中都会有一个指针，作为一个链表的表头，当遇见相同的哈希值时，相同的哈希值都会被放在链表之中，如果该槽中没有元素，这个指针指向null。使用链接法后，哈希表T的字典操作就是另一种写法(还是用伪代码)。12345678CHAINED-HASH-INSERT(T,x) insert x at the head of list T[h(x.key)]CHAINED-HASH-SEARCH(T,K) search for an element with key in list T[h(k)]CHAINED-HASH-DELETE(T,x) delete x from the list T[h(x.key)] 插入的最坏运行时间依旧是O(1)，而且可能要快一些，如果是查找的话，最坏运行时间与表长度成正比，假设这是一个双向链表，删除一个元素的运行时间是O(1)。但是如果是单向链表情况下，删除与查找的渐进运行时间是一样的。 2.2 链接法分析&emsp;&emsp;给定一个能存放n个元素，具有m个槽位的哈希表T，定义T的装载因子$\alpha$为n/m，就是一个链的每个槽位的平均装载数，我们对链接法的分析借助$\alpha$来说明。&emsp;&emsp;首先分析最坏性能，即所有的值都被映射到了一个槽中，那么当前哈希表中只存在一张长度为n的链表，这时候最坏的查找时间为$\Theta$(n)，再加上哈希函数的运算时间，显然，这种情况跟用链表没啥区别，性能比较差。&emsp;&emsp;如果想要尽量避免最坏性能，而且在平均性能附近浮动的话，那么很大部分上就取决于哈希函数h，一个好的哈希函数可以尽可能的将关键字向m个槽中分散。如果一个元素等可能的散列到m个槽中的任意一个，，与其他元素无关，这样可以称为简单均匀散列，期望值为$\alpha$。&emsp;&emsp;假设O(1)时间内计算出哈希值h(k),而查找关键字元素k的时间线性的依赖于表T[h(k)]的长度nh(k)，先不考虑那个计算哈希值的时间，先来看一下查找的期望值，一共就两种情况，要么找到，要么找到不到。 定理一： 在简单均匀散列的假设下，对于用链接法解决冲突的散列表，一次不成功查找的平均时间为$\Theta$(1+$\alpha$) &emsp;&emsp;任何尚未被存储在表中的关键字k都等可能地被散列到m个槽中，然而在查找k的时候，不成功的话去，肯定是查找到了h(k)所在链表T[h(k)]的末尾，期望时间也是如此此时期望长度为E[nh(k)]=$\alpha$(平均情况下每个槽中有$\alpha$个数据，查找不成功时，这$\alpha$个数据肯定都被遍历过)。再带上计算哈希值用的$\Theta$(1)时间，一共用了$\Theta$(1+$\alpha$)。 定理二： 在简单均匀散列的假设下，对于用链接法解决冲突的散列表，一次成功查找所需的平均时间是$\Theta$(1+$\alpha$)。 引理5.1 给定一个样本空间S和S中的的一个事件A，设 XA=I{A}，那么E[XA]=Pr{A}。 这是在《算法导论》第五章概率分析和随机算法中第5.2节指示器随机变量中的一个引理，I{A}是指示器随机变量，它为概率与期望之间的转换提供了一个便利的方法，具体不在此赘述，这不是重点，这是概率论的范畴，在这里引入只是下面证明需要使用这个定理。 &emsp;&emsp;查找成功的情况有点复杂，因为每个链表被被查找到的情况并不是等可能的，某一条链表被查找到的概率与所包含的元素数成正比(因为出现在每一个地方的情况是等可能的，链表越长，概率越大)，但是查找的期望时间并没有改变。&emsp;&emsp;假设要查找的元素存在于表中，且出现在等可能位置，在对元素x的一次成功的查找中，所检查的元素数比x所在的链表中，出现在x前面的元素数多1。在该链表中，出现在x之前的元素都是在x之后插入的，这是因为新的元素都是在表头插入的。为了确定所检查元素的期望数目，对x所在的链表，在x之后插入到表中的期望元素数加一，再对表中n个元素x取平均，设xi 表示插入到表中的第i 个元素，i=1,2,…,n，并设ki=xi.key。对关键字ki和kj，定义指示器随机变量Xij=I{h(ki)=h(kj)}。在简单一致散列的假设下，有Pr{h(ki)=h(kj)}=1/m，从而根据引理(5.1)，有E[Xij]=1/m。于是，在一次成功的查找中，所检查元素的期望数目为&emsp;&emsp;先说一下这一个公式是怎么来的，首先在离散数学里面，计算一个算法在平均状态下的计算复杂性，可以转变成一个变量的期望值，设一个实验的样本空间是可能输入aj(j=1,2,…,n)的集合，且令随机变量X对aj 赋值是aj 作为输入时该算法用到的操作次数。基于我们对输入的了解，对每个可能的输入aj 赋给一个概率p(aj)。那么该算法在平均状态下的复杂性是每个可能的输入的概率为1/n。也就是说x等于表中第i 个元素的概率是1/n,指示器随机变量Xij=I{h(ki)=h(kj)}的含义是如果i 和j 散列在同一个槽位当中，也就是在同一个链表中时，加1次。因此，综上所述，在x之后插入到表中的所检查元素的总数目为：算法在平均状态下的所检查元素的总数目为：&emsp;&emsp;因此，一次成功查找的全部时间(包含哈希函数运算时间$\Theta$(1))为$\Theta$(1+$\alpha$)。&emsp;&emsp;至此我们已经将链接法分析完毕，我们来看一下分析结果，如果哈希表中槽数与表中的元素成正比，则有n=Q(m)，从而a=n/m=O(m)/m=O(1)，所以我们查找需要的是平均常数时间，当链表使用双向链表时，插入操作最坏当情况下需要O(1)时间，删除最坏情况下需要的也是O(1)时间，所以，全部的字典操作都可以在O(1)时间内完成。 3. 哈希函数&emsp;&emsp;这一块应该是在哈希表之下，但是东西太多，只好单独拉出来讲。&emsp;&emsp;哈希表中保证元素能够等可能的映射在每一个槽中，一个好的哈希函数是非常关键的。一个好的哈希函数应该满足简单均匀散列假设：每个关键字都被等可能的散列到m个槽位中的任意一个，并与其他关键字散列到哪个槽位无关。然而实际情况下并没有办法检查这一条是否成立，因为我们一般都不知道关键字散列的具体分布。一般来说都是将关键字转化为自然数，比如ASCII码，然后再实用哈希函数进行处理。下面总结几个常用的计算哈希值方法。 3.1 除法散列法&emsp;&emsp;设k为关键字，m为槽数。计算哈希函数是我们可以用k对m取余数，将关键字k映射到m中对某一个槽中： h(k) = k mod m 比如m = 12，k = 100，那么h(k) = 4,由于只有一次运算，所以速度上还是可以的。&emsp;&emsp;书上说在使用除法散列法时，需要避免一些m的选值，比如m不能取2的幂，因为如果m = 2p，则h(k)就是k的最低p位数字，一开始我根本就不懂这一句话是什意思，后来看了一下英文原版发现如果m = 2p，h(k)的值会等于k的二进制的最低的p位，如果m = 8，就像下面这张从网上找到的图一样。仔细分析一下，如果一个数越是接近2i,其二进制中就会存在大段连续的0和1，取余运算的本质无非是个减去m的n倍的减法。那么做减法的时候，m中间大段的0或1就会让哈希原值的中间一段有非常大的可能性仍然保持原样。哈希函数的本质目的就是混淆，原值的变化产生哈希值无规律、等概率、不可预测、不能逆推的变化为最佳。如果做完哈希运算之后，哈希值和原值中间居然有一大段二进制位保持不变，那么这个哈希函数就可以说是失败到不能再失败了。就着这个哈希表而言，取这样的值在概率上也会让哈希值重复度过高，不符合均匀散列，而m取素数的时候相对来说能让哈希值分布得更均匀一些。所以在设计哈希函数的时候，最好要考虑关键字的所有位。 3.2 乘法散列法&emsp;&emsp;执行一个乘法散列法分两步，第一步让关键字k乘一个常数A(0&lt;A&lt;1)，，提取其结果的小数部分，然后用m乘以这个值，再向下取整。 h(k) = ⌊m(kA MOD 1)⌋ 与除法散列法相比，乘法散列法中m的选择并不关键，一般选为2的幂，比较推荐A的值是 $\dfrac{\sqrt{5}-1}{2}$ = 0.618 033 9887…唯一的缺点就是他生成的哈希函数没有除法散列法那么均匀。 3.3 全域哈希&emsp;&emsp;无论什么样的哈希算法，如果某个恶意的用户针对该哈希算法选择性的输入关键字，那么就会将所有的关键字全部散列到同一个槽中，使得哈希表运行效率变成最低，不论多好的哈希函数都会这样，唯一的方法就是每一次计算使用的都不是一个哈希函数，而是随机的，这种方法称为全域哈希，不管输入什么样的关键字，性能都很好。 4. 开放寻址法&emsp;&emsp;开放寻址法中，所有的元素都被存放在哈希表中，也就是表中的每一项要么是空，要么是一个元素。当要查找的时候，就要检查整张表，直到找或者遍历了整张表。这里没有链表，数据也不会在表外，所以在开放寻址法中，哈希表可能会被填满，以至于无法插入新元素，装载因子$\alpha$绝对不会超过1。&emsp;&emsp;开放寻址法中不需要指针或者对象，只需要计算出槽的存取序列，可以节省原来指针空间以提供更多的槽，减少了冲突，提高了速度。&emsp;&emsp;开放寻址法中如果要插入元素，需要检查哈希表，这个过程叫探查，直到找到一个空的位置插入，检查不一是从头开始按顺序来(这样的执行效率是$\Theta$(n))，而是依赖于待插入的关键字，为了确定探查槽，现在需要加一个参数，对每一个关键字k，都会有一个探查序列，探查序列可能是顺序的，也可能基于某种方法，是0到m-1的一个排列， &lt;h(k,0),h(k,1),…,h(k,m-1)&gt; 如果给定一个关键字k，首先会看h(k,0)是否为空，如果为空，则插入；如果不为空，则看h(k,1)是否为空，以此类推，但是删除的时候比较麻烦，一般必须删除关键字的应用中，链接法更常见。其中具体的操作在后面实现哈希表的时候再说。&emsp;&emsp;有三种方法可以计算开放寻址法中的探查序列，但是都不能满足均匀散列的假设，，因为他们产生的不同的探查序列都不会超过m2个(均匀散列要有m!个)。 4.1 线性探查&emsp;&emsp;h’为辅助散列函数： h(k,i) = (h’(k) + i) mod m i = 0,1,…,m-1 &emsp;&emsp;给定一个关键字k，首先探查槽位T[h’(k)]，然后是T[h’(k) + 1]，直到T[m-1]，再从T[0]开始，直到最后的T[h’(k)-1]，相当于是从初始位置顺序开始探查。在线性探查中，初始探查位置决定了整个序列，所以有m种不同的探查序列。&emsp;&emsp;线性探查有个缺点，就是一次群集。当表中i，i+1，i+2位置上都已经填满时，下一个哈希地址为i，i+1，i+2，i+3的关键字记录都将竞争i+3的位置。随着连续被占用的槽位不断增加，平均查找时间也不断增加。 4.2 二次探查&emsp;&emsp;二次探查的函数： h(k,i) = (h’(k) + c1i +c2i2) mod m i = 0,1,…,m-1 c1c2为辅助常数，初始探查位置为T[h’(k)]，后续的探查位置要在此基础上加上一个偏移量，该偏移量以二次的方式依赖于探查号i。这种探查的效果要比线性探查好。但是，如果两个关键字的初始探查位置相同，那它们的探查序列也是相同的，因为h(k1,0)=h(k2,0)蕴含着h(k1,i)=h(k2,i)，这一性质会导致二次群集。类似于线性探查，二次探查也仅有m个不同的探查序列。 4.3 双重散列&emsp;&emsp;双重散列的函数： h(k,i) = (h1(k) +i*h2(k)) mod m &emsp;&emsp;i = 0,1,…,m-1 &emsp;&emsp;双重散列中有两个辅助函数h1和h2，他的初始位置是T[h1(k)],后续的探查位置在此基础上的偏移量为h2(k)对m取余数。&emsp;&emsp;双重散列比线性探查和二次探查都要好，与前两者不同，双重探查的的探查序列以两种不同的方式依赖于关键字k。为能查找整张表，值h2(k)要与表的大小m互质。确保这个条件成立的一种方法是取m为2的幕，并设计一个总产生奇数的h2。另一种方法是取m为质数，并设计一个总是产生较m小的正整数的h2。因为当m取质数或者取2的幂时，双重散列法会有$\Theta$(m2)种探查序列，比前两个只有$\Theta$(m)种探查序列更接近均匀假设。 &emsp;&emsp;因为证明过程较长，就不再对开放寻址法进行分析了，具体过程在《算法导论》中有详细推导过程。&emsp;&emsp;在网上搜寻有关资料的时候看到了一个比较能清晰解释但是有点恶心的说法，贴上原文链接和文章内容。原文链接：https://blog.csdn.net/shaobingj126/article/details/8156675 &emsp;&emsp;简单地讲，也就是说，一间厕所，来了一个顾客就蹲其对应的位置，如果又来一个顾客，把厕所单间门拉开，一看里面有位童鞋正在用劲，那么怎么办？很自然的，拉另一个单间的门，看看有人不，有的话就继续找坑。当然了，一般来说，这个顾客不会按顺序一个一个地拉厕所门，而是会去拉他认为有可能没有被占用的单间的门，这可以通过闻味道，听声音来辨别，这就是寻址查找算法。如果找遍了所有厕所单间，看尽了所有人的光屁股，还是找不到坑，那么这座厕所就该扩容了。当然了，厕所扩容不会就只单单增加一个坑位，而是综合考虑成本和保证不让太多顾客拉到裤子里，会多增加几个坑位，比如增加现有坑位的0.72倍。为什么是0.72呢，这是所长多年经营所得到的经验值，为了让自己的经验发扬光大，需要出去演讲，又不能太俗，总不能说“厕所坑位因子”吧，那就把把0.72叫做“装填因子”或者“扩容因子”吧。目前很多产品使用0.72这个常数。 5. 完全散列&emsp;&emsp;哈希技术的平均性能非常优异，而且当关键字集合为静态时，还能提供非常棒的最坏性能。所谓静态就是将关键字集合存入表中，不再变化，例如程序语言中的保留字集合就是一个静态集合。有一种哈希方法叫完全散列，使用该方法查找时，最坏的性能也是O(1)。&emsp;&emsp;该方法采用二级散列，而且每一级都是全域哈希，如图所示。&emsp;&emsp;第一级就是一个带链接和哈希表，利用从全域散列函数集合中的一个函数h，将n个关键字放入m个槽中。然后再用一个比较小的二次哈希表Sj和哈希函数hj将其再次映射到二次哈希表，而不是使用连接法建立关键字链表。利用好的哈希函数hj，可以使其再第二级不出现冲突。&emsp;&emsp;为了让第二级不产生冲突，需要让哈希表Sj的大小mj为j中关键字nj的平方。虽然说总体看起来储存需求很大，但是可以适当选择第一级的哈希函数，使预期储存空间限制在O(n)。大致就是如此，还有一些细节就不再阐述了。 &emsp;&emsp;因为有其他事情的原因，这个总结断断续续写了好几天，说实话，还有很多细节没有写出来，但是我觉得说得太细了反而会影响整体。这不算很难的一个数据结构，可以说比较简单，而实现起来也容易，但是分析起来却超过了大多数的数据结构。先消化一段时间，先准备计赛，然后开始树和图的总结。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb安装]]></title>
    <url>%2F2019%2F02%2F05%2Fmongodb%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[mongodb安装&emsp;&emsp;大年初一无聊，玩玩mongodb。 写在前面&emsp;&emsp;再知乎上经常听到大名鼎鼎的nosql类型数据库，相对于关系型数据库来说，这个东西部署很简单，储存方式为虚拟内存+持久化存储，总结起来一个字————快。nosql存起来是key-value形式的，也支持集合和对象但是并不支持事务，也不能连表查询。&emsp;&emsp;传统的数据库体系非常成熟，可以进行事务操作，能用SQL语句进行复杂操作，不过当数据量极大时，关系型数据库效率会大打折扣，我猜一个很大的原因是外键的关系，查询中一系列组合索引会消耗很长时间，好多公司会针对这种情况进行分库分表，阿里巴巴开发手册里也说明尽量不要使用外键，将所有的关联事务在服务层解决。But，这种情况我从来没遇见过，我这种小菜鸡目前写的东西是肯定没有这么多用户的，这些都是我听说的。&emsp;&emsp;而内存级数据库就厉害了，利用内存的io性能可以进行极速读写和查询，虽然不能有SQL进行复杂的查询，但是mongodb能写JavaScript啊，总比那辣鸡SQL舒服多了，除了没有事务之外，其他基本就不是事了。废话不说，下面讲安装。 安装mongodb OS: MacOS 安装homebrew&emsp;&emsp;MacOS包管理工具，可以使用brew命令安装工具，和Ubuntu下的apt-get命令十分类似，不过MacOS中并没有自带homebrew，需要自己进行下载。 /usr/bin/ruby -e “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 很简单，直接输入这一条指令就行了，不过必须要有Xcode才能运行成功，下载时如果发现没有Xcode会提示安装，期间需要输入密码。 使用homebrew安装mongodb&emsp;&emsp;其实还有其他的方法可以安装mongodb，比如直接从官网下载，不过需要自己配置环境变量和写配置文件，然而我比较懒，使用homebrew安装的mongodb是自动配置好环境变量的，所以就使用这种方法了。 brew install mongodb 简单粗暴，一个命令就装完了，不过需要点配置，先在根目录(就是跟主机名一样的那个目录)下，创建/data/db文件夹，现在直接输入一个mongod指令是可以直接打开数据库的，不过会报Warning，因为mongodb是可以不需要用户就可以进入的，但是会对你进行警告，所以现在要添加一个账户。先进入数据库。输入指令,出现>就代表进入了数据库指令界面。 >use admin 切换到admin。 >db.createUser( { user: “admin”, //用户名 pwd: “passwd”, //密码 roles: [ { role: “userAdminAnyDatabase”, db: “admin” } ] //设置权限 }) 添加用户，权限为userAdminAnyDatabase，可以管理所有用户。然后可以执行命令 mongod –auth –port 27017 –dbpath ~/data/db --auth表示需要用户验证，--port表示端口号，默认就是27017，--dbpath表示数据库存储位置。 再开启一个命令行界面，输入指令mongo，在之前的命令行界面可以看到。 2019-02-05T22:32:28.130+0800 I NETWORK [listener] connection accepted from 127.0.0.1:49902 #1 (1 connection now open) 说明接入了一个服务。接着输入指令。 > use adminswitched to db admin> db.auth(‘root’,’root’);1 切换到admin之后用db.auth(“用户名”,”密码”);就可以进入用户了，1表示成功，主命令行会出现如下内容，表示认证成功。 2019-02-05T22:35:38.116+0800 I ACCESS [conn1] Successfully authenticated as principal admin on admin 这就算打开了，不过还要知道这个数据库怎么关，根据网上的资料来说，不能直接强制关闭，需要在界面中执行db.shutdownServer();命令，于是我愉快的执行了这个指令，结果…… assert failed : unexpected error: Error: shutdownServer failed: not authorized on admin to execute command { shutdown: 1.0 }Error: assert failed : unexpected error: Error: shutdownServer failed: not authorized on admin to execute command { shutdown: 1.0 } at Error () at doassert (src/mongo/shell/assert.js:11:14) at assert (src/mongo/shell/assert.js:20:5) at DB.shutdownServer (src/mongo/shell/db.js:212:9) at (shell):1:4 emmmm……他给我抛了个异常，费了劲跑到了官网查看一下官方文档，发现网上那些教程净瞎写，一篇破教程被十几个人转发还没人发现不对，也是服气。原来在mongodb并不像mysql一样有一个可以拥有一切权限的admin用户，而且可以看一下mongodb也根本没有一个admin库，所谓userAdminAnyDatabase权限也就是可以管理所有用户，并没有权限关闭数据库，所以我们要给这个用户添加可以shutdown角色hostManger。 > db.grantRolesToUser( “admin” , [ { role: “hostManager”, db: “admin” } ]) 现在进入admin登录用户后运行db.shutdownServer(); server should be down…2019-02-05T22:50:38.485+0800 I NETWORK [js] trying reconnect to 127.0.0.1:27017 failed2019-02-05T22:50:38.487+0800 I NETWORK [js] reconnect 127.0.0.1:27017 failed failed 这就已经关闭了，同时也可以看见运行mongodb的那个命令行进程也关闭了。&emsp;&emsp;还有要运行下面这一段命令，添加执行命令权限，不然什么都干不了。 db.grantRolesToUser ( “admin”, [ { role: “__system”, db: “admin” } ] ) &emsp;&emsp;具体怎么用再说吧，等后面学习总结。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>nosql</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本数据结构]]></title>
    <url>%2F2019%2F02%2F02%2F%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[基本数据结构&emsp;&emsp;虽然以前都学过也都用过，但是并没有具体总结过，这里权当是复习了，使用C++的面对对象方法重新实现一下这几个数据结构。 1. 栈&emsp;&emsp;一种动态集合，增加和删除操作元素的位置都是预定好的，在栈中，最先被删除的元素就是最后被放入的元素，实行的是后进先出，我们把数据放入栈称作压入，把数据拿出栈叫做弹出，一般把栈比做把网球放进球筒中，拿出来的时候肯定先拿出最后放进去那个，打个不恰当的比方就是一个人吃了再吐,先上代码。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next; node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class myStack&#123; int cnts; //入栈数量 node&lt;T&gt; *head; //栈的头部public: myStack()&#123; cnts = 0; head = new node&lt;T&gt;; &#125; void stackPush(T arg); //入栈 T stackPop(); //出栈 T stackTop(); //获取栈顶元素 void printStack(); //打印栈 int counts(); //获取栈内元素个数 bool isEmpty(); //判断空&#125;;template&lt;class T&gt;void myStack&lt;T&gt;::stackPush(T arg)&#123; node&lt;T&gt; *pnode = new node&lt;T&gt;(arg); //申请入栈元素的空间 pnode-&gt;next = head-&gt;next; head-&gt;next = pnode; cnts++;&#125;template&lt;class T&gt;T myStack&lt;T&gt;::stackPop()&#123; if (head-&gt;next!=nullptr) &#123; node&lt;T&gt;* temp = head-&gt;next; head-&gt;next = head-&gt;next-&gt;next; T popVal = temp-&gt;value; delete temp; cnts--; return popVal; &#125;&#125;template&lt;class T&gt;T myStack&lt;T&gt;::stackTop()&#123; if (head-&gt;next!=nullptr) &#123; return head-&gt;next-&gt;value; &#125;&#125;template&lt;class T&gt;void myStack&lt;T&gt;::printStack()&#123; if (head-&gt;next != nullptr) &#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr) &#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; endl; &#125; &#125;&#125;template&lt;class T&gt;int myStack&lt;T&gt;::counts()&#123; return cnts;&#125;template&lt;class T&gt;bool myStack&lt;T&gt;::isEmpty()&#123; if (cnts) return false; else return true;&#125; &emsp;&emsp;这回就直接写一个C++的模版类stack.hpp，扔到头文件里是为了更好的调用。我觉得一个类能更好的描述一个操作集合，里面的数据用了链表结构储存，关于链表下面再说。&emsp;&emsp;当这个对象被实例化的时候，这个栈是空的，此时cnts=0，如果对一个空栈进行弹出操作，那么通常会抛出一个异常，这叫做栈下溢，如果添加数据超过存储上限，称为栈上溢，不过书上这么说是因为书上是用数组实现的，而链表长度不定，最大长度理论上和内存条大小有关，所以不考虑栈上溢，而且链表增加与删除只需要修改节点就行了，非常容易实现，栈的插入、删除、判空的操作时间均为O(1)。 2. 队列&emsp;&emsp;队列的插入操作叫做入队，删除操作叫做出队，队列与栈的操作相反，是先进先出，队列有队头与队尾，与我们排队一样，先到的人先办事，废话不说，上代码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next; node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class myQueue&#123; public: int cnts; //入队数量 node&lt;T&gt; *head; //队的头部 node&lt;T&gt; *tail; //队的尾部 myQueue()&#123; cnts = 0; head = new node&lt;T&gt;; tail = new node&lt;T&gt;; tail-&gt;next=head; &#125; void enqueue(T arg); //入队 T dequeue(); //出队 T getfrist(); //获取队首元素 void printQueue(); //打印队列 int counts(); //获取队列内元素个数 bool isEmpty(); //判断空&#125;;template&lt;class T&gt;void myQueue&lt;T&gt;::enqueue(T arg)&#123; node&lt;T&gt; *pnode = new node&lt;T&gt;(arg); //申请入队元素的空间 if(cnts==0)&#123; tail-&gt;next=pnode; head-&gt;next=pnode; &#125;else&#123; tail-&gt;next-&gt;next=pnode; tail-&gt;next=pnode; &#125; cnts++;&#125;template&lt;class T&gt;T myQueue&lt;T&gt;::dequeue()&#123; if(cnts!=0)&#123; node&lt;T&gt;* temp = head-&gt;next; head-&gt;next = head-&gt;next-&gt;next; T popVal = temp-&gt;value; delete temp; cnts--; return popVal; &#125; &#125;template&lt;class T&gt;T myQueue&lt;T&gt;::getfrist()&#123; if(cnts!=0)&#123; return head-&gt;next-&gt;value; &#125;&#125;template&lt;class T&gt;void myQueue&lt;T&gt;::printQueue()&#123; if (head-&gt;next != nullptr) &#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr) &#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; endl; &#125; &#125;&#125;template&lt;class T&gt;int myQueue&lt;T&gt;::counts()&#123; return cnts;&#125;template&lt;class T&gt;bool myQueue&lt;T&gt;::isEmpty()&#123; if (cnts) return false; else return true;&#125; &emsp;&emsp;同样的，队列队删除、插入、判空操作需要的时间也都是O(1)。 3. 链表&emsp;&emsp;链表中的各个对象按线性顺序排列。与数组不同，链表的存储并不是一整块内存区域，也就是说他顺序并不是由下标来决定的，而是由每一个元素中的指针来决定的，在内存中的表现就是他存储的数据是离散的，而且长度可以不确定，大小随着长度增长而增长，其长度理论上可以占满整个内存，在实现队列和栈时并没有对容量上限作出检查的原因就是如此。&emsp;&emsp;链表中的每一个节点就是一个对象，每一个对象都有一个key和一个指针，有时还会包含一些卫星数据，如果x为链表中的一个元素，那么x的指针x.next则指向他的下一个后继元素，一般链表都会有一个头节点，称为头，这个节点相当于一个“柄”，他的后面连接着整条链表，我们通常通过这个“柄“对这个链表操作，最后一个节点被称为尾，头节点的指针为空，说明该链表为空。&emsp;&emsp;链表不止一种形式，上面说的是单链表，还有一种双向链表，在单链表的基础上，每一个节点增加了一个前驱指针，可以从链表的头和尾同时进行访问，也就是能够进行顺序和逆序操作。此外还有循环链表，这种链表的尾指针指向头指针，直观看来他的结构是闭合的，是一个各个元素组成的圆环，下面讨论的都是乱序的双向链表,而且有头节点。&emsp;&emsp;先看一下关于链表的实现。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include&lt;iostream&gt;using namespace std;template&lt;class T&gt;class node&#123; public: T value; //储存的值 node&lt;T&gt;* next;//后继指针 node&lt;T&gt;* pre;//前驱指针 node() :next(nullptr)&#123;&#125; //构造函数 node(T t) :value(t), next(nullptr)&#123;&#125;&#125;;template&lt;class T&gt;class Link&#123; private: int count; node&lt;T&gt; *head;//头节点 public: Link()&#123; count=0; head = new node&lt;T&gt;; &#125; void addnode(T t);//增加节点 void deletenode(T t);//删除节点 node&lt;T&gt;* searchnode(T t);//查找节点 void printlink();//打印链表&#125;;template&lt;class T&gt;void Link&lt;T&gt;::addnode(T t)&#123; node&lt;T&gt; *p=new node&lt;T&gt;(t); if(count!=0)&#123; p-&gt;next=head-&gt;next; head-&gt;next-&gt;pre=p; head-&gt;next=p; p-&gt;pre=head; &#125;else&#123; p-&gt;next=head-&gt;next; head-&gt;next=p; p-&gt;pre=head; &#125; count++;&#125;template&lt;class T&gt;node&lt;T&gt;* Link&lt;T&gt;::searchnode(T t)&#123; if(head-&gt;next!=nullptr)&#123; node&lt;T&gt; *temp; temp=head; while(temp-&gt;next!=nullptr)&#123; temp = temp-&gt;next; if (temp-&gt;value==t) &#123; return temp; &#125; &#125; cout&lt;&lt;"未找到该元素"&lt;&lt;t&lt;&lt;"！"&lt;&lt;endl; return nullptr; &#125; return nullptr;&#125;template&lt;class T&gt;void Link&lt;T&gt;::deletenode(T t)&#123; node&lt;T&gt; *temp; temp=searchnode(t); if (temp!=nullptr) &#123; if(temp-&gt;pre!=nullptr)&#123; temp-&gt;pre-&gt;next=temp-&gt;next; &#125; if(temp-&gt;next!=nullptr)&#123; temp-&gt;next-&gt;pre=temp-&gt;pre; &#125; &#125; count--;&#125;template&lt;class T&gt;void Link&lt;T&gt;::printlink()&#123; if (head-&gt;next != nullptr)&#123; node&lt;T&gt;* temp = head; while (temp-&gt;next != nullptr)&#123; temp = temp-&gt;next; cout &lt;&lt; temp-&gt;value &lt;&lt; " "; &#125; cout&lt;&lt;endl; &#125;&#125; &emsp;&emsp;searchnode使用简单的线性方法去遍历链表并返回元素指针，如果不存在key值为t的对象，则返回nullptr，在搜索有n个对象的链表nullptr的最坏运行时间为$\Theta$(n)，也就是遍历整条链表。&emsp;&emsp;addnode可以像链表中插入关键字为t的元素，将其链接在链表的前端，其运行时间为O(1)。&emsp;&emsp;deletenode将一个元素x从链表中移除，需要先通过searchnode找到该对象的指针，通过修改其前一个元素和后一个元素的前驱指针与后继指针即可将其移出链表，操作时间为O(1)，但是删除操作首先调用了searchnode方法，所以又耗费了$\Theta$(n)时间，所以其最坏情况为$\Theta$(n)。&emsp;&emsp;printlink使用线性方法遍历整个链表，并逐一打印其元素的值，由于其必定遍历整条链表，所以对于n个对象的链表来说，他的运行时间为$\Theta$(n)。&emsp;&emsp;《算法导论》中还介绍了一种叫做哨兵的哑对象，代表NIL，属性与其他对象相同，它可以将常规的双向链表转化为有哨兵的双向循环链表，它位于表头与表尾之间，next指向表头，pre指向表尾，类似的，表尾的next和表头的pre都指向哨兵，哨兵的next指向表头，我们可以去掉head，把他的引用代替为对哨兵.next的引用，空链表只有一个哨兵，他的next和pre都指向他自己。哨兵并不能降低该数据结构操作的时间渐进界，但是可以降低一个常数因子，可以做到代码简洁，但是并不能提高速度，如果有多条短链表的话，就不建议使用哨兵，因为会加大内存开销。 4. 有根树&emsp;&emsp;链表可以推广到任意的同构数据结构，比如对栈和队列，我就是使用链表对书上用数组描述的伪代码进行重构。树与链表很类似，都是使用节点进行数据存储，但是链表是一条长链，而树是分叉的，指向其他节点的指针并非一个，会随树的种类而变化。 4.1 二叉树&emsp;&emsp;顾名思义，这个树有两个叉，二叉树的每一个节点存在着一个属性和三个节点：p、left、right，分别代表父节点、左子节点和右子节点，如果p节点为空，则说明该节点为根节点，如果p根节点为空，则说明该二叉树为空，如果二叉树为近似完全二叉树(若设二叉树的深度为k，除第k层外，其他各层（1～（k-1）层）的节点数都达到最大值，且第k层所有的节点都连续集中在最左边)，可以将其称之为堆(不是Java的垃圾堆)，后面讲到堆排序的会具体说明。二叉树的代码等到对二叉搜索树进行总结时再贴出。 4.2 分支无限制有根树&emsp;&emsp;这个货就厉害了，他的分叉没有限制，但是麻烦的是因为他们没有限制，所以不知道他下面到底连了几个节点，而且如果设置一个很大的常数k作为子节点数量，但是只有几个子节点有数据，那么内存空间就大大的被浪费了。&emsp;&emsp;不过还是有方法解决这个问题，而且对于任意n个子节点的有根树来说，只需要O(n)的储存空间。这种方法叫做左孩子右兄弟表示法，与二叉树类似，有一个指向父节点的p指针，但是指向子节点的指针并不是n个，而是只有两个指针left-child和right-sibling。&emsp;&emsp;left-child指向该节点众多子节点中最左侧的那一个节点，right-sibling指向与自己同级的右侧相邻的兄弟节点。left-child为空表示其没有子节点，right-sibling为空表示其为同级中最右侧节点。&emsp;&emsp;树还有很多表示方法，等到后面学其他数据结构的时候再进行总结。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归分治]]></title>
    <url>%2F2019%2F02%2F01%2F%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%2F</url>
    <content type="text"><![CDATA[递归分治&emsp;&emsp;在之前的归并排序中，使用了递归分治的方法，整个算法经历了三个过程。&emsp;&emsp;分解：将原问题划分为一个个子问题，子问题与原问题形式一样，只是规模更小。&emsp;&emsp;求解：递归的求出子问题。如果子问题规模足够小，停止递归，直接求解。&emsp;&emsp;合并：将子问题的解组合成原问题的解。&emsp;&emsp;当问题足够大时，需要递归求解，这就叫做递归情况，当问题已经分解到足够小时并且不用再递归时，递归已经“触底”，进入基本情况。 1.递归式&emsp;&emsp;递归与分治紧密相关，递归式可以很清楚的刻画分治的运行时间，递归式是一个等式或者一个不等式，可以用更小输入上的一个函数值来描述一个函数。在算法分析中，我用递归式描述了归并排序最坏运行时间T(n)。 $$T(n) =\begin{cases} Θ(1)&amp;n = 1\ 2T(n/2)+ Θ(n)&amp;n &gt; 1 \end{cases}$$ 求解后得到T(n) = $\Theta$(nlgn)。&emsp;&emsp;递归并不是一定要等分，也可以将问题划分为规模不等的子问题，如2/3与1/3的划分，分解和合并都是线性的，这样的递归式就是T(n) = T(2n/3)+T(n/3)+$\Theta$(n)。&emsp;&emsp;有三种求解递归式的方法，可以得出算法为“$\Theta$”或“O”的渐进界方法。 1.1 代入法&emsp;&emsp;这个方法比较玄学，因为第一步需要猜，首先需要猜出解的形式，然后用数学归纳法求出解的常数，并证明解是正确的。当将归纳假设应用于较小值时，将猜测的函数代入，所以叫“代入法”。但是首先我们要猜对解的形式。&emsp;&emsp;由于这个方法并没有一个通用的公式，解出来基本靠经验和运气，所以不再往深处讨论。细节部分翻《算法导论》。 1.2 递归树法&emsp;&emsp;画出递归树是一个非常直观的方法，像在算法分析时为归并排序画出的递归树一样，在树中，每一个节点代表每一个子问题的代价，子问题对应某一次递归函数的调用。将树中每层的代价求和，可以得到每一层的代价，将所有层代价求和，可以得到递归树的总代价。&emsp;&emsp;递归树可以生成猜测，然后用代入法验证猜测是否正确，如果在画递归树时代价求和十分准确，那么也可以直接用递归树证明解是否正确。 1.3 主方法&emsp;&emsp;主方法为递归式提供了一个通式 T(n) = aT(n/b)+f(n) 其中a &gt;= 1、b &gt; 1且都是常数，f(n)是渐进正函数。这个公式描述了一个算法的运行时间：他将规模为n的问题分解为a个子问题，每个子问题规模为n/b，其中a和b都是正常数。a个子问题递归的求解，每个花费时间为T(n/b)。函数f(n)包含了问题分解和子问题解合并的代价。因为n/b不一定为整数，所以一般将T(n/b)替换为T($\lfloor$n/b$\rfloor$)或者T($\lceil$n/b$\rceil$)。，并不会影响渐进式性质。 主定理：令a &gt;= 1和b &gt; 1是常数，f(n)是一个函数，T(n)是定义在非负整数上的递归式 T(n) = aT(n/b)+f(n)其中将n/b解释为$\lfloor$n/b$\rfloor$或者$\lceil$n/b$\rceil$。T(n)有如下渐进界。&emsp;&emsp;1.若某个常数$\epsilon$ &gt; 0 有f(n) = O(nlogba-$\epsilon$)，则T(n) = $\Theta$(nlogba)&emsp;&emsp;2.若f(n) = $\Theta$(nlogba)，则T(n)=$\Theta$(nlogbalgn)&emsp;&emsp;3.若对某个常数$\epsilon$ &gt; 0有f(n) = Ω(nlogba+$\epsilon$)，且对某个常数c &lt; 1和所有足够大的n有af(n/b) &lt;= cf(n)，则T(n) = $\Theta$(f(n))。 &emsp;&emsp;先来尝试理解一下主定理的含义，这三种情况都是将函数f(n)与函数nlogba比较。直觉上，两个函数的较大者决定了递归式的解。nlogba更大，就是情况1，解为T(n)=$\Theta$(nlogba)。如果f(n)更大，就是情况3，解为T(n) = $\Theta$(f(n))。若两个函数相当，就是情况2，要再乘上一个对数因子解为T(n)=$\Theta$(nlogbalgn)=$\Theta$(f(n)lgn)。&emsp;&emsp;第一种情况下并不是f(n)小于$\Theta$(nlogba)就够了，而是要多项式的意义上小于。f(n)必须渐进的小于nlogba，要相差一个因子n$\epsilon$，$\epsilon$是大于0的常数。第三种情况与第一种相似，并且要满足“正则“条件af(n/b)&lt;=cf(n)。&emsp;&emsp;这三种情况并未覆盖所有可能性，三种情况之间都有一定的缝隙，比如f(n)和nlogba并不是形式意义上的大于或小于关系，这种情况无法使用主定理。 主方法使用 &emsp;&emsp;使用主定理时只需要确定情况即可得到解。举个栗子 T(n) = 9T(n/3)+n 对于这个递归式，a = 9,b = 3,f(n) = n,所以nlogba = nlog39 = $\Theta$(n2)。f(n) = O(nlog39-$\epsilon$)，其中$\epsilon$ = 1，适用于情况1，得到解T(n) = $\Theta$(n2)。情况2和情况3的解法与此类似。证明部分见《算法导论》。 2. 最大子数组问题&emsp;&emsp;在一个数组中找出最大的非空连续子数组。 2.1 暴力破解&emsp;&emsp;尝试对每一种情况进行组合，n个数中一共有C(n,2)种组合，C(n,2) = $\Theta$(n2)，每次操作至少也是个常量，所以这种方法的运行时间为Ω(n2).很显然，暴力破解代价有点大。&emsp;&emsp;从另外一个角度来看看输入的数据。目的是找到一段数据，使其从开头到结尾的净变化值最大，所以现不看每次输入的数据，而是每次输入数据的变化值，定义第i个数的变化为第i个数与第i-1个数之差，将其看作数组A，问题就转化为数组A的和最大非空连续子数组，这样的连续子数组为最大子数组，乍一看对于一个n个数的串，我们还是需要检查C(n-1,2)=$\Theta$(n2)个子数组，重新组织计算方式，利用之前计算出的子数组和来计算当前子数组的和，使得每个子数组的和计算时间为O(1)，使暴力破解花费的时间为$\Theta$(n2)，代价还是有点大，我们需要将运行时间限制在o(n2)。 2.2 分治法&emsp;&emsp;首先说明，只有当数组中包含负数时，最大子数组大问题才有意义，否则，最大子数组直接就是数组A。&emsp;&emsp;假定我们寻找数组A的最大子数组，我们需要将数组划分为两个规模大致相同的两个子数组，也就是说找到中央位置，然后将其对半拆分，在数组A[low..high]中的任何子数组必定满足一下条件。 完全位于数组A[low..mid]中，low &lt;= i &lt;= j &lt;= mid. 完全位于数组A[mid+1..high]中，mid &lt; i &lt;= j &lt;= high 跨越中点，low &lt;= i &lt;= mid &lt;= j &lt;= high &emsp;&emsp;最大子数组也必定在这三种情况之内，我们可以递归求解A[low..mid]和A[mid+1..high]的最大子数组，这两个子问题与原问题一样，只是规模更小。剩下的工作就是寻找跨越中点的最大子数组，然后在三者之间挑选值最大者。此时问题加入了限制，子数组必须跨越中点，任何跨越中点的子数组都由A[i..mid]和A[mid+1..j]组成，只要找出形如A[i..mid]和A[mid+1..j]的最大子数组，然后合并即可，下面是该操作的C++实现。1234567891011121314151617181920212223242526272829303132333435int verylow=numeric_limits&lt;int&gt;::lowest();;class node&#123; public: int left; int right; int sum;&#125;;node find_max_crossing_subarray(int *A,int low,int mid,int high)&#123; int left_sum=verylow; int max_left,right_sum,max_right; int sum=0; for(int i = mid; i &gt;= low; i--)//求出左半侧最大子数组 &#123; sum=sum+A[i];//累计求和 if(sum&gt;left_sum)&#123;//逐个相加筛选最大和 left_sum=sum; max_left=i;//记录下标 &#125; &#125; right_sum=verylow; sum=0; for(int j = mid+1; j &lt;= high; j++)//求右侧最大子数组，与上同 &#123; sum=sum+A[j]; if(sum&gt;right_sum)&#123; right_sum=sum; max_right=j; &#125; &#125; node message; message.left=max_left; message.right=max_right; message.sum=left_sum+right_sum; return message;&#125; &emsp;&emsp;如果子数组包含n个元素(n=high-low+1)，调用find_max_crossing_subarray将花费$\Theta$(n)的时间，两次for循环每次花费$\Theta$(1)，第一个for循环执行了mid-low+1次迭代，第二次循环执行了high-mid次迭代，总循环迭代次数为 (mid-low+1)+(high-mid) = high-low+1 = n find_max_crossing_subarray是线性时间的，现在可以对最大子数组进行分治。1234567891011121314151617181920212223node find_maximum_subarray(int *A,int low,int high)&#123; if (high==low)&#123;//数组只有一个元素，本身就是最大和 node n; n.left=low; n.right=high; n.sum=A[low]; return n; &#125; node left_node,right_node,cross_node; int mid=(low+high)/2;//子数组划分 //递归求解左右最大子数组 left_node=find_maximum_subarray(A,low,mid);//左子数组 right_node=find_maximum_subarray(A,mid+1,high);//右子数组 cross_node=find_max_crossing_subarray(A,low,mid,high);//跨越中点的最大子数组，可以看作合并 if(left_node.sum&gt;=cross_node.sum&amp;&amp;left_node.sum&gt;=right_node.sum)&#123;//判断，返回最大子数组 return left_node;//最大子数组在左边 &#125;else if(right_node.sum&gt;=left_node.sum&amp;&amp;right_node.sum&gt;=cross_node.sum) &#123; return right_node;//最大子数组在右边 &#125;else&#123; return cross_node;//最大子数组跨越中点 &#125;&#125; 2.3 分治算法分析&emsp;&emsp;在find_maximum_subarray中，首先现花费常量的时间对n = 1做时间分析，直接得出T(1) = $\Theta$(1)。&emsp;&emsp;当n &gt; 1时开始递归。12行和13行求解的问题均为n/2个元素的子数组，所以每个子数组求解的时间为T(n/2),因为需要求左子数组和右子数组两个问题，所以运行总时间为2T(n/2)，14行调用find_max_crossing_subarray花费了$\Theta$(n)，后面的判断仅仅花费了$\Theta$(1)。因此可以列出等式； T(n) = $\Theta$(1)+2T(n/2)+$\Theta$(n)+$\Theta$(1) = 2T(n/2)+$\Theta$(n) 与上面的等式进行组合，可以得到： $$T(n) =\begin{cases} Θ(1)&amp;n=1\ 2T(n/2)+ Θ(n)&amp;n&gt;1 \end{cases}$$ 和归并排序的递归式一样。&emsp;&emsp;接下来用主方法来求解这个递归式。对于递归式： T(n) = 2T(n/2)+$\Theta$(n) a = 2，b = 2，f(n) = $\Theta$(n)，因此nlogba = nlog22 = n。因为f(n) = $\Theta$(n)，对应情况2，于是得到解T(n) = $\Theta$(nlgn)。我们得到了一个复杂度优于暴力破解的算法，递归分治可以给出一个渐进最快的算法，但是有时候还会有更快的算法，比如最大子数组问题还存在着一个速度更快的线性算法，等到学习总结到了动态规划再对该方法讨论。 &emsp;&emsp;《算法导论》中还有一个矩阵乘法的Strasson算法，虽然知道怎么写，也明白那样做可以减少一次矩阵运算，但是想不出来为什么把矩阵这么拆分组合，等我想通了再更新一下。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渐进记号]]></title>
    <url>%2F2019%2F01%2F31%2F%E6%B8%90%E8%BF%9B%E8%AE%B0%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[渐进记号&emsp;&emsp;在算法分析里面对算法运行时间以及效率问题分析时，当n的规模变得足够大时，我们引入了渐进分析记号$\Theta$来描述当输入规模无限大时，算法运行时间如何随输入规模变大而变大，现在来记录一下另外几种渐进记号约定，用来刻画算法某个其他其他方面的函数(空间与时间)。 &emsp;&emsp;在了解算法运行时间时，我们有时也要在意到底需要哪个运行时间，不是所有时候都需要最坏运行时间，我们希望有一个综合性的来刻画任何输入的运行时间，所以我们需要更多的渐进记号。 1. $\Theta$记号&emsp;&emsp;在分析插入排序与归并排序的运行时间时引入了这个记号插入排序的最坏运行时间为T(n)=$\Theta$(n2)，而归并排序只有T(n)=$\Theta$(nlgn)。&emsp;&emsp;来重新定义一下这个记号，对于一个给定的g(n)，用$\Theta$(n)表示一个函数集合： $\Theta$(g(n)) = {f(n):存在正常量c1、c2和n0，使得所有n &gt;= n0,有0 &lt;= c1g(n) &lt;= f(n) &lt;= c2g(n)} &emsp;&emsp;若存在正常量c1、c2能够在n足够大时，让函数夹在c1g(n)与c2g(n)之间，则f(n)就属于$\Theta$(g(n)),这里的$\Theta$(g(n))是一个函数集合，而非一个函数，所以我在算法分析里简单的把$\Theta$记为了去掉低阶项，忽略系数，这是一个非形式化的概念。在下面的图像中可以看出f(n)高于c1g(n)或者低于c2g(n)，换一句话说，对于所有的n&gt;=n0,f(n)存在一个常量因子等于g(n)，我们称g(n)为f(n)的渐进紧确界。 &emsp;&emsp;$\Theta$(g(n))要求其内的每一个成员f(n)都渐进非负，即n足够大时，f(n)非负(g(n)本身也必须非负，否则集合$\Theta$(g(n))为0)，这个要求对其他符号也成立。 $\Theta$符号有一个活用，当出现$\Theta$(1)时，并没有什么变量趋于无穷，所以$\Theta$(1)用来指任意一个常量或者某个关于变量的常量函数，其他符号也一样。 2. O记号&emsp;&emsp;当我们只需要一个函数的渐进上界的时候，我们采用O记号来表示。对于给定的函数g(n)，用O(g(n))来表示一下函数集合： O(g(n)) = {f(n)：存在正常量c和n0，使得对所有n &gt;= n0，有0 &lt;= f(n) &lt;= cg(n)}。 &emsp;&emsp;用O给出了一个函数在常量因子中的一个上界，从图中可以看出，在n0及其有边的所有n值中，f(n)总是小于或者等于cg(n)。 &emsp;&emsp;f(n)=$\Theta$(g(n))是一个比f(n)=O(g(n))更强的概念，f(n)=$\Theta$(g(n))包含f(n)=O(g(n))，按照集合论的方法，写作$\Theta$(g(n)) $\subseteq$ O(g(n))，所以如果证明了任意二次函数 an2+bn+c a &gt; 0 如果证明了该函数在$\Theta$(n2)中，也就证明了这个函数在O(n2)中，任意线性函数an+b a &gt; 0也在O(n2)中。&emsp;&emsp;O记号可以仅仅通过检查算法的总体结构来描述算法的运行时间，例如插入排序的双重循环结构中就会产生一个O(n2)的上界：内层循环的每一次代价至多为O(1)，下标i与j均最多为n，双重循环会产生至多n2次访问。&emsp;&emsp;O通常用来修限制算法的最坏运行时间，在每一次输入时，对于插入排序的最坏时间O(n2)适用于每一个输入。但是在插入排序最坏运行时间$\Theta$(n2)并不意味着每一次输入运行时间界都是$\Theta$(n2)，例如当输入已经被排序时，这时候运行时间界就是$\Theta$(n)。&emsp;&emsp;一般当我们说“运行时间为O(n2)”时，指存在一个O(n2)的函数f(n)，使得对任意n，不管选择什么特定规模的n输入，运行时间上界都是f(n)，这就是运行的最坏情况时间为O(n2)。 3. Ω记号&emsp;&emsp;有运行时间上界，当然对应的也有运行时间下界，称为函数的渐进下界，用符号Ω表示，对于给定的函数g(n)，用Ω(g(n))来表示一下函数集合： Ω(g(n)) = {f(n):存在正常量c&lt;和n0，使得所有n&gt;=n0,有0 &lt;= cg(n) &lt;= f(n)} 下图可以直观解释Ω记号的含义，这里不说太多。 &emsp;&emsp;根据上面三个符号的定义，很容易得出一个定理 对于任意两个函数f(n)与g(n)，有f(n)=$\Theta$(g(n))，当且仅当f(n) = O(g(n))且f(n) = Ω(g(n))。 &emsp;&emsp;当存在二次函数an2+bn+c a &gt; 0时，有an2+bn+c = $\Theta$(n)时，同时说明了an2+bn+c = O(n)、an2+bn+c = Ω(n)，通常用渐进上界与渐进下界来证明渐进紧确界，当一个算法的运行时间为Ω(g(n))时，对于每个值，不管选择什么输入，n足够大时，他的时间都是g(n)的整数倍，说插入排序的最好运行时间为Ω(n)，蕴含着插入排序的运行时间为Ω(n)。&emsp;&emsp;所以称插入排序的运行时间介于Ω(n)和O(n2)之间，落入n的线性函数与n的二次函数之间的任何地方。插入排序运行时间不是Ω(n2)因为存在输入使得其能在$\Theta$(n)时间内运行。如果说插入排序运行最坏运行时间为Ω(n2)并不错误，因为存在一个输入使得该算法运行时间为Ω(n2)。 4. 等式与不等式&emsp;&emsp;在渐进记号中的等号两端并非等价的，一般渐进记号在等式(或不等式)右端时，通常指集合的成员关系，比如n = O(n2)指n $\in$ O(n2)，我们将公式中的渐进记号解释为不带名称的匿名函数。入2n2+3n+1 = 2n2+$\Theta$(n),这里的$\Theta$(n)指在$\Theta$(n)集合之内的一个函数。&emsp;&emsp;这种方式可以帮我们消除等式中一些无关紧要的细节，如果只对某个算法的渐进行为感兴趣，就没必要说明所有的低阶项，都被理解为包含在了$\Theta$(n)表示的匿名函数中。&emsp;&emsp;某些情况下渐进记号可能出现在等式左边，如： 2n2+$\Theta$(n) = $\Theta$(n2) 无论怎样选择等号左边的匿名函数，总有一种办法来选择等号右边的匿名函数使等式成立。在2n2+$\Theta$(n) = $\Theta$(n2)中我们可以取$\Theta$(n)集合中的3n+1，使等式变为2n2+3n+1 = $\Theta$(n2)，这样2n2+$\Theta$(n)就在$\Theta$(n2)集合之内，可以说等式左边提供的细节比等式右边更粗糙。 5. o记号与ω记号&emsp;&emsp;O记号与Ω记号可能并非渐进紧却的，比如2n2 = O(n2)是渐进紧却的，而2n = O(n2)就不是渐进紧却的，我们用小写的o与ω定义集合。 o(g(n)) = {f(n):对任意正常量c&gt;0,存在常量n0&gt;0，使得所有n &gt;= n0,有0 &lt;= f(n) &lt; cg(n)},如2n = o(n2)但2n2 $\neq$ o(n2)。 ω(g(n)) = {f(n):对任意正常量c &gt; 0,存在常量n0 &gt; 0，使得所有n &gt;= n0,有0 &lt;= cg(n) &lt; f(n)}，如n2/2 = ω(n),而2n $\neq$ ω(n)。 &emsp;&emsp;这两个符号都是非渐进紧却的，与O记号和Ω记号最大的不同是，o记号与ω记号并不能等于g(n)关于c常量的函数。 f(n) $\in$ ω(g(n)),当且仅当 g(n) $\in$ o(f(n)) &emsp;&emsp;将f与g类比为实数a和b。 记号 含义 类比 $\Theta$ 渐进紧却界 a = b O 渐进上界 a &lt;= b Ω 渐进下界 a &gt;= b o 非紧却上界 a &lt; b ω 非紧却下界 a &gt; b 6. 函数比较&emsp;&emsp;实数的很多性质都适用于函数的渐进比较，设f(n)和g(n)渐进为正。 6.1 传递性 f(n) = $\Theta$(g(n)) 且 g(n) = $\Theta$(h(n)) 蕴含 f(n) = $\Theta$(h(n))f(n) = O(g(n)) 且 g(n) = O(h(n)) 蕴含 f(n) = O(h(n))f(n) = Ω(g(n)) 且 g(n) = Ω(h(n)) 蕴含 f(n) = Ω(h(n))f(n) = o(g(n)) 且 g(n) = o(h(n)) 蕴含 f(n) = o(h(n))f(n) = ω(g(n)) 且 g(n) = ω(h(n)) 蕴含 f(n) = ω(h(n)) 6.2 自反性 f(n) = $\Theta$f(n)f(n) = Of(n)f(n) =Ωf(n) 6.3 对称性 f(n) = $\Theta$(g(n)) 当且仅当 g(n) = $\Theta$(f(n)) 6.4 转置对称性 f(n) = O(g(n)) 当且仅当 g(n) = Ω(f(n))f(n) = o(g(n)) 当且仅当 g(n) = ω(f(n)) 6.5 三分性&emsp;&emsp;对于任意两个数a、b，有三种情况必须成立：a &gt; b、a &lt; b、a = b。任意两个实数可以比较，但不是任意两个渐进函数都可以比较。对于g(n)与f(n)，f(n) = O(g(n))和f(n) = Ω(g(n))可能都不成立。例如n与n1+sin n，因为n1+sin n的幂值在0到2之间来回摆动。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法分析]]></title>
    <url>%2F2019%2F01%2F24%2F%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[算法分析 The analysis of algorithm is the theoretical study of computer program performance and resource usage算法分析是理论研究关于计算机程序性能和资源利用的研究。 &emsp;&emsp;这一句话是MIT公开课上Charles Leiserson第一节课对于算法分析的定义。学习算法是为了如何让计算机更快，也就是解决计算机的性能问题。当然，在目前软件工程领域来讲，性能已经不是第一位了，软件的可维护性、健壮性、安全性、可扩展性、模块化、用户友好性、更多的功能都非常重要。为什么很多人更加青睐于MacOS而非Windows？因为用户友好性以及安全性，在计算机发展史上，这是一个非常大的飞跃。&emsp;&emsp;但是，所有的这些事情看似都比性能重要，我们为什么要学算法？ 1.性能与用户体验是紧密联系的。 &emsp;&emsp;打开一个应用需要等半天是不会有什么用户体验的，即便界面再漂亮也没有用。所以性能达不到需求，用户体验很难保证。 2.性能直接决定了可行与不可行。 &emsp;&emsp;程序太慢或者程序占用内存太大会直接导致程序在一些机器上不可行。 3.算法是描述程序行为的一种语言。 &emsp;&emsp;这是一门被计算机学科广泛使用的语言，已经被所欧的实践者所采用的理论语言，是程序的思考方式。Charles Leiserson用一个很有趣的例子去描述了性能为什么处于最底层。他把性能比做经济中的货币，而可维护性、用户体验是食物和水，虽然食物和水对于生命来说比金钱更重要，但他们都需要货币去换取，性能是他们的保障。这就是为什么那么多人用Java去写程序而非C，尽管前者比后者多耗费了三倍的性能，因为Java的良好特性值得人们去付出这些代价，在这里，性能就被充当为了货币去获取其他的对象，这就是性能处于底层的原因。 4.很有趣。 现在就从一个非常简单的排序问题引入算法分析。 排序问题&emsp;&emsp;这是一个很古老的问题，输入一组序列a1，a2……an。按需求后输出a1‘&lt;=a2‘&lt;=……&lt;=an‘。使得他们单调递增。 插入排序&emsp;&emsp;首先引入插入排序，插入排序的算法思想，数组从左到右将其中每一个元素依次设为key，key的左半部分是已经排序过的数组，每一次遍历就会将已排序部分增长一，而每一次循环将会把已排序数组的值一个一个向右移动，知道出现合适位置，并将key插入其中，所以这个排序称之为插入排序。下面是对《算法导论》中插入排序伪代码的C++实现。123456789101112void insertSort(int *a,int length)&#123; int key,i; for(int j=1;j&lt;length;j++)&#123;//从第二个开始遍历，直到最后一个 key=a[j];//标记当前需要插入的数 i=j-1;//i作为指针，遍历j之前的数 while(i&gt;=0&amp;&amp;a[i]&gt;key)&#123; //开始遍历，如果遍历到的数比key大，就把这个数向后移一位，知道循环完毕或者碰到比key小的数 a[i+1]=a[i]; i=i-1; &#125; a[i+1]=key;//将key插入 &#125; 这就是插入排序，现在我们对这个排序进行一个分析，首先关注运算时间问题。 1.输入本身 如果输入的数组本身有序，那么我们做的事将会很少，但是最坏情况下，也就是逆序情况下，我们将会得到最坏对运行时间。 2.输入规模 我们在输入几个元素的情况下，这个算法很快，但是如果输入几十亿个元素对话，就需要很长时间，所以输入规模越大，运行时间越长。 3.运行时间上界 这代表对用户的承诺，就是是说保证这个程序运行不会超过某个时间，相反的，要说说这个程序运行至少多长时间，这种信息就不能被当作承诺。 算法分析平均情况分析&emsp;&emsp;有些情况下会这么做，在这里T(n)就是输入规模为n下所有可能输入的期望时间，也就是概率论中所说的E(x)，但是我们需要一个输入统计的分布假设，否则期望就无从谈起，最常见的是均匀分布。 最好情况分析&emsp;&emsp;一般被称为假象，因为最好情况基本很少出现，即使出现也是仅仅针对于少量情况适用，比如对一个已排好序的数组排序，但是不包含大量其他情况。 最坏情况分析&emsp;&emsp;这是我们通常最关注的问题，我们首先设T(n)为输入规模为n时的最长运行时间，只有做最坏的打算，我们才能对我们的程序进行承诺。&emsp;&emsp;先看一下插入排序的最坏时间。 首先取决于运行他的计算机。 一般比较算法比较的是他的相对运算速度也就是同一台机子上的运行速度，所以这个一般不考虑。我们需要计算机的大局观，一种观点，就是渐进分析。 渐进分析&emsp;&emsp;渐进分析的思路是忽略掉那些依赖于机器的常量，以及不是去检查实际的运行时间，而是关注运行时间增长。&emsp;&emsp;首先采用渐进符号$\Theta$。 弃去公式的低阶项，并忽略前面的常数因子，如果公式是3n3+90n2-5n+6046,那么这个公式可以说是$\Theta$(n3)。 当n趋近于无穷大时，$\Theta$(n2)肯定比$\Theta$(n3)快，无论其他项是什么也动摇不了这个结果，即使在一台慢速计算机运行$\Theta$(n2)，在一台高速计算机运行$\Theta$(n3)。但是这两个函数相交于一个n0点，如果这个n0过大，计算机就无法计算，所以某些时候会青睐一些低速算法，尽管从渐进角度看，他们会比较慢，但在合理输入情况下他们可能会更快。&emsp;&emsp;继续分析插入排序，在逆向排序好的情况下，插入排序需要将所有项全部翻一遍，假设每一个操作耗费一个原子时间，循环中所有的操作都是原子时间，所以在上面的算法中，j从1循环到length，我们可以写成数学中的求和符号，从1到length,设length为n,对于一个给定的j值，循环将会进行$\Theta$(j)次操作，因为内层循环里的i以j-1为初值，在i每次取新值时进行固定数量的操作，i从j-1降到0。 T(n)=$\sum_{j=1}^n$$\Theta$(j)=$\Theta$(n2)连续整数求和，算术级数 对于很小的n，插入排序很快，但对于巨大的n，就不行了，我们需要一个更快的算法————归并排序。 归并排序&emsp;&emsp;首先对于数组A[1……n] 1.如果n为1，那么数组就是排好序的。2.递归递归的对A[1到n/2向上取整]的这一部分，以及A[n/2+1向上取整到n]这部分排序3.把排好序的两个表归并 归并过程如图所示实质上就是将两个已排序好对的数组，也就是A[1..N/2]和A[N/2+1..n]，进行归并，得到一个排序好的数组，每一步都是固定的操作与数组长度无关，所以对于总数为n的输入，时间是$\Theta$(n)的。&emsp;&emsp;对于这个递归，我们可以写出一个递推式。 $$T(n) =\begin{cases} Θ(1)&amp;n=1\ 2T(n/2)+ Θ(n)&amp;n&gt;1 \end{cases}$$ &emsp;&emsp;这是一个树状结构，树的末端时间只有$\Theta$(1)，而树的高度是lgn，叶的节点总数为n，完全扩展的递归树有lgn+1层，每层贡献总代价$\Theta$(n)，如果计算总数就是$\Theta$(n)lgn+$\Theta$(n)，根据渐进的思想，最后结果就是$\Theta$(nlgn)，考虑渐进，他比$\Theta$(n2)快,在数据足够大的情况下，归并排序将优于插入排序，差不多n大于30归并就更快了。下面是对《算法导论》中归并排序的C++实现。1234567891011121314151617181920212223242526272829303132333435const int N=204800;void Merge(int *arr,int p,int q,int r)&#123; int n1=q-p+1;//左数组长度 int n2=r-q;//右数组长度 int left[n1+1],right[n2+1];//开辟新的左右数组 for(int i = 0; i !=n1; ++i)&#123; left[i]=arr[p+i];//为左数组赋值 &#125; left[n1]=N;//左数组“哨兵” for(int j = 0; j!= n2; ++j) &#123; right[j]=arr[q+j+1];//为右数组赋值 &#125; right[n2]=N;//右数组“哨兵” int i=0,j=0; for(int k = p; k !=r+1; ++k)//将左右数组归并至原数组 &#123; if(left[i]&gt;right[j])&#123; arr[k]=right[j]; ++j; &#125;else&#123; arr[k]=left[i]; ++i; &#125; &#125;&#125;void MergeSort(int *arr,int p,int r)&#123; //分治法，将数组分割，将复杂问题化简为数个简单问题 if(p&lt;r)&#123; int q=(p+r)/2;//数组分割标记，中间下标 MergeSort(arr,p,q);//分割左边数组 MergeSort(arr,q+1,r);//分割右边数组 Merge(arr,p,q,r);//进行归并排序 &#125;&#125; 这就是算法分析的一部分，后面还有更多的分析方法，如果想知道更多的证明细节，详见《算法导论》。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-使用QueryDSL查询框架]]></title>
    <url>%2F2019%2F01%2F22%2FSpringBoot-%E4%BD%BF%E7%94%A8QueryDSL%E6%9F%A5%E8%AF%A2%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[SpringBoot-使用QueryDSL查询框架(基础) QueryDSL是一个通用的查询框架，框架的核心原则是创建安全类型的查询，开始QueryDSL仅支持Hibernate（HQL），在不断开源人士加入QueryDSL团队后，陆续发布了针对JPA，JDO，JDBC，Lucene，Hibernate Search，MangoDB，Collections 和RDF(Relational Data Format) Bean等。 简述&emsp;&emsp;在使用SpringDataJPA的时候，内置的QueryByExampleExecutor对复杂查询来说显然不怎么好用，于是在这里引入一个便于我们进行复杂查询的框架————QueryDSL。这个框架可以完美的和SpringDataJPA进行融合，接下来主要说怎么用。 项目构建&emsp;&emsp;怎么搭建SpringBoot、数据源、数据库连接、配置SpringDataJPA就不说了，在原先的基础上添加项目依赖。12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-jpa&lt;/artifactId&gt; &lt;version&gt;4.0.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt; &lt;version&gt;4.2.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.inject&lt;/groupId&gt; &lt;artifactId&gt;javax.inject&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt;&lt;/dependency&gt; 仅仅添加依赖还不够，我们需要在下面的&lt;plugins&gt;标签中添加插件 1234567891011121314151617&lt;!--该插件生成查询对象--&gt; &lt;plugin&gt; &lt;groupId&gt;com.mysema.maven&lt;/groupId&gt; &lt;artifactId&gt;apt-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;process&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/generated-sources/java&lt;/outputDirectory&gt; &lt;processor&gt;com.querydsl.apt.jpa.JPAAnnotationProcessor&lt;/processor&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 然后在idea界面右上角打开Edit Configurations,新建一个maven，填入如图所示信息。在右上角启动这个插件。该插件会自动扫描项目内配置了@Entity的实体类，并根据实体的内定义的字段以及关联类通过JPAAnnotationProcessor自动创建Q[实体类名称]的查询实体，创建完成后会将实体存放到我们配置outputDirectory属性目录下。如图所示。 准备准备数据库信息&emsp;&emsp;先构建一个普通的User实体类,项目启动时会自动根据这个实体类建表，建表后向里面扔几条测试数据。123456789101112131415161718192021/** * @Author: cp * @Date: 2019/1/21 下午11:13 * @Version 1.0 */@Entity@Table(name = "user")public class User implements Serializable &#123; @Id @Column(name = "id") @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @Column(name = "name") private String name; @Column(name = "age") private String age; @Column(name = "address") private String address; @Column(name = "pwd") private String pwd; /*setter and getter*/ 创建基类JPA&emsp;&emsp;正如上一篇所说，正常情况下一个项目肯定不可能就继承一个JpaRepository接口，再使用其他模块时还需要多个接口继承，如果每一个业务数据接口都继承几个相同的接口的话，对于系统设计和代码复用性来说不是个什么好的选择。所以我们需要一个基类JPA。12345678/** * @Author: cp * @Date: 2019/1/21 下午11:16 * @Version 1.0 */@NoRepositoryBeanpublic interface BaseJPA&lt;T&gt; extends JpaRepository&lt;T,Integer&gt;,JpaSpecificationExecutor&lt;T&gt;,QuerydslPredicateExecutor&lt;T&gt; &#123;&#125; @NoRepositoryBean是为了避免自动实例化。 创建对应模块JPA&emsp;&emsp;创建对应User模块的数据逻辑接口JPA，因为BaseJPA已经继承了我们所需要的所有接口，所以我们只需要继承BaseJPA就可以了。1234567/** * @Author: cp * @Date: 2019/1/21 下午11:18 * @Version 1.0 */public interface UserJPA extends BaseJPA&lt;User&gt; &#123;&#125; 执行插件生成Q结构查询实体&emsp;&emsp;这个不多说了，刚才生成过了。打开自动创建的实体后可以看到QueryDSL自动为我们创建的查询字段以及构造函数。 编写控制器&emsp;&emsp;为了方便我就不写Service层了，所有业务逻辑全部放在Controller层，不过正式开发时一定要写。12345678910111213141516/** * @Author: cp * @Date: 2019/1/21 下午11:20 * @Version 1.0 */@RestControllerpublic class UserController &#123; @Autowired private UserJPA userJPA; @Autowired private EntityManager entityManager; private JPAQueryFactory queryFactory; @PostConstruct public void initFactory()&#123; queryFactory=new JPAQueryFactory(entityManager); &#125; 声明EntityManager的注入以及JPAQueryFactory工厂对象的创建，通过@PostConstruct注解在类初始化的时候完成对JPAQueryFactory对象的实例化。 查询查询并排序12345678@RequestMapping(value = "/queryAll")public List&lt;User&gt; queryAll()&#123; QUser qUser=QUser.user;//创建查询对象 return queryFactory//查询并返回 .selectFrom(qUser)//源 .orderBy(qUser.id.desc())//根据id倒序 .fetch();//执行并返回结果集&#125; &emsp;&emsp;queryAll方法内首先获取了对应UserBean的查询实体QUserBean，通过QUserBean内自动生成的字段获取，我们使用JPAQueryFactory工厂对象的selectFrom方法来简化查询，该方法代替了select&amp;from两个方法，注意：也是仅限单表操作时可以使用。&emsp;&emsp;这样写起来感觉就像在写原生SQL，在一系列的条件都添加完成后，调用fetch方法执行我们的条件查询并且获取对应selectFrom查询实体的类型集合，要注意一点：这里如果selectFrom参数的实体类型不是User那fetch方法返回集合的类型也不是List&lt;User&gt;。&emsp;&emsp;启动后访问这个方法就可以在网页中见到这些数据，同时在控制台中也可以看见生成的SQL语句。1234567891011Hibernate: select user0_.id as id1_3_, user0_.address as address2_3_, user0_.age as age3_3_, user0_.name as name4_3_, user0_.pwd as pwd5_3_ from user user0_ order by user0_.id desc 根据主键查询数据&emsp;&emsp;这个方法是最常用的方法，现在我们有两种写法，一种是使用QueryDSL，另外一种是和SpringDataJPA结合。使用QueryDSL：12345678@RequestMapping(value = "/detail/&#123;id&#125;")public User detail(@PathVariable("id") Integer id)&#123; QUser qUser=QUser.user;//使用querydsl return queryFactory .selectFrom(qUser)//源 .where(qUser.id.eq(id))//条件 .fetchOne();//结果&#125; SpringDataJPA+QueryDSL:12345@RequestMapping(value = "/detail_2/&#123;id&#125;") public Optional&lt;User&gt; detail_2(@PathVariable("id") Integer id)&#123; QUser qUser=QUser.user;//使用querydsl return userJPA.findOne(qUser.id.eq(id));//查询并返回结果 &#125; 网上的一些教程关于SpringDataJPA+QueryDSL的方法返回值好多都是直接返回实体类，不过最新的方法已经修改为返回Optional类，用来解决万恶的空指针异常。&emsp;&emsp;这两种代码效果是一样的，都是根据主键返回对应信息，看起来SpringDataJPA+QueryDSL还简单一点，不过这只限于单表查询，一旦涉及各种复杂操作还是QueryDSL简单。在使用QueryDSL查询指定主键时，我们使用了where方法并且指定了id字段需要eq参数id，这个eq是QueryDSL内置的一个方法，用于查询指定值数据，当然其他字段也同样可以使用eq方法来完成条件查询，都是可以变通使用的。 模糊查询&emsp;&emsp;根据name字段模糊查询。12345678@RequestMapping(value = "like")public List&lt;User&gt; likeQueryByName(String name)&#123; QUser qUser=QUser.user;//使用querydsl return queryFactory .selectFrom(qUser)//源 .where(qUser.name.like("%"+name+"%"))//条件 .fetch();&#125; like是QueryDSL内置方法，只要存入条件，就可以完成模糊查询，可以看一下控制台输出的SQL语句。1234567891011Hibernate: select user0_.id as id1_3_, user0_.address as address2_3_, user0_.age as age3_3_, user0_.name as name4_3_, user0_.pwd as pwd5_3_ from user user0_ where user0_.name like ? escape '!' 更新使用SpringDataJPA更新实体&emsp;&emsp;与Hibernate一样，SpringDataJPA内置了一个save方法用于保存、更新实体内容，如果存在主键值则更新对应信息，反则是添加一条新信息。我们新添加一个方法。12345678910@RequestMapping(value = "/updateJPA")public String updateJPA()&#123; User user=new User(); user.setId(1); user.setName("王五"); user.setAge("13"); user.setPwd("321"); userJPA.save(user); return "success";&#125; 很简单，并没有什么难理解的地方，直接访问这个方法就可以看结果了，重点是先看控制台输出的SQL语句，首先输出了一条查询语句。12345678910select user0_.id as id1_3_0_, user0_.address as address2_3_0_, user0_.age as age3_3_0_, user0_.name as name4_3_0_, user0_.pwd as pwd5_3_0_ from user user0_ where user0_.id=? 这是因为SpringDataJPA先去查询数据库中有没有这个主键的信息，如果有，才会执行下面的更新语句。123456789update user set address=?, age=?, name=?, pwd=? where id=? 使用QueryDsl更新实体1234567891011121314151617181920@Transactional@RequestMapping(value = "updateDsl")public String updateByQueryDsl()&#123; User user=new User(); user.setPwd("090"); user.setAge("80"); user.setName("赵四"); user.setId(1); user.setAddress("北京"); QUser qUser=QUser.user; queryFactory .update(qUser) .set(qUser.name,user.getName()) .set(qUser.address,user.getAddress()) .set(qUser.age,user.getAge()) .set(qUser.pwd,user.getPwd()) .where(qUser.id.eq(user.getId())) .execute(); return "SUCCESS";&#125; &emsp;&emsp;与SpringDataJPA不同的是，使用save方法的时候，如果实体类中某一条是空的，那么数据库中对应字段也会被设为空，而使用QueryDsl时就可以随心所欲的选择只更新某几条信息。设置完成更新字段后需要设置更新的条件，不设置也是可以的，当然这里肯定跟原生SQL一样，不设置条件就更新表内全部的数据。最后使用execute()执行操作就行了。 敲黑板：在执行update/delete方法时必须添加事务，也就是@Transactional注解，否则会抛出异常。 这是控制台输出的语句。12345678910Hibernate: update user set name=?, address=?, age=?, pwd=? where id=? 删除使用SpringDataJPA删除实体信息1234567@RequestMapping(value = "/deletejpa")public String deletejpa()&#123; User user=new User(); user.setId(4); userJPA.deleteById(user.getId());//删除指定主键的值 return "success";&#125; &emsp;&emsp;简单粗暴，不用解释。 使用QueryDsl删除实体信息123456789101112@Transactional//这个必须有@RequestMapping(value = "/deletequery")public String deleteQueryDsl()&#123; User user=new User(); user.setId(4); QUser qUser=QUser.user; queryFactory .delete(qUser)//删除 .where(qUser.id.eq(user.getId()))//条件 .execute();//执行 return "sueecss";&#125; &emsp;&emsp;跟update方法差不多，就是把update改成delete，看起来好像比SpringDataJPA还麻烦了，但是我们引入这个框架是为了复杂查询，所以我们加一个限定条件，删除id大于四，且name值为王五的人。123456789101112131415@Transactional//这个必须有@RequestMapping(value = "/deletequeryid")public String deletequeryById()&#123; User user=new User(); user.setName("mvm"); QUser qUser=QUser.user; queryFactory .delete(qUser)//删除 .where( qUser.name.eq(user.getName()) .and(qUser.id.gt(4))//条件 ) .execute();//执行 return "success";&#125; SpringDataJPA想要完成这个就没那么简单了，看一下生成的SQL语句。1234567Hibernate: delete from user where name=? and id&gt;? 还是很规范的，输出的SQL完全根据我们设置的条件来自动生成，QueryDsl内的条件可以跟原生SQL完全一样，可以完全采用SQL的思想来编写条件。 最后&emsp;&emsp;以上是SpringDataJPA结合QueryDSL的基础操作，后续还会继续学习多表关联查询、自定义返回对象、聚合查询、子查询等复杂操作。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>SpringDataJPA</tag>
        <tag>QueryDSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-整合SpringDataJPA]]></title>
    <url>%2F2019%2F01%2F21%2FSpringBoot-%E6%95%B4%E5%90%88SpringDataJPA%2F</url>
    <content type="text"><![CDATA[SpringBoot-整合SpringDataJPA1.简述&emsp;&emsp;SpringDataJPA是Spring Data的一个子项目，默认底层是Hibernate，使用基于JPA的Repository,极大的减少对数据库访问的代码量，仅仅使用内部接口就可以完成简单的crud操作。 2.搭建项目&emsp;&emsp;还使用之前搭好的框架进行测试，由于是测试，为了方便就不搭建Service层了，所有逻辑代码都放在了Controller层，数据源使用alibaba的Druid数据源，关于Druid以后慢慢说，不放在这里讨论。 2.1导入依赖并修改配置文件&emsp;&emsp;首先在pom.xml原来的基础中添加配置。123456789&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 然后修改配置文件，添加关于Druid和SpringBootJPA的依赖123456789101112131415161718192021222324252627282930313233343536spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp datasource: type: com.alibaba.druid.pool.DruidDataSource url: jdbc:mysql://127.0.0.1:3306/test3?characterEncoding=UTF-8 username: root password: 20141232 driver-class-name: com.mysql.jdbc.Driver #最大活跃数 maxaActive: 20 #初始化数量 InitialSize: 1 #最大连接等待时间 maxWait: 60000 #打开PSCache，并指定大小 poolPreparedStatements: true maxPoolPreparedStatementPerConnectionSize: 20 minIdle: 1 connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: select 1 from dual testWhileIdle: true testOnBorrow: false testOnReturn: false filters: stat, wall, log4j jpa: properties: hibernate: hbm2ddl: auto: update show_sql: true format_sql: true 之后还需要在入口文件ZzuliApplication注解中添加参数。123456@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)public class ZzuliApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZzuliApplication.class, args); &#125;&#125; 禁止掉多数据源的自动注入，如果不这样做就会产生maven的依赖包冲突，导致重复依赖。 2.2 使用JpaRepository&emsp;&emsp;与Hibernate一样，都需要一个实体类对数据库表结构进行映射，在启动项目时，会对实体类创建相应的表结构。所以先在domain包里创建StudentEntity类，get和set方法在此省略了。1234567891011121314151617181920212223242526272829package cn.edu.zzuli.domain;import cn.edu.zzuli.base.BaseEntity;import javax.persistence.*;import java.io.Serializable;/** * @Author: cp * @Date: 2019/1/20 下午3:56 * @Version 1.0 */@Entity@Table(name = "studentEntity")public class StudentEntity implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "s_id") private Integer id; @Column(name = "s_name") private String name; @Column(name = "s_age") private Integer age; @Column(name = "s_address") private String address; /*getter and setter*/&#125; 对这几个注解做一下说明，使用@Entity会对实体类进行持久化操作，当JPA检测到实体类中有@Entity注解时，会在数据库中生成对应的表结构信息。@Table用来指定表名，@Id用来指定主键，配合@GeneratedValue(strategy = GenerationType.IDENTITY)指定主键自增策略，这里将主键自增交给数据库去做，所以使用IDENTITY，@Column用来指定对应表中的字段名。&emsp;&emsp;之后我们需要创建StudentJPA接口,先创建一个jpa包，在下面创建StudentJPA接口，继承JpaRepository，需要两个参数，一个是实体类对象，一个是主键类型。1234567/** * @Author: cp * @Date: 2019/1/20 下午4:01 * @Version 1.0 */public interface StudentJPA extends JpaRepository&lt;StudentEntity,Integer&gt; &#123;&#125; 我们查看JpaRepository就可以看到这个接口又继承了PagingAndSortingRepository和QueryByExampleExecutor接口，PagingAndSortingRepository又继承了CrudRepository接口，浓浓的Spring风格，基本上看名字就知道这个接口大概是干什么的，至于为什么这么设计，那就是架构的问题了。 2.1.1 CrudRepository&emsp;&emsp;看名字就知道，这个接口中包含了crud操作，也就是creat、select、delete、update、exist、count.1234567891011121314@NoRepositoryBeanpublic interface CrudRepository&lt;T, ID&gt; extends Repository&lt;T, ID&gt; &#123; &lt;S extends T&gt; S save(S var1); &lt;S extends T&gt; Iterable&lt;S&gt; saveAll(Iterable&lt;S&gt; var1); Optional&lt;T&gt; findById(ID var1); boolean existsById(ID var1); Iterable&lt;T&gt; findAll(); Iterable&lt;T&gt; findAllById(Iterable&lt;ID&gt; var1); long count(); void deleteById(ID var1); void delete(T var1); void deleteAll(Iterable&lt;? extends T&gt; var1); void deleteAll();&#125; 如果继承该接口就会拥有所有该接口的实现。 2.1.2 PagingAndSortingRepository&emsp;&emsp;顾名思义，分页和排序，而且继承了CrudRepository接口，拥有其所有的接口实现。12345@NoRepositoryBeanpublic interface PagingAndSortingRepository&lt;T, ID&gt; extends CrudRepository&lt;T, ID&gt; &#123; Iterable&lt;T&gt; findAll(Sort var1); Page&lt;T&gt; findAll(Pageable var1);&#125; 2.1.3 QueryByExampleExecutor&emsp;&emsp;这个接口就是实现条件查询和复杂查询的，可以使用exmple的方式查询。12345678public interface QueryByExampleExecutor&lt;T&gt; &#123; &lt;S extends T&gt; Optional&lt;S&gt; findOne(Example&lt;S&gt; var1); &lt;S extends T&gt; Iterable&lt;S&gt; findAll(Example&lt;S&gt; var1); &lt;S extends T&gt; Iterable&lt;S&gt; findAll(Example&lt;S&gt; var1, Sort var2); &lt;S extends T&gt; Page&lt;S&gt; findAll(Example&lt;S&gt; var1, Pageable var2); &lt;S extends T&gt; long count(Example&lt;S&gt; var1); &lt;S extends T&gt; boolean exists(Example&lt;S&gt; var1);&#125; 但是我感觉这个东西不怎么好用，以后会有替代品。 2.1.4 JpaRepository&emsp;&emsp;我们用的就是这个接口，它拥有以上所有接口的方法实现，并添加了条件查询和保存集合数据的方法，基本上简单的数据库操作就不需要写SQL了。1234567891011121314@NoRepositoryBeanpublic interface JpaRepository&lt;T, ID&gt; extends PagingAndSortingRepository&lt;T, ID&gt;, QueryByExampleExecutor&lt;T&gt; &#123; List&lt;T&gt; findAll(); List&lt;T&gt; findAll(Sort var1); List&lt;T&gt; findAllById(Iterable&lt;ID&gt; var1); &lt;S extends T&gt; List&lt;S&gt; saveAll(Iterable&lt;S&gt; var1); void flush(); &lt;S extends T&gt; S saveAndFlush(S var1); void deleteInBatch(Iterable&lt;T&gt; var1); void deleteAllInBatch(); T getOne(ID var1); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; var1); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; var1, Sort var2);&#125; 3.测试3.1 创建Controller层&emsp;&emsp;在controller 包中新建JPAStudentController类，这回测试就不使用JSP了，直接带上@RestController注解，并注入StudentJPA，返回json格式验证数据。12345678910/** * @Author: cp * @Date: 2019/1/20 下午4:06 * @Version 1.0 */@RestControllerpublic class JPAStudentController &#123; @Autowired private StudentJPA studentJPA;&#125; 3.2开启测试3.1.1 查询&emsp;&emsp;在JPAStudentController中添加list方法，使用JpaRepository内部实现的findAll方法。1234@RequestMapping(value = "/stu")public List&lt;StudentEntity&gt; list() &#123; return studentJPA.findAll();&#125; 接着在浏览器访问访问localhost:8080/stu，可以看到页面返回的数据。可以看到我在数据库预先准备的数据。&emsp;&emsp;除此之外，只要继承JpaRepository接口，我们还能使用方法规则进行查询，我第一次见的时候感觉挺神奇的，举个栗子，我在StudentJPA中定义StudentEntity findByName(String name);方法，他就可以直接被解析为。1select from StudentEntity where name=? 超级方便，简单的查询直接写定义这么一个方法就好了，不过弊端就是对于复杂查询，方法名会超级长，而且很难实现。&emsp;&emsp;如果想对SQL语句进行细致优化的话，我们还可以使用@Query注解自定义SQL。打开StudentJPA，在其中添加以年龄为条件的查询。12@Query(value = "select * from student_entity where s_age&gt;=?",nativeQuery = true)public List&lt;StudentEntity&gt; SelectByAge(int age); nativeQuery这个设置为true表明使用原生SQL，否则默认启用HQL。在controller层添加代码。1234@RequestMapping(value = "/age")public List&lt;StudentEntity&gt; age()&#123; return studentJPA.SelectByAge(20);&#125; 重启项目并在浏览器中输入localhost:8080/age。 3.1.2 增加&emsp;&emsp;增加数据只需要将实体类当作参数，调用StudentJPA的save方法即可。123456789@RequestMapping(value = "/add") public String add()&#123; StudentEntity entity=new StudentEntity(); entity.setAddress("郑州轻工业大学"); entity.setAge(19); entity.setName("范秉洋"); studentJPA.save(entity); return "添加成功"; &#125; save方法不仅仅用于增加，如果实体类中传入主键，那么save方法就会变为根据主键更新数据库操作，下面就不再写更新的方法了。 3.1.3 删除&emsp;&emsp;直接使用JpaRepository提供的deleteById方法即可。12345@RequestMapping(value = "/delete")public String delete(Integer userId)&#123; studentJPA.deleteById(userId); return "删除成功";&#125; 3.1.4 自定义语句&emsp;&emsp;如同在查询中使用@Query注解一样，增删改查都可以使用原生SQL对数据库操作，不过@Query只能查询，那么就需要配合另一个注解@Modifying一起使用。创建一个根据姓名和年龄删除数据的方法。123@Modifying@Query(value = "delete from student_entity where s_name=? and s_age=?",nativeQuery = true)public void deleteQuery(String name,Integer age); 如果这么写的话，会抛出一个TranscationRequiredException异常，意思就是当前操作需要事务，所以要在这个方法前加@Transactional开启自动化管理。 3.1.5 自定义BaseRepository&emsp;&emsp;正常情况下一个项目肯定不可能就继承一个JpaRepository接口，再使用其他模块时还需要多个接口继承，如果每一个业务数据接口都继承几个相同的接口的话，不是不可以，但是对于系统设计和代码复用性来说不是个什么好的选择。&emsp;&emsp;创建一个叫base的包，在里面添加BaseRepository接口，并继承JpaRepository。12345678/** * @Author: cp * @Date: 2019/1/20 下午10:29 * @Version 1.0 */@NoRepositoryBeanpublic interface BaseRepository&lt;T,PK extends Serializable&gt; extends JpaRepository&lt;T,PK&gt; &#123;&#125; @NoRepositoryBean:这个注解如果配置在继承了JpaRepository接口以及其他SpringDataJpa内部的接口的子接口时，子接口不被作为一个Repository创建代理实现类。 以后再创建接口继承BaseRepository就行了，他有JpaRepository所有实现方法。 3.1.6 分页查询&emsp;&emsp;对于一般项目来说分页是少不了的，当然，SpringDataJPA也内置了分页方法。&emsp;&emsp;先创建一个BaseEntity，添加几个字段：当前页码、每页条数、排序列，排序方法。12345678910111213141516171819202122232425/** * @Author: cp * @Date: 2019/1/20 下午10:45 * @Version 1.0 */public class BaseEntity &#123; /** * 默认页码 */ protected int page=1; /** * 默认分页数量 */ protected int size=2; /** * 排序列名为id */ protected String sidx="id"; /** * 排序规则 * @return */ protected String sord="desc"; /*getter and setter*/&#125; 修改StudentEntity类继承BaseEntity,由于数据不多，所以设定一页就两条。在JPAStudentController中添加cut方法，并添加对应分页逻辑。注意在分页中页码是从0开始的。1234567@RequestMapping(value = "/page") public List&lt;StudentEntity&gt; Page(Integer page)&#123; StudentEntity entity=new StudentEntity(); entity.setSize(2); entity.setPage(page); return studentJPA.findAll(PageRequest.of(entity.getPage()-1,entity.getSize())).getContent(); &#125; 从网上找的分页方法是使用PageRequest对象，不过使用时发现这个方法的构造方法在最新版本中被废弃了，查看源码后得知取而代之的是静态的PageRequest.of()方法。接下来重启项目并访问该方法。 3.1.6 排序&emsp;&emsp;BaseEntity预设好了对应字段，所以重新编辑page方法，将Sort对象添加在PageRequest.of()中就可以实现排序。12345678910111213@RequestMapping(value = "/page") public List&lt;StudentEntity&gt; Page(Integer page)&#123; StudentEntity entity=new StudentEntity(); entity.setSize(2); entity.setPage(page); entity.setSord("desc"); //获取排序对象 Sort.Direction sort_Direction=Sort.Direction.ASC.toString().equalsIgnoreCase(entity.getSord())?Sort.Direction.ASC:Sort.Direction.DESC; //设置排序对象 Sort sort=new Sort(sort_Direction,entity.getSidx()); //执行分页 return studentJPA.findAll(PageRequest.of(entity.getPage()-1,entity.getSize(),sort)).getContent(); &#125; 我们现在将顺序按照id倒序排序，SpringDataJPA对排序方式添加了一个枚举类型，创建Sort对象时也需要枚举对象，因为我们BaseEntity配置的是字符串所以上面多了一步判断排序方式返回枚举对象。重启项目。 4.总结&emsp;&emsp;基本操作就这么多了，包括了：CURD、分页、排序、自定义SQL、定义BaseRepository、事务处理等。用起来还是挺方便的，但是这并不是全部，后面还会对一对多、多对多等复杂查询进行总结，今天就先到这里吧。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>SpringDataJPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[我的第一篇博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-快速搭建]]></title>
    <url>%2F2019%2F01%2F19%2FSpringBoot-%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[SpringBoot-快速搭建1.SpringBoot简述 Spring Boot makes it easy to create stand-alone production-grade Spring based Applications that you can “just run”.We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration. &emsp;&emsp;摘自官网,大致意思是创建独立的、快速的、生产级的基于Spring的应用，对Spring和第三方库进行整合，轻松上手，只需要很少的配置就可以使用。 1.1优势&emsp;&emsp;在之前使用SSH(都9102年了我估计也没多少人用Struts了吧)与SSM时，虽然相对于传统的JSP+servlet+jdbc的开发模式简化了开发流程，但是在项目初期搭建的时候相当繁琐，大致需要： 1.配置xml，加载一坨配置文件. 2.配置Spring和SpringMVC，这两个还好，可以写到一个里面，配置Struts那是相当难受. 3.配置数据源，配置日志文件. 4.配置Mybatis的Mapper文件和Hibernate的配置文件 5.各种动态扫描注解 …… &emsp;&emsp;如果是在我学Django之前,我还是可以忍受这种东西的，但是学完Django之后，这种繁琐的配置就让人觉得很难受，我一直期望着Java中有一个和Django一样一个框架就可以干完所有活而且配置还非常简单的框架。得益于我喜欢在知(bi)乎上瞎逛，我了解了SpringBoot相关的技术链，并在禹州实训期间闲的没事干的情况下蹦入了这个坑，试了之后就一个字————爽！&emsp;&emsp;基本全程就几个配置文件，而且都不是xml，所有配置都是自动配置的，而且它内置tomcat，在使用idea的情况下，idea内置的各种工具可以实现全部框架的自动生成，再这里吹一波JetBrain。 突出优势：简单、快速、方便的搭建项目主流开发框架无配置集成提高开发、部署效率 2.项目搭建2.1开发环境 os:MacOS 10.13.4 ide:IDEA 2018.1.4 jdk:10.0.1 maven:3.3.9 tomcat:9.0(框架自带) 2.2新建项目&emsp;&emsp;新建项目时，点击左侧的Spring Initializr,然后点击next。 &emsp;&emsp;下一个页面是修改项目信息，第一个参数Group的域，第二个参数Artifact一般代表公司名称，这两个参数都是提供给maven的。最后一个参数Package为项目总包名，余下几个参数，Type一般都选择maven，Languge一般选择Java，也可以使用其他虚拟机语言，比如Kotlin、Groovy。Packing是打包方式，可以选择打成jar包或者war包 钩上web模版选择位置&emsp;&emsp;因为依托与maven，第一次加载时间可能比较长，因为所有配置需要从网络上去拉取，下载完毕后文件目录如图所示。看一下目录，大致分为三块： java文件：代码源文件，逻辑代码都在这里，里面那个ZzuliApplicaion是系统入口文件。 resource文件：静态资源文件，里面放着所有的静态资源和配置文件，application.properties中可以添加其他配置。 test文件：测试模块，里面内涵一个Junit测试。 外面有一个pom.xml文件，这个是maven的配置文件。 2.3 测试运行&emsp;&emsp;在之前创建的cn.edu.zzuli文件夹下创建Controller文件夹，并创建HelloWorldController类进行测试。1234567891011121314151617package cn.edu.zzuli.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author: cp * @Date: 2019/1/19 下午4:38 * @Version 1.0 */@RestControllerpublic class HelloWorldController &#123; @RequestMapping("/php") public String php()&#123; return "php是世界上最好的语言"; &#125;&#125; 基本上还是和SpringMVC很像的，唯一一点不同就是@RestController注解，这个注解就是原来@Controller和@ResponseBody注解的合体版。&emsp;&emsp;直接点击右上角的箭头或者启动ZzuliApplicaion类。出现这个界面就说明SpringBoot启动成功了。接着在浏览器中直接访问http://localhost:8080/php信息已被返回。 3.分析项目3.1 pox.xml文件&emsp;&emsp;SpringBoot自动生成的pox.xml文件如下12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.edu&lt;/groupId&gt; &lt;artifactId&gt;zzuli&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;zzuli&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 相较于一般的pox.xml文件，这里多了一个&lt;parent&gt;标签,这个标签是在配置SpringBoot的父级依赖,有了这个，当前的项目才是SpringBoot项目，spring-boot-starter-parent是一个特殊的starter，它用来提供相关的maven默认依赖，使用它之后，常用的包依赖就可以省去version标签。&emsp;&emsp;\repository\org\springframework\boot\spring-boot-dependencies\2.0.1.RELEASE\spring-boot-dependencies-2.0.1.RELEASE.pom里面有SpringBoot的所有依赖，想了解的去这里看看 3.2 入口类&emsp;&emsp;SpringBoot项目通常有一个名为*Application的入口类，入口类里有一个main方法，这个main方法其实就是一个标准的Java应用的入口方法。&emsp;&emsp;@SpringBootApplication是SpringBoot的核心注解，它是一个组合注解，该注解组合了：@Configuration、@EnableAutoConfiguration、@ComponentScan若不是用@SpringBootApplication注解也可以使用这三个注解代替。&emsp;&emsp;其中，EnableAutoConfiguration让SpringBoot根据类路径中的jar包依赖为当前项目进行自动配置，例如，添加了spring-boot-starter-web依赖，会自动添加Tomcat和SpringMVC的依赖，那么SpringBoot会对Tomcat和SpringMVC进行自动配置。&emsp;&emsp;SpringBoot还会自动扫描@SpringBootApplication所在类的同级包以及下级包里的Bean，所以入口类建议就配置在grounpID+arctifactID组合的包名下（这里为cn.edu.zzuli包），使用idea自动创建项目的话idea会自动在这里创建。 3.3 配置文件&emsp;&emsp;SpringBoot使用一个全局的配置文件application.properties或application.yml，放置在src/main/resources目录或者类路径的/config下。&emsp;&emsp;SpringBoot不仅支持常规的properties配置文件，还支持yaml语言的配置文件。yaml是以数据为中心的语言，在配置数据的时候具有面向对象的特征。&emsp;&emsp;SpringBoot的全局配置文件的作用是对一些默认配置的配置值进行修改。这里我将application.properties修改为application.yml。 4.使用SpringBoot&emsp;&emsp;上面只是一些简单的搭建以及运行，下面再深入了解一下SpringBoot的应用。SpringBoot的默认视图支持是 Thymeleaf 模板引擎。但是咱不会，只好还用JSP。 4.1集成JSP1.在pom.xml文件中集成JSP，向pom.xml中添加1234567891011121314&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2.在application.yml文件配置视图解析器，将我们的 JSP 文件重定向到 /WEB-INF/views/ 目录下：12345spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp 3.修改@RestController注解为@Controller(不改写不能写jsp)，然后修改 php 方法： 12345678910111213/** * @Author: cp * @Date: 2019/1/19 下午4:38 * @Version 1.0 */@Controllerpublic class HelloWorldController &#123; @RequestMapping("/php") public String php(Model model)&#123; model.addAttribute("php","php是世界上最好的语言"); return "php"; &#125;&#125; 4.在src/main目录下依次创建webapp/WEB-INF/views目录，并创建一个 hello.jsp 文件： 123&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8" %&gt;谁是世界上最好的语言？&lt;br&gt;$&#123;php&#125; 再刷新页面就可以看到效果了。 4.2集成MyBatis1.在pox.xml中添加对mysql和mybatis对依赖123456789101112&lt;!-- mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt;&lt;/dependency&gt; 2.在配置文件中添加配置12345678910111213spring: mvc: view: prefix: /WEB-INF/views/ suffix: .jsp datasource: url: jdbc:mysql://127.0.0.1:3306/test3?characterEncoding=UTF-8 username: root password: 20141232 driver-class-name: com.mysql.jdbc.Driver jpa: hibernate: ddl-auto: update 3.先在cn.edu.zzuli中创建domain包和mapper包，再创建Student实体类和StudentMapper映射类分别放入domain和mapper包,1234567891011121314151617181920212223242526272829package cn.edu.zzuli.domain;/** * @Author: cp * @Date: 2019/1/19 下午6:35 * @Version 1.0 */public class Student &#123; int id; String name; int age; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 123456789101112131415161718package cn.edu.zzuli.mapper;import cn.edu.zzuli.domain.Student;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Select;import java.util.List;/** * @Author: cp * @Date: 2019/1/19 下午6:41 * @Version 1.0 */@Mapperpublic interface StudentMapper &#123; @Select("select * from student") List&lt;Student&gt; findAll();&#125; 4.写一个StudentContorller 123456789101112131415161718192021222324252627package cn.edu.zzuli.Controller;import cn.edu.zzuli.domain.Student;import cn.edu.zzuli.mapper.StudentMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import java.util.List;/** * @Author: cp * @Date: 2019/1/19 下午6:44 * @Version 1.0 */@Controllerpublic class StudentController &#123; @Autowired StudentMapper studentMapper; @RequestMapping("/studentlist") public String studentlist(Model model)&#123; List&lt;Student&gt; students=studentMapper.findAll(); model.addAttribute("list",students); return "list"; &#125;&#125; 5.写一个list.jsp文件 1234567891011121314151617&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c"%&gt;&lt;table align='center' border='1' cellspacing='0'&gt; &lt;tr&gt; &lt;td&gt;id&lt;/td&gt; &lt;td&gt;name&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items="$&#123;list&#125;" var="s" varStatus="st"&gt; &lt;tr&gt; &lt;td&gt;$&#123;s.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;s.name&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt;&lt;/table&gt; 刷新网络，大功告成。 以上就是springboot的搭建，随后的时间里会对Springboot的其他模块进行更深入的探索。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
</search>
